#!/bin/bash

#SBATCH --job-name=TEST # Job name

##SBATCH --mail-type=END,FAIL # Mail events (NONE, BEGIN, END, FAIL, ALL)
#SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL)
#SBATCH --mail-user=milias@theor.jinr.ru # Where to send mail

#SBATCH --no-requeue
#SBATCH --ntasks=1 # Run on a single CPU
#SBATCH --cpus-per-task=8

##SBATCH --mem=1000mb # Job memory request
#SBATCH --mem=4GB # Job memory request

#SBATCH --time=4-00:00:00 # Time limit days-hrs:min:sec

#SBATCH --tmp=50G

#SBATCH --propagate=NONE
##SBATCH --array=1-5 # array of 5 jobs

#SBATCH --output=NONE

#SBATCH --partition=sl7 
##SBATCH --partition=ib

# define the unified slurm logfile
log=slurm_job_std-err-out.log-$SLURM_CLUSTER_NAME-$SLURM_JOB_PARTITION-N$SLURM_NTASKS-n$SLURM_CPUS_ON_NODE-jid$SLURM_JOB_ID

echo -e "=== Started SLURM job on the CICC cluster ==="  > $log 2>&1

echo -e "\nRunning on host `hostname`" >> $log  2>&1
echo -e " Job start date: `date` "        >> $log  2>&1

echo -e "\nJob user is SLURM_JOB_USER= $SLURM_JOB_USER"  >> $log  2>&1
echo -e "User job SLURM_JOB_NAME=$SLURM_JOB_NAME has assigned ID SLURM_JOBID=$SLURM_JOBID" >> $log 2>&1
echo -e "This job was submitted from the computer SLURM_SUBMIT_HOST=$SLURM_SUBMIT_HOST" >> $log  2>&1
echo -e "and from the home directory SLURM_SUBMIT_DIR:" >> $log  2>&1
echo -e "SLURM_SUBMIT_DIR = $SLURM_SUBMIT_DIR" >> $log  2>&1

echo -e "Job is running on the cluster compute node: SLURM_CLUSTER_NAME=$SLURM_CLUSTER_NAME"  >> $log 2>&1
echo -e "and is employing SLURM_JOB_NUM_NODES=$SLURM_JOB_NUM_NODES node/nodes:"  >> $log  2>&1
echo -e "SLURM_JOB_NODELIST = $SLURM_JOB_NODELIST"   >> $log  2>&1
echo -e "Job partition is SLURM_JOB_PARTITION=$SLURM_JOB_PARTITION \n"  >> $log 2>&1
echo -e "Number of allocated CPUs on each single node, SLURM_CPUS_ON_NODE=$SLURM_CPUS_ON_NODE ."  >> $log 2>&1
echo -e "Number of all reserved threads over ALL nodes, SLURM_NTASKS=$SLURM_NTASKS ."   >> $log 2>&1
echo -e "Job has reserved memory per node, SLURM_MEM_PER_NODE=$SLURM_MEM_PER_NODE MB of memory"  >> $log  2>&1

# CPU model, total numer of CPUs, number of allocated CPUs
echo -e "The node's CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq   >> $log  2>&1
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`   >> $log  2>&1
echo "BTW, this node has total $NPROCS CPUs available for an EXCLUSIVE job."  >> $log 2>&1
echo "Based on reserved memory, this node got $SLURM_CPUS_ON_NODE CPUs allocated for SLURM calculations." >> $log 2>&1
echo "This job wants SLURM_NTASKS=$SLURM_NTASKS threads . "  >> $log 2>&1

# Generate Machinefile for mpi such that hosts are in the same
# order as if run via srun
MACHINEFILE="nodes.$SLURM_JOB_PARTITION.$SLURM_JOB_ID"
srun -l /bin/hostname | sort -n | awk '{print $2}' > $MACHINEFILE
echo -e "\ngenerated machinefile for MPI, $MACHINEFILE"; ls -lt $PWD/$MACHINEFILE; echo "containing:"; cat $MACHINEFILE >> $log 2>&1


echo -e "\n\n The total memory at the running node (in GB)"  >> $log 2>&1
free -t -g  >> $log 2>&1


# disk space 
echo -e "\n\n Disk system on the computing node `hostname`  :"   >> $log 2>&1
ls -lt /  >> $log 2>&1
df -h /tmp  >> $log 2>&1
df -h $SLURM_SUBMIT_DIR  >> $log 2>&1
echo -e "variable TMPDIR=$TMPDIR" >> $log 2>&1
echo -e "ls -lt /scr/u ?" >> $log 2>&1
ls -lt /scr/u  >> $log 2>&1
echo -e "ls -lt /scr ?" >> $log 2>&1
ls -lt /scr  >> $log 2>&1
df -h $TMPDIR >> $log 2>&1

# modules
echo -e "\n\n Kicking on modules:"   >> $log 2>&1
source /cvmfs/bmn.jinr.ru/config/x86_64-alma9/cluster_config.sh >> $log 2>&1
echo -e "module avail ?" >> $log 2>&1
module avail >> $log  2>&1


# basic utility software
echo -e "\n\n Checking presence of software:" >> $log 2>&1
echo -e "Python ?"  >> $log 2>&1
Python -V >> $log 2>&1
echo -e "gfortran ?"  >> $log 2>&1
gfortran --version >> $log 2>&1
echo -e "mpirun ?"  >> $log 2>&1
mpirun --version >> $log 2>&1


# run own application ...
QE=/scr/u/milias/software/quantum-espresso/q-e_develop
echo -e "\n\n Application software QE=$QE" >> $log 2>&1
ls -lt $QE/.  >> $log  2>&1


echo -e "\n\n Job end date: `date` "        >> $log 2>&1

exit 
