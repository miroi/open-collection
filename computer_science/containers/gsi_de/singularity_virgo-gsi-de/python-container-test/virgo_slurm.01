#!/bin/bash
#SBATCH -J PySing

## max. execution time
##SBATCH -t 1-00:00:00
##SBATCH -t 0-08:00:00
#SBATCH -t 0-01:00:00

# number of nodes
#SBATCH -N 1
##SBATCH --exclusive
##SBATCH --mem=120GB
##SBATCH --mem=24GB
##SBATCH --mem=12GB
#SBATCH --mem=4GB

# do not restart in the case of nodefail!
#SBATCH --no-requeue
#SBATCH --no-kill

## CPU count  - max. 40 per node ?
##SBATCH -n 6

##  partition (queue)
##SBATCH -p parallel
##SBATCH -p long
#SBATCH -p main

## memory NO !!!
##SBATCH --mem-per-cpu=28G

## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

## mail
#SBATCH --mail-user=M.Ilias@gsi.de 
#SBATCH --mail-type=ALL

 
echo -e "Job user is SLURM_JOB_USER= $SLURM_JOB_USER"
echo -e "User job SLURM_JOB_NAME=$SLURM_JOB_NAME has assigned ID SLURM_JOBID=$SLURM_JOBID"

echo -e "This job was submitted from the computer SLURM_SUBMIT_HOST=$SLURM_SUBMIT_HOST"
echo -e " and from the home directory SLURM_SUBMIT_DIR=$SLURM_SUBMIT_DIR"

echo -e "Job is running on the cluster compute node: SLURM_CLUSTER_NAME=$SLURM_CLUSTER_NAME"
echo -e "and is employing SLURM_JOB_NUM_NODES=$SLURM_JOB_NUM_NODES node/nodes:"
echo -e "SLURM_JOB_NODELIST = $SLURM_JOB_NODELIST"
echo
echo -e "Job partition is SLURM_JOB_PARTITION=$SLURM_JOB_PARTITION \n"
echo -e "The job requests SLURM_CPUS_ON_NODE=$SLURM_CPUS_ON_NODE CPU per task."

module use /cvmfs/it.gsi.de/modulefiles/
echo "modules at disposal:"
module avail
echo

#
#source /lustre/ukt/milias/bin/intel-mpi-mkl_2019.4/intel/bin/compilervars.sh intel64 -platform linux  
#source /lustre/ukt/milias/bin/intel-mpi-mkl_2019.4/intel/bin/compilervars.sh intel64 -platform linux 
#source /lustre/ukt/milias/bin/intel-mpi-mkl_2019.4/intel/compilers_and_libraries_2019.4.243/linux/mpi/intel64/bin/mpivars.sh
#module load compiler/intel/19.0 echo -e "Activated Intel 19.0, MKL & Intel-MPI"
#which mpiifort; which mpiicc; which mpiicpc; which mpirun
#mpiifort -v; mpiicc -v; mpirun --version

#module load cmake/3.15.3
#module load compiler/intel/17.4

echo -e "\n\n loaded modules:"
module list

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"

# CPU model, total numer of CPUs, number of allocated CPUs
echo -e "The node's CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo "This node has total $NPROCS CPUs available for EXCLUSIVE job."
#echo "This node has $SLURM_CPUS_ON_NODE CPUs allocated for SLURM calculations."

echo -e "\n the memory at the node (in GB)"
free -t -g
echo -e "\n"

echo -e "\nMy PATH=$PATH\n"
#echo -e "Python -v :\c"; python -V
#echo -e "cmake ? which cmake = \c"; which cmake
#echo -e "\n mpirun ? \c"; which mpirun; mpirun --version
echo

echo -e "\n For comparison, /tmp local disk;  df -h /tmp/."; df -h /tmp
echo -e "\n"

# Singularity variables #
   export LUSTRE_HOME=/lustre/ukt/milias
   export SINGULARITY_CONTAINERS=$LUSTRE_HOME/containers
   export SINGULARITY_DEFINITIONS=$SINGULARITY_CONTAINERS/definitions
   alias sc='ls -lt $SINGULARITY_CONTAINERS'
   export SINGULARITY_CACHEDIR=$LUSTRE_HOME/containers/cache
   export SINGULARITY_TMPDIR=$LUSTRE_HOME/containers/tmp

echo -e "SINGULARITY_CONTAINERS=$SINGULARITY_CONTAINERS"
echo -e "SINGULARITY_DEFINITIONS=$SINGULARITY_DEFINITIONS"
echo -e "SINGULARITY_CACHEDIR=$SINGULARITY_CACHEDIR"
echo -e "SINGULARITY_TMPDIR=$SINGULARITY_TMPDIR"

ls -lt $SINGULARITY_CONTAINERS/python.sif

singularity exec $SINGULARITY_CONTAINERS/python.sif python --version

# for running jobs from your homedir
#cd $SLURM_SUBMIT_DIR

echo -e "Launching hello_world !"
#singularity exec   $SINGULARITY_CONTAINERS/python.sif ./hello_world.py
singularity exec   $SINGULARITY_CONTAINERS/python.sif $SLURM_SUBMIT_DIR/hello_world.py


exit
