#!/bin/bash

#SBATCH -J sQg

## max. execution time
##SBATCH -t 7-00:00:00
#SBATCH -t 0-08:00:00
##SBATCH -t 0-01:30:00

# number of nodes
#SBATCH -N 1
##SBATCH --exclusive
##SBATCH --mem=120GB
#SBATCH --mem=64GB

# do not restart in the case of nodefail!
#SBATCH --no-requeue
#SBATCH --no-kill

## CPU count  - max. 40 per node ?
##SBATCH -n 4
#SBATCH --ntasks-per-node=16

##  partition (queue)
##SBATCH -p long
#SBATCH -p main

## memory NO !!!
##SBATCH --mem-per-cpu=28G

## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

## mail
#SBATCH --mail-user=M.Ilias@gsi.de 
#SBATCH --mail-type=ALL

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"

echo -e "\n\n Job user is $SLURM_JOB_USER and his job $SLURM_JOB_NAME has assigned ID $SLURM_JOBID"
echo -e "This job was submitted from the computer $SLURM_SUBMIT_HOST"
echo -e "and from the home directory:"
echo $SLURM_SUBMIT_DIR
echo
echo It is running on the cluster compute node: $SLURM_CLUSTER_NAME
echo and is employing $SLURM_JOB_NUM_NODES node/nodes:
echo $SLURM_JOB_NODELIST
echo
echo -e "Job partition is: $SLURM_JOB_PARTITION \n"
echo -e "The job requests $SLURM_CPUS_ON_NODE CPU per task."

MACHINEFILE="nodes.$SLURM_JOB_PARTITION.$SLURM_JOB_ID"
# Generate Machinefile for mpi such that hosts are in the same
# order as if run via srun
srun -l /bin/hostname | sort -n | awk '{print $2}' | grep lxb* > $MACHINEFILE
echo -e "\ngenerated machinefile for MPI, $MACHINEFILE"; ls -lt $PWD/$MACHINEFILE; cat $MACHINEFILE


# CPU model, total numer of CPUs, number of allocated CPUs
echo -e "The node's CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo "This node has total $NPROCS CPUs available for an EXCLUSIVE job."
echo "SLURM_CPUS_ON_NODE=${SLURM_CPUS_ON_NODE} CPUs allocated for SLURM calculations."
echo "This job wants SLURM_NTASKS=$SLURM_NTASKS threads . "

RATIO=$(( $SLURM_CPUS_ON_NODE / $SLURM_NTASKS ))
echo -e "Ratio SLURM_CPUS_ON_NODE/SLURM_NTASKS=$SLURM_CPUS_ON_NODE/$SLURM_NTASKS=\c";echo $(( $SLURM_CPUS_ON_NODE / $SLURM_NTASKS ))
echo -e $RATIO
# set OpenMP threads accordingly
#export USE_OPENMP=1
export OPENBLAS_NUM_THREADS=$RATIO
echo -e "\nFor openblas internal parallelization, OPENBLAS_NUM_THREADS=$OPENBLAS_NUM_THREADS"


echo -e "\n the memory at the node (in GB)"
free -t -g
echo -e "\n"

#
#  Load Spack modules
#
spack unload --all
#
spack load gcc@10.2.0 target=x86_64;
export LD_LIBRARY_PATH=/cvmfs/vae.gsi.de/vae23/spack-0.19/opt/linux-debian10-x86_64/gcc-8.3.0/gcc-10.2.0-agxjp3zexhitnb3g6czo5p4im3hi74ht/lib/gcc/x86_64-pc-linux-gnu/10.2.0:$LD_LIBRARY_PATH
#
spack load openmpi%gcc target=x86_64
export LD_LIBRARY_PATH=/cvmfs/vae.gsi.de/vae23/spack-0.19/opt/linux-debian10-x86_64/gcc-10.2.0/openmpi-4.1.5-phbdvrf3few3givo575jlifx6dhnfgk7/lib:$LD_LIBRARY_PATH
#
spack load elpa%gcc target=x86_64
export LD_LIBRARY_PATH=/cvmfs/vae.gsi.de/vae23/spack-0.19/opt/linux-debian10-x86_64/gcc-10.2.0/elpa-2021.11.001-uorwjue22nh7br4jthmt3lfugpeivfms/lib:$LD_LIBRARY_PATH
#
spack load amdfftw%gcc target=x86_64
export LD_LIBRARY_PATH=/cvmfs/vae.gsi.de/vae23/spack-0.19/opt/linux-debian10-zen/gcc-10.2.0/amdfftw-3.0-oehivet3nneevlfpyzp5ovxfiz32ik46/lib:$LD_LIBRARY_PATH
#
spack load amdscalapack%gcc target=x86_64
export LD_LIBRARY_PATH=/cvmfs/vae.gsi.de/vae23/spack-0.19/opt/linux-debian10-x86_64/gcc-10.2.0/amdscalapack-3.2-zmrsnzmnifwusgdparcdnpdksnehsbcm/lib:$LD_LIBRARY_PATH
#
spack load openblas%gcc target=x86_64
export LD_LIBRARY_PATH=/cvmfs/vae.gsi.de/vae23/spack-0.19/opt/linux-debian10-x86_64/gcc-10.2.0/openblas-0.3.21-q7nhojttkz52xuf4zkxk7vvgllqnxh34/lib:$LD_LIBRARY_PATH

echo -e "\n all spack loaded modules:"
spack find --loaded

echo -e "\n Python -v :\c"; python -V
#
export QE=/lustre/ukt/milias/work/software/quantum-espresso/qe-7.2
export QEbin=$QE/build_gnu_openmpi_openblas
echo -e "\n ldd ${QEbin}/bin/pw.x"; ldd ${QEbin}/bin/pw.x

export SCRATCH=/lustre/ukt/milias/scratch
export ESPRESSO_TMPDIR=$SCRATCH/QEjob.$SLURM_JOB_PARTITION.N$SLURM_JOB_NUM_NODES.n$SLURM_NTASKS.jid$SLURM_JOBID
mkdir $ESPRESSO_TMPDIR
echo -e "\nQE work directory, df -h ESPRESSO_TMPDIR=$ESPRESSO_TMPDIR and its free space:"
df -h $ESPRESSO_TMPDIR
echo -e "For comparison, /tmp local disk;  df -h /tmp/."; df -h /tmp
echo -e "\n"

## for running jobs from your homedir
cd $SLURM_SUBMIT_DIR

# echo "SLURM_CPUS_ON_NODE/SLURM_NTASKS=$SLURM_CPUS_ON_NODE/$SLURM_NTASKS";

 #proj=Cs-Au64.scf_k22x22x1_ewft44_z10
 #proj=IOO_on_Quartz-G_001_Si7cut_4x4.qe_scf
 proj=sQ-G.scf_z15_k10x10x10

 echo -e "\nRunning QE job with mpirun $SLURM_NTASKS threads, memory of $SLURM_MEM_PER_NODE MB, input=$proj.in "
 mpirun -machinefile $MACHINEFILE -np $SLURM_NTASKS $QE/pw.x < $proj.in > $proj.$SLURM_JOB_PARTITION.N$SLURM_JOB_NUM_NODES.n$SLURM_NTASKS.omp$OPENBLAS_NUM_THREADS.id$SLURM_JOBID.out

echo -e "\n Working disk space and its content, ESPRESSO_TMPDIR=$ESPRESSO_TMPDIR :"
df -h $ESPRESSO_TMPDIR
ls -lt  $ESPRESSO_TMPDIR


exit
