#!/bin/bash
#SBATCH -J Qg

##  partition (queue)
##SBATCH -p long
#SBATCH -p main

## max. execution time
##SBATCH -t 7-00:00:00
#SBATCH -t 0-08:00:00

# number of nodes
#SBATCH -N 4
##SBATCH --exclusive
#SBATCH --mem=360GB
##SBATCH --mem=8GB

# do not restart in the case of nodefail!
#SBATCH --no-requeue
#SBATCH --no-kill

## CPU count  - max. 40 per node ?
#SBATCH --ntasks-per-node=36

## memory NO !!!
##SBATCH --mem-per-cpu=28G

## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

## mail
#SBATCH --mail-user=M.Ilias@gsi.de 
#SBATCH --mail-type=ALL

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"

echo -e "\n the memory at the node (in GB)"
free -t -g
echo -e "\n"

# CPU model, total numer of CPUs, number of allocated CPUs
echo -e "The node's CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo "This node has total $NPROCS CPUs available for an EXCLUSIVE job."

echo -e "\n\n Job user is $SLURM_JOB_USER and his job $SLURM_JOB_NAME has assigned ID $SLURM_JOBID"
echo -e "This job was submitted from the computer $SLURM_SUBMIT_HOST"
echo -e "and from the home directory:"
echo $SLURM_SUBMIT_DIR
echo
echo It is running on the cluster compute node: $SLURM_CLUSTER_NAME
echo and is employing $SLURM_JOB_NUM_NODES node/nodes:
echo $SLURM_JOB_NODELIST
echo
echo -e "Job partition is: $SLURM_JOB_PARTITION \n"
echo -e "The job requests $SLURM_CPUS_ON_NODE CPU per task."
#
echo "SLURM_CPUS_ON_NODE=${SLURM_CPUS_ON_NODE} CPUs allocated for SLURM calculations per node."
echo "SLURM_NTASKS_PER_NODE=${SLURM_NTASKS_PER_NODE} ntasks allocated for SLURM calculations per node."
echo "SLURM_NTASKS=${SLURM_NTASKS}"
echo "SLURM_NPROCS=${SLURM_NPROCS}"

#
#  Load Spack modules
#
spack load gcc@10.2.0 target=x86_64;
export LD_LIBRARY_PATH=/cvmfs/vae.gsi.de/vae23/spack-0.19/opt/linux-debian10-x86_64/gcc-8.3.0/gcc-10.2.0-agxjp3zexhitnb3g6czo5p4im3hi74ht/lib/gcc/x86_64-pc-linux-gnu/10.2.0:$LD_LIBRARY_PATH
#
spack load hwloc@2.8.0 target=x86_64
export LD_LIBRARY_PATH=/cvmfs/vae.gsi.de/vae23/spack-0.19/opt/linux-debian10-x86_64/gcc-10.2.0/hwloc-2.8.0-rpajza4w5yeshrr7qnbtwiz7cgsc5y4l/lib/:$LD_LIBRARY_PATH
#
spack load openmpi%gcc target=x86_64
export LD_LIBRARY_PATH=/cvmfs/vae.gsi.de/vae23/spack-0.19/opt/linux-debian10-x86_64/gcc-10.2.0/openmpi-4.1.5-phbdvrf3few3givo575jlifx6dhnfgk7/lib:$LD_LIBRARY_PATH
#
spack load elpa%gcc target=x86_64
export LD_LIBRARY_PATH=/cvmfs/vae.gsi.de/vae23/spack-0.19/opt/linux-debian10-x86_64/gcc-10.2.0/elpa-2021.11.001-uorwjue22nh7br4jthmt3lfugpeivfms/lib:$LD_LIBRARY_PATH
#
spack load amdscalapack%gcc target=x86_64
export LD_LIBRARY_PATH=/cvmfs/vae.gsi.de/vae23/spack-0.19/opt/linux-debian10-x86_64/gcc-10.2.0/amdscalapack-3.2-zmrsnzmnifwusgdparcdnpdksnehsbcm/lib:$LD_LIBRARY_PATH
#
spack load openblas%gcc target=x86_64
export LD_LIBRARY_PATH=/cvmfs/vae.gsi.de/vae23/spack-0.19/opt/linux-debian10-x86_64/gcc-10.2.0/openblas-0.3.21-q7nhojttkz52xuf4zkxk7vvgllqnxh34/lib:$LD_LIBRARY_PATH

echo -e "\n all spack loaded modules:"
spack find --loaded

echo -e "\n Python -v :\c"; python -V

#

export NWCHEM_EXECUTABLE=/lustre/ukt/milias/work/software/nwchem/nwchem-7.2.0_gnu_openmpi_openblas/bin/LINUX64/nwchem
echo -e "\n ldd NWCHEM_EXECUTABLE=$NWCHEM_EXECUTABLE"; ldd $NWCHEM_EXECUTABLE

# set working scratch directory for the run
SCR=NWChem_SLURMjob.${SLURM_JOB_PARTITION}.N${SLURM_JOB_NUM_NODES}.n${SLURM_NTASKS}.jid${SLURM_JOBID}
export TMPDIR=/lustre/ukt/milias/scratch/$SCR
mkdir $TMPDIR
cd $TMPDIR
echo -e "\n I am in NWChem working directory, TMPDIR=$TMPDIR"
echo -e "for control, pwd=";pwd
echo -e "Size of this work directory, df -h \$TMPDIR...:";df -h $TMPDIR
echo -e "\n The local home directory with NWChem input file, SLURM_SUBMIT_DIR=${SLURM_SUBMIT_DIR}"

# for running jobs from your homedir
#cd $SLURM_SUBMIT_DIR

#npr=$((SLURM_CPUS_ON_NODE/2))
#echo -e "number of MPI threads npr=$npr"

#project=AtOHonQ-G.dkh_b3lyp
#project=Q-G.dkh_b3lyp
project=Q-G.ecp_b3lyp
cp ${SLURM_SUBMIT_DIR}/Quartz-G_001.molecular_slab_smaller.xyz  $PWD/.
echo -e "\n Launching NWChem run with project=${project}"
mpirun -np  ${SLURM_NTASKS} ${NWCHEM_EXECUTABLE}  ${SLURM_SUBMIT_DIR}/$project.nw  > ${SLURM_SUBMIT_DIR}/$project.${SLURM_JOB_PARTITION}_N${SLURM_JOB_NUM_NODES}_n${SLURM_NTASKS}_jid${SLURM_JOBID}.out

# save conveged geometry to homedir 
#cp $TMPDIR/HD_geom*   ${SLURM_SUBMIT_DIR}/.

echo -e "\n Working files remained in scratch dir \$TMPDIR:"
ls -lt $TMPDIR/* 
du -h -s $TMPDIR
cd .. ; echo -e "just for control -  pwd=\c";pwd
#echo -e "\nDeleting scratch directory $TMPDIR: "; /bin/rm -r $TMPDIR
#echo -e "\n Afterwards, content of ls -lt /mnt/local/$USER :"; ls -lt /mnt/local/$USER

echo -e "\n Time of finishing job is `date` \n"

echo -e "\n returning to ${SLURM_SUBMIT_DIR}, list of files:"
cd ${SLURM_SUBMIT_DIR}
ls -lt

exit
