

Running on host n05p026
Time is Wed Oct 15 11:59:54 MSK 2025 


Job user is SLURM_JOB_USER= dsen
User job SLURM_JOB_NAME=debug has assigned ID SLURM_JOBID=14273218
This job was submitted from the computer SLURM_SUBMIT_HOST=space03.hydra.local
and from the home directory SLURM_SUBMIT_DIR:
/lustre/home/user/d/dsen/Documents/2_example/mopac_example/3_ASE_MOPAC/BFGS_coarse_final
Job is running on the cluster compute node: SLURM_CLUSTER_NAME=gvr
and is employing SLURM_JOB_NUM_NODES=1 node/nodes:
SLURM_JOB_NODELIST = n05p026
Job partition is SLURM_JOB_PARTITION=slo-ice 

Number of allocated CPUs on each single node, SLURM_CPUS_ON_NODE=32 .
Number of all reserved threads over ALL nodes, SLURM_NTASKS=32 .
Job has reserved memory per node, SLURM_MEM_PER_NODE= MB of memory

generated machinefile for MPI, nodes.slo-ice.14273218
-rw-r--r-- 1 dsen hybrilit 256 Oct 15 11:59 /lustre/home/user/d/dsen/Documents/2_example/mopac_example/3_ASE_MOPAC/BFGS_coarse_final/nodes.slo-ice.14273218
containing:
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
n05p026
The node's CPU model name	: Intel(R) Xeon(R) Platinum 8368Q CPU @ 2.60GHz
BTW, this node has total 152 CPUs available for an EXCLUSIVE job.
Based on reserved memory, this node got 32 CPUs allocated for SLURM calculations.
This job wants SLURM_NTASKS=32 threads . 


Loaded modules:
Currently Loaded Modulefiles:
  1) intel/oneapi      3) BASE/1.0
  2) GVR/v1.0-1        4) Python/v3.10.13

Running on host n05p026
Time is Wed Oct 15 11:59:55 MSK 2025 


The total memory at the node (in GB)
              total        used        free      shared  buff/cache   available
Mem:           1991         248        1738           2           4        1739
Swap:             3           0           3
Total:         1995         248        1742



ldd  /bin/pw.x :
ldd: /bin/pw.x: No such file or directory

ldd  /bin/vasp_ncl :
ldd: /bin/vasp_ncl: No such file or directory

ldd  /lustre/home/user/d/dsen/bin/mopac/mopac//bin/mopac :
	linux-vdso.so.1 =>  (0x00007fff46d0b000)
	libmopac.so.2 => /lustre/home/user/d/dsen/bin/mopac/mopac/bin/libmopac.so.2 (0x00002b7e6f3db000)
	libmkl_intel_lp64.so.1 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mkl/latest/lib/intel64/libmkl_intel_lp64.so.1 (0x00002b7e73f59000)
	libmkl_intel_thread.so.1 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mkl/latest/lib/intel64/libmkl_intel_thread.so.1 (0x00002b7e74af8000)
	libmkl_core.so.1 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mkl/latest/lib/intel64/libmkl_core.so.1 (0x00002b7e78247000)
	libiomp5.so => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/compiler/lib/intel64_lin/libiomp5.so (0x00002b7e7c6b5000)
	libm.so.6 => /lib64/libm.so.6 (0x00002b7e7cad5000)
	libdl.so.2 => /lib64/libdl.so.2 (0x00002b7e7cdd7000)
	libmpifort.so.12 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mpi/latest/lib/libmpifort.so.12 (0x00002b7e7cfdb000)
	libmpi.so.12 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mpi/latest/lib/release/libmpi.so.12 (0x00002b7e7d38f000)
	librt.so.1 => /lib64/librt.so.1 (0x00002b7e7ea5f000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x00002b7e7ec67000)
	libc.so.6 => /lib64/libc.so.6 (0x00002b7e7ee83000)
	/lib64/ld-linux-x86-64.so.2 (0x00002b7e6f1b7000)
	libgcc_s.so.1 => /lustre/home/user/m/milias/work/software/ams/linux.openmpi/ams2021.107/bin/lib/libgcc_s.so.1 (0x00002b7e7f251000)
	libifport.so.5 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/compiler/lib/intel64_lin/libifport.so.5 (0x00002b7e7f469000)
	libifcoremt.so.5 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/compiler/lib/intel64_lin/libifcoremt.so.5 (0x00002b7e6f1f2000)
	libimf.so => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/compiler/lib/intel64_lin/libimf.so (0x00002b7e7f697000)
	libsvml.so => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/compiler/lib/intel64_lin/libsvml.so (0x00002b7e7fdc8000)
	libintlc.so.5 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/compiler/lib/intel64_lin/libintlc.so.5 (0x00002b7e81e5b000)


My PATH=/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/Python/v3.10.13/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/vtune/latest/bin64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/vpl/latest/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mpi/latest/libfabric/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mpi/latest/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/itac/latest/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/inspector/latest/bin64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/dpcpp-ct/latest/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/dev-utilities/latest/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/debugger/latest/gdb/intel64/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/bin/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/clck/latest/bin/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/advisor/latest/bin64:/lustre/home/user/d/dsen/backup/resources:/lustre/home/user/d/dsen/.local/bin:/lustre/home/user/d/dsen/bin:/lustre/home/user/m/milias/work/software/ams/linux.openmpi/ams2021.107/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:/lustre/home/user/d/dsen/.local/bin:/lustre/home/user/d/dsen/bin


My LD_LIBRARY_PATH=/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/Python/v3.10.13/lib64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/Python/v3.10.13/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/vpl/latest/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/tbb/latest/lib/intel64/gcc4.8:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mpi/latest/libfabric/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mpi/latest/lib/release:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mpi/latest/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mkl/latest/lib/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/itac/latest/slib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/ippcp/latest/lib/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/ipp/latest/lib/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/dnnl/latest/cpu_dpcpp_gpu_dpcpp/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/debugger/latest/dep//lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/debugger/10.1.2/libipt/intel64/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/debugger/10.1.2/gdb/intel64/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/dal/latest/lib/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/lib/x64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/lib/emu:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/compiler/lib/intel64_lin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/clck/latest/lib/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/ccl/latest/lib/cpu_gpu_dpcpp:/lustre/home/user/d/dsen/bin/mopac/mopac/bin 
Python -v :Python 3.10.13

 ifort -V: Intel(R) Fortran Intel(R) 64 Compiler Classic for applications running on Intel(R) 64, Version 2021.4.0 Build 20210910_000000
Copyright (C) 1985-2021 Intel Corporation.  All rights reserved.


 mpiifort -V: Intel(R) Fortran Intel(R) 64 Compiler Classic for applications running on Intel(R) 64, Version 2021.4.0 Build 20210910_000000
Copyright (C) 1985-2021 Intel Corporation.  All rights reserved.


 mpirun ? /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mpi/latest/bin/mpirun
Intel(R) MPI Library for Linux* OS, Version 2021.4 Build 20210831 (id: 758087adf)
Copyright 2003-2021, Intel Corporation.

Current directory where this SLURM job is running /lustre/home/user/d/dsen/Documents/2_example/mopac_example/3_ASE_MOPAC/BFGS_coarse_final
It has the disk space of (df -h) :
Filesystem                               Size  Used Avail Use% Mounted on
10.220.25.3@o2ib,10.220.25.4@o2ib:/home  650T  544T  107T  84% /lustre/home


Starting ASE parallel job on Wed Oct 15 11:59:55 MSK 2025

Successfully read 630 atoms from 1IYT_geopt_pm7water_geo-arc.pdb

Starting MOPAC optimization...
Note: MOPAC reports gradients in kcal/mol·Å; ASE converts to eV/Å

=== Final Results ===
Heat of Formation: -139.144630 eV
Maximum Force: 0.085651 eV/Å
RMS Force: 0.023447 eV/Å

=== Final structures saved ===
  pm7water_optimized.xyz - XYZ format
  pm7water_optimized.pdb - PDB format

 ASE finished on : Wed Oct 15 12:20:51 MSK 2025
