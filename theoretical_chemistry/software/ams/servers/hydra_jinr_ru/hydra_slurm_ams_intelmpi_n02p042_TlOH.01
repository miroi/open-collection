#!/bin/bash

#SBATCH -J intelmpiams

##  partition (queue)
##SBATCH -p cpu
##SBATCH -p test
#SBATCH -p cascade

## max. execution time
##SBATCH -t 1-00:00:00
#SBATCH -t 0-01:00:00

##SBATCH --exclusive

##SBATCH --mem=32GB
##SBATCH --mem=24GB
#SBATCH --mem=8GB

# do not restart in the case of nodefail!
#SBATCH --no-requeue
#SBATCH --no-kill

##  allowed CPU nodes for JIRN AMS license:  n02p042,n01p001
#
#SBATCH -N 1 -n 4  --nodelist=n02p042

## memory NO !!!
##SBATCH --mem-per-cpu=28G

## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

## mail
#SBATCH --mail-user=Miroslav.Ilias@umb.sk
#SBATCH --mail-type=ALL

 
echo Job user is $SLURM_JOB_USER and his job $SLURM_JOB_NAME has assigned ID $SLURM_JOBID
echo This job was submitted from the computer $SLURM_SUBMIT_HOST
echo and from the home directory:
echo $SLURM_SUBMIT_DIR
echo
echo It is running on the cluster compute node:
echo $SLURM_CLUSTER_NAME
echo and is employing $SLURM_JOB_NUM_NODES node/nodes:
echo $SLURM_JOB_NODELIST
echo
echo -e "Job partition is $SLURM_JOB_PARTITION \n"
echo The job requests $SLURM_CPUS_ON_NODE CPU per task.

echo "modules at disposal:"
module avail
echo

#
module purge
#module load intel/v2021.1

echo -e "\n\n loaded modules:"
module list

# use server's IntelMPI, that is intel/v2021.1
#export SCM_USE_LOCAL_IMPI=1

####   Use AMS's shipped  IntelMPI   ####
export SCM_USE_IMPI_2021=1
#source /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.intelmpi/ams2021.103/bin/intelmpi/bin/mpivars.sh
#
#
# In both cases you might need to edit the $AMSBIN/start script around line 287, to change the location pointed to by the "I_MPI_PMI_LIBRARY" variable. 
#
#

# load AMS envirovariables
#source /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/AMS/v2021.102/amsbashrc.sh
#source /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/amsbashrc.sh
#source /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.intelmpi+StaticMKL/ams2021.103/amsbashrc.sh
source /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.intelmpi/ams2021.103/amsbashrc.sh

      #export SCM_TMPDIR=/tmp
      #export DIRAC_TMPDIR=/zfs/hybrilit.jinr.ru/user/m/milias/scratch

      echo -e "Disk space for AMS: df -h $SCM_TMPDIR "; df -h $SCM_TMPDIR

     # export ADFBIN=$ADFHOME/bin
     # export AMSBIN=$AMSHOME/bin
     # export ADFRESOURCES=$AMSHOME/atomicdata
      #export SCM_OPENGL1_FALLBACK=1
      #unset SCM_OPENGL1_FALLBACK
      #export  SCM_OPENGL_SOFTWARE=1
      #extend_string LD_LIBRARY_PATH  /home/milias/Work/software/ams/opengl_lib/fallback  $LD_LIBRARY_PATH
      echo 'AMS environmental variables activated on' $THIS_HOSTNAME
      echo 'AMS AMSHOME='$AMSHOME
      echo 'AMS AMSBIN='$AMSBIN
      echo 'AMS SCM_TMPDIR='$SCM_TMPDIR
      echo 'AMS SCMLICENSE='$SCMLICENSE
     # echo '/home/milias/Work/software/ams/opengl_lib/fallback SCM_OPENGL1_FALLBACK='$SCM_OPENGL1_FALLBACK
      echo 'SCM_OPENGL_SOFTWARE='$SCM_OPENGL_SOFTWARE

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"

# CPU model, total numer of CPUs, number of allocated CPUs
echo -e "The node's CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo "This node has total $NPROCS CPUs available for an EXCLUSIVE job."
#echo "This node has $SLURM_CPUS_ON_NODE CPUs allocated for SLURM calculations."

echo -e "\n The total memory at the node (in GB)"
free -t -g
echo -e "\n"

echo -e "\nldd $AMSHOME/bin/ams.exe:"
ldd  $AMSHOME/bin/ams.exe

#echo -e "\n ifort -V: \c"; ifort -V

echo -e "\nMy PATH=$PATH\n"
echo -e "Python -v :\c"; python -V
echo -e "\n mpirun ? \c"; which mpirun; mpirun --version
echo

# for running jobs from your homedir
#cd $SLURM_SUBMIT_DIR

#$AMSHOME/Utils/run_test dftb
#$AMSHOME/Utils/run_test band
#$AMSHOME/Utils/run_test reaxff

export NSCM=$SLURM_CPUS_ON_NODE
#echo -e "\n Running  $ADFBIN/ams.exe $AMSHOME/examples/band/NaCl/NaCl.run  with NSCM=$SLURM_CPUS_ON_NODE"
#$ADFBIN/ams.exe $AMSHOME/examples/band/NaCl/NaCl.run

#
# In both cases you might need to edit the $AMSBIN/start script around line 287, to change the location pointed to by the "I_MPI_PMI_LIBRARY" variable. 
#

echo -e "\n Running $SLURM_SUBMIT_DIR/TlOH.band-revpbe_srzora_tzp_sc.run  : "
  $SLURM_SUBMIT_DIR/TlOH.band-revpbe_srzora_tzp_sc.run

exit
