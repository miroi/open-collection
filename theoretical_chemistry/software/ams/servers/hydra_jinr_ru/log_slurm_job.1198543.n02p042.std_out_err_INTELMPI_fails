Job user is milias and his job intelmpiams has assigned ID 1198543
This job was submitted from the computer space13.hydra.local
and from the home directory:
/zfs/store5.hydra.local/user/m/milias/work/projects/open-collection/theoretical_chemistry/software_runs/ams/servers/hydra_jinr_ru

It is running on the cluster compute node:
gvr
and is employing 1 node/nodes:
n02p042

Job partition is cascade 

The job requests 4 CPU per task.
modules at disposal:

-------------- /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/modulefiles --------------
ABINIT/v8.10.3_intel2018              GEANT4/v4.10.07.p01_gcc910
AmberTools/v20                        Ginac/v1.7.3-1
AMS/v2021.102                         GROMACS/v2019.3
BASE/1.0                              GROMACS/v5.1.3_gcc485_cuda80
CLN/v1.3.4-1                          GSL/v1.16-1
CMake/v3.16.5                         GSL/v2.6
CMake/v3.19.1                         GVR/v1.0-1
COMSOL/v5.6                           intel/v2018.1.163-9
cuda/v10.0-1                          intel/v2019.3.199
cuda/v10.1-1                          intel/v2021.1
cuda/v8.0-1                           intel-qs/v20-07-14
cuda/v9.2                             intel-qs/v21-01-14
DIRAC/v19.0_intel2018                 java/v8u181
ELPA/v2020.05.001_intel2018_python365 java/v8u91-1
EMACS/v25.3                           LAMMPS/v12.12.18
EOS/v1.0                              LAPACK/v3.9.0
FairRoot/oct17p1                      libpqxx/v7.0_gcc910
FairRoot/v16.06_gcc485                Maple/v2020.2
FairRoot/v18.0.4                      Mathematica/v11.2-1
FairRoot/v18.2.0_gcc485               MATLAB/R2020b
FairRoot/v18.2.1_gcc485               ndm-lite/v1.0
FairSoft/june19p1_gcc485              opencv/v4.1.0
FairSoft/june19p2_gcc485              openmpi/v1.8.8-1
FairSoft/may16p1_gcc485               openmpi/v2.1.2-2
FairSoft/may18p1                      openmpi/v3.1.2
FairSoft/oct17p1                      openmpi/v3.1.3
fftw/v3.3.7-2                         openmpi/v3.1.3_psm2
fftw/v3.3.7-5                         PandaRoot/dec17p2b
FLAIR/v2.3.0                          PandaRoot/may19
FLUKA/v2011.2x-8                      PostgreSQL/v12.1_gcc910
FLUKA/v2020.0.3                       protobuf/v3.11.3
gcc/v10.2.0                           Python/v2.7.10-3
gcc/v11.2.0                           Python/v3.6.5
gcc/v4.9.3-1                          quantum-espresso/v6.4.1
gcc/v5.3.0-1                          reduce-algebra/svn-4830
gcc/v6.2.0-2                          ROOT/v6-18-00
gcc/v7.2.0-1                          SMASH_gcc485/v1.8
gcc/v8.2.0-1                          TRNG/v4.24_gcc102
gcc/v8.3.0                            Valgrind/v3.16.1_gcc485
gcc/v9.1.0-1                          zlib/v1.2.11-1



 loaded modules:
Currently Loaded Modulefiles:
  1) intel/v2021.1
Disk space: df -h /tmp 
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda3       216G  7.5G  198G   4% /
ADF environmental variables activated on
ADF AMSHOME=/zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.intelmpi+StaticMKL/ams2021.103
ADF SCM_TMPDIR=/tmp
ADF SCMLICENSE=/zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.intelmpi+StaticMKL/ams2021.103/license.txt
SCM_OPENGL_SOFTWARE=1

Running on host n02p042.gvr.local
Time is Tue Aug 24 22:50:06 MSK 2021 

The node's CPU model name	: Intel(R) Xeon(R) Platinum 8268 CPU @ 2.90GHz
This node has total 96 CPUs available for anEXCLUSIVE job.

 The total memory at the node (in GB)
              total        used        free      shared  buff/cache   available
Mem:            187           9          90           0          87         177
Swap:             3           0           3
Total:          191           9          94



ldd /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.intelmpi+StaticMKL/ams2021.103/bin/ams.exe:
	linux-vdso.so.1 =>  (0x00007ffe1f30e000)
	libprimme.so => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.intelmpi+StaticMKL/ams2021.103/bin/lib/libprimme.so (0x00002b37b98d4000)
	libplumed.so.2.5.0 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.intelmpi+StaticMKL/ams2021.103/bin/lib/libplumed.so.2.5.0 (0x00002b37b9af2000)
	libmpi.so.12 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mpi/2021.1.1//lib/release/libmpi.so.12 (0x00002b37b9cfd000)
	libmpifort.so.12 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mpi/2021.1.1//lib/libmpifort.so.12 (0x00002b37bb076000)
	librt.so.1 => /lib64/librt.so.1 (0x00002b37bb434000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x00002b37bb63c000)
	libdl.so.2 => /lib64/libdl.so.2 (0x00002b37bb858000)
	libstdc++.so.6 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.intelmpi+StaticMKL/ams2021.103/bin/lib/libstdc++.so.6 (0x00002b37bba5c000)
	libresolv.so.2 => /lib64/libresolv.so.2 (0x00002b37bbe3b000)
	libiomp5.so => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.intelmpi+StaticMKL/ams2021.103/bin/lib/libiomp5.so (0x00002b37bc054000)
	libc.so.6 => /lib64/libc.so.6 (0x00002b37bc3f6000)
	/lib64/ld-linux-x86-64.so.2 (0x00002b37b96b0000)
	libgcc_s.so.1 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.intelmpi+StaticMKL/ams2021.103/bin/lib/libgcc_s.so.1 (0x00002b37bc7c4000)
	libplumedKernel.so.2.5.0 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.intelmpi+StaticMKL/ams2021.103/bin/lib/libplumedKernel.so.2.5.0 (0x00002b37bc9dc000)
	libm.so.6 => /lib64/libm.so.6 (0x00002b37be1f9000)
	libfabric.so.1 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mpi/2021.1.1//libfabric/lib/libfabric.so.1 (0x00002b37be4fb000)
	libz.so.1 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.intelmpi+StaticMKL/ams2021.103/bin/lib/../zlib/lib/libz.so.1 (0x00002b37be741000)

My PATH=/zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.intelmpi+StaticMKL/ams2021.103/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/dev-utilities/2021.1.1/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/intelpython/latest/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/intelpython/latest/condabin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/clck/2021.1.1/bin/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mkl/latest/bin/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/debugger/10.0.0/gdb/intel64/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/vpl/2021.1.1/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/compiler/2021.1.1/linux/bin/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/compiler/2021.1.1/linux/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/compiler/2021.1.1/linux/ioc/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/advisor/2021.1.1/bin64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/inspector/2021.1.1/bin64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mpi/2021.1.1/libfabric/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mpi/2021.1.1/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/itac/2021.1.1/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/vtune/2021.1.1/bin64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/dpcpp-ct/2021.1.1/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:.:/zfs/hybrilit.jinr.ru/user/m/milias/.local/bin:/zfs/hybrilit.jinr.ru/user/m/milias/bin

Python -v :Python 3.7.9 :: Intel Corporation

 mpirun ? /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/intelpython/latest/bin/mpirun
Intel(R) MPI Library for Linux* OS, Version 2021.1 Build 20201112 (id: b9c9d2fc5)
Copyright 2003-2020, Intel Corporation.


 Running /zfs/store5.hydra.local/user/m/milias/work/projects/open-collection/theoretical_chemistry/software_runs/ams/servers/hydra_jinr_ru/TlOH.band-revpbe_srzora_tzp_sc.run  : 
Fatal error in MPI_Init: Other MPI error, error stack:
MPIR_Init_thread(805).....: fail failed
MPID_Init(1859)...........: channel initialization failed
MPIDI_CH3_Init(126).......: fail failed
MPID_nem_init_ckpt(857)...: fail failed
MPIDI_CH3I_Seg_commit(427): PMI_KVS_Get returned -1
Fatal error in MPI_Init: Other MPI error, error stack:
MPIR_Init_thread(805).....: fail failed
MPID_Init(1859)...........: channel initialization failed
MPIDI_CH3_Init(126).......: fail failed
MPID_nem_init_ckpt(857)...: fail failed
MPIDI_CH3I_Seg_commit(427): PMI_KVS_Get returned -1
Fatal error in MPI_Init: Other MPI error, error stack:
MPIR_Init_thread(805).....: fail failed
MPID_Init(1859)...........: channel initialization failed
MPIDI_CH3_Init(126).......: fail failed
MPID_nem_init_ckpt(857)...: fail failed
MPIDI_CH3I_Seg_commit(427): PMI_KVS_Get returned -1
[unset]: readline failed
[unset]: readline failed
[unset]: readline failed
srun: error: n02p042: tasks 1-3: Exited with exit code 15
srun: got SIGCONT
slurmstepd: error: *** JOB 1198543 ON n02p042 CANCELLED AT 2021-08-24T22:51:40 ***
slurmstepd: error: *** STEP 1198543.0 ON n02p042 CANCELLED AT 2021-08-24T22:51:41 ***
 
 ============
 Timer stack:
 ------------
  1629834608.50  AMS
  1629834608.51  ppinit
 ============

Process received SIGTERM
srun: forcing job termination
srun: Job step aborted: Waiting up to 122 seconds for job step to finish.
