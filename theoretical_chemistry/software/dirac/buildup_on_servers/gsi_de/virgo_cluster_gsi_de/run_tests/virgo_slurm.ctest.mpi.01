#!/bin/bash

#SBATCH -J DIRAC

## max. execution time
##SBATCH -t 4-00:00:00
#SBATCH -t 0-04:00:00

# number of nodes
#SBATCH -N 1
##SBATCH --exclusive

##SBATCH --mem=120GB
#SBATCH --mem=16GB

# do not restart in the case of nodefail!
#SBATCH --no-requeue
#SBATCH --no-kill

## CPU count  - max. 40 per node ?
#SBATCH -n 8

##  partition (queue)
#SBATCH -p main

## memory NO !!!
##SBATCH --mem-per-cpu=28G

## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

## mail
#SBATCH --mail-user=M.Ilias@gsi.de 
#SBATCH --mail-type=ALL

 
echo -e "\nJob user is SLURM_JOB_USER= $SLURM_JOB_USER"
echo -e "User job SLURM_JOB_NAME=$SLURM_JOB_NAME has assigned ID SLURM_JOBID=$SLURM_JOBID"

echo -e "This job was submitted from the computer SLURM_SUBMIT_HOST=$SLURM_SUBMIT_HOST"
echo -e " and from the home directory SLURM_SUBMIT_DIR=$SLURM_SUBMIT_DIR"

echo -e "Job is running on the cluster compute node: SLURM_CLUSTER_NAME=$SLURM_CLUSTER_NAME"
echo -e "and is employing SLURM_JOB_NUM_NODES=$SLURM_JOB_NUM_NODES node/nodes:"
echo -e "SLURM_JOB_NODELIST = $SLURM_JOB_NODELIST"
echo
echo -e "Job partition is SLURM_JOB_PARTITION=$SLURM_JOB_PARTITION \n"
echo -e "\nThe job requests SLURM_CPUS_ON_NODE=$SLURM_CPUS_ON_NODE CPU per task.\n"

spack load cmake@3.21.4 target=x86_64
spack load hdf5@1.10.7 target=x86_64
spack load intel-parallel-studio@professional.2020.1 target=x86_64
spack load intel-mkl@2020.4.304 target=x86_64
spack unload openmpi
spack find --loaded

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"

# CPU model, total numer of CPUs, number of allocated CPUs
echo -e "The node's CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo "This node has total $NPROCS CPUs available for an EXCLUSIVE job."
#echo "This node has $SLURM_CPUS_ON_NODE CPUs allocated for SLURM calculations."
echo "This node has $SLURM_NPROCS CPUs allocated for SLURM calculations."

echo -e "\n The memory at the node (in GB)"
free -t -g
echo -e "\n"

# my OpenMPI
export PATH=/lustre/ukt/milias/bin/openmpi-4.1.4-intel2020-i8/openmpi-4.1.4/bin:$PATH
export MANPATH=/lustre/ukt/milias/bin/openmpi-4.1.4-intel2020-i8/openmpi-4.1.4/share/man:$MANPATH
export LD_LIBRARY_PATH=/lustre/ukt/milias/bin/openmpi-4.1.4-intel2020-i8/openmpi-4.1.4/lib:$LD_LIBRARY_PATH


## set internal OpenMP parallelization for MKL - Slurm CPU count
#export MKL_NUM_THREADS=$SLURM_CPUS_ON_NODE
#export MKL_NUM_THREADS=24
#export MKL_NUM_THREADS=$NPROCS
#echo -e  "\nInternal MKL parallelization MKL_NUM_THREADS=$MKL_NUM_THREADS\n"

export OMP_NUM_THREADS=1
export MKL_DYNAMIC="FALSE"
export OMP_DYNAMIC="FALSE"

export DIRAC_ROOTDIR=/lustre/ukt/milias/work/software/dirac/devel_trunk
export DIRAC_BUILDDIR=$DIRAC_ROOTDIR/build_openmpi4.1.4_intel20mkl_i8
#
export PAM=$DIRAC_BUILDDIR/pam
export BASDIR=$DIRAC_ROOTDIR/basis_dalton:$DIRAC_ROOTDIR/basis:$DIRAC_ROOTDIR/basis_ecp

echo -e "\n ifort -V: \c"; which ifort; ifort -V
echo -e "Intel MKL library ? MKLROOT=$MKLROOT"
echo -e "mpif90 -V: \c "; which mpif90; mpif90 -V
echo -e "mpicc -V: \c "; mpicc -V
echo -e "mpicxx -V: \c "; mpicxx -V
echo -e "Python -v :\c"; python -V
echo -e "cmake ? \c"; which cmake; cmake  --version
echo -e "ctest ? \c"; which ctest; ctest  --version
echo -e "\n mpirun ? \c"; which mpirun; mpirun --version

echo -e "\nMy PATH=$PATH\n"

echo -e "\nldd $DIRAC_BUILDDIR/dirac.x:"
ldd $DIRAC_BUILDDIR/dirac.x

# project dir
export DIRAC_TMPDIR=/lustre/ukt/milias/scratch

echo -e "\nDIRAC scratch directory space, $DIRAC_TMPDIR"
df -h $DIRAC_TMPDIR/.
echo -e "\n For comparison, /tmp local disk;  df -h /tmp/."; df -h /tmp
echo -e "\n"

# for running jobs from your homedir
#cd $SLURM_SUBMIT_DIR

#THISMPI=10
#THISMPI=4
#let "NUMTHR=$NPROCS/$THISMPI"
#export MKL_NUM_THREADS=$NUMTHR
export MKL_NUM_THREADS=1
#echo -e "\nThis node has $NPROCS CPUs available for this EXCLUSIVE JOB and dirac.x is running via $THISMPI threads."
#echo -e "Therefore, for the MKL internal parallelization, number of calculated threads=$MKL_NUM_THREADS \n"
#
#$PAM --noarch --mpi=$SLURM_CPUS_ON_NODE --gb=47.0 --ag=47.8 --mol=Cf2.xyz  --inp=Cf2.x2c-a4p.scf_occ98_98_mj1_v3z.inp --suffix=i17mkl_mpi$SLURM_CPUS_ON_NODE-$SLURM_JOB_PARTITION-$SLURM_JOBID-out  --get="DFPCMO=DFPCMO.Cf2.lsym_x2c-a4p_scf_v3z_occ98_98_mj1"  
#
#$PAM --noarch --mpi=$SLURM_CPUS_ON_NODE --gb=2.0 --ag=2.2 --mol=$DIRAC_ROOTDIR/test/cc_energy_and_mp2_dipole/H2O.mol --inp=$DIRAC_ROOTDIR/test/cc_energy_and_mp2_dipole/ccsd.inp
#
#$PAM --noarch --mpi=4 --gb=2.0 --ag=2.2 --mol=$DIRAC_ROOTDIR/test/cc_energy_and_mp2_dipole/H2O.mol --inp=$DIRAC_ROOTDIR/test/cc_energy_and_mp2_dipole/ccsd.inp
#
#

unset DIRAC_MPI_COMMAND
export DIRAC_MPI_COMMAND="mpirun -np 2"
echo -e "\nDIRAC_MPI_COMMAND=$DIRAC_MPI_COMMAND"

let "NUMTHR=$SLURM_NPROCS/2"; echo "SLURM_NPROCS=$SLURM_NPROCS, then divided by 2,  NUMTHR=$NUMTHR"

cd $DIRAC_BUILDDIR; echo -e "\nI am in: \c"; pwd
echo -e "\nRunning 'ctest -VV -j$NUMTHR'"
#ctest -VV  -j$NUMTHR -L short
ctest -VV  -j$NUMTHR 

exit
