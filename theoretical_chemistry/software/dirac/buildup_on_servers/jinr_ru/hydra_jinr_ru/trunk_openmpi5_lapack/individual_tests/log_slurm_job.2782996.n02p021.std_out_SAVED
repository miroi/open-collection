
Running on host n02p021
Time is Mon Sep  9 00:32:00 MSK 2024 


 The total memory at the node (in GB)
              total        used        free      shared  buff/cache   available
Mem:            187          23         138           0          25         163
Swap:             3           0           3
Total:          191          23         142


 Job user is milias and his job cc2 has assigned ID 2782996
This job was submitted from the computer SLURM_SUBMIT_HOST=space06.hydra.local

This job was submitted from the home directory:
SLURM_SUBMIT_DIR=/lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/buildup_on_servers/jinr_ru/hydra_jinr_ru/trunk_openmpi5_lapack/individual_tests

This job is running on the cluster compute node:
SLURM_CLUSTER_NAME=gvr

This job is employing SLURM_JOB_NUM_NODES=4 node/nodes:
SLURM_JOB_NODELIST = n02p[021-024]

Job's partition is  SLURM_JOB_PARTITION=cascade

This job reserved SLURM_CPUS_ON_NODE=4 threads per node.
This job wants, in total, SLURM_NTASKS=16 threads . 


The master's node CPU model name	: Intel(R) Xeon(R) Platinum 8268 CPU @ 2.90GHz
This node has total NPROCS=96 CPUs available for an master EXCLUSIVE job.


 Generated machinefile for MPI, MACHINEFILE=nodes.cascade.2782996
-rw-r--r-- 1 milias hybrilit 128 Sep  9 00:32 /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/buildup_on_servers/jinr_ru/hydra_jinr_ru/trunk_openmpi5_lapack/individual_tests/nodes.cascade.2782996
MACHINEFILE contains these nodes:
n02p021	n02p021	n02p021	n02p021	n02p022	n02p022	n02p022	n02p022	n02p023	n02p023	n02p023	n02p023	n02p024	n02p024	n02p024	n02p024


 Loaded modules:
Currently Loaded Modulefiles:
  1) GVR/v1.0-1               5) gcc/v12.3.0
  2) BASE/1.0                 6) openmpi/v5.0.3_gcc1230
  3) Python/v3.10.2           7) LAPACK/v3.12.0_gcc1230
  4) CMake/v3.29.2

 python ? :/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/Python/v3.10.2/bin/python
Python 3.10.2
 cmake ? :/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/CMake/v3.29.2/bin/cmake
cmake version 3.29.2

CMake suite maintained and supported by Kitware (kitware.com/cmake).
 mpif90 ? :/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/bin/mpif90
GNU Fortran (GCC) 12.3.0
Copyright (C) 2022 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

 mpicc ? :/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/bin/mpicc
gcc (GCC) 12.3.0
Copyright (C) 2022 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

 mpicxx ? :/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/bin/mpicxx
g++ (GCC) 12.3.0
Copyright (C) 2022 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

 mpirun ? :/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/bin/mpirun
mpirun (Open MPI) 5.0.3

Report bugs to https://www.open-mpi.org/community/help/

 LAPACK and  BLAS libs in /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/LAPACK/v3.12.0_gcc1230/lib64 ?
total 8115
drwxr-xr-x 4 cvmfs cvmfs    4096 Jun  6 14:27 cmake
lrwxrwxrwx 1 cvmfs cvmfs      12 Jun  6 14:27 libblas.so -> libblas.so.3
lrwxrwxrwx 1 cvmfs cvmfs      17 Jun  6 14:27 libblas.so.3 -> libblas.so.3.11.0
lrwxrwxrwx 1 cvmfs cvmfs      13 Jun  6 14:27 libcblas.so -> libcblas.so.3
lrwxrwxrwx 1 cvmfs cvmfs      18 Jun  6 14:27 libcblas.so.3 -> libcblas.so.3.11.0
lrwxrwxrwx 1 cvmfs cvmfs      14 Jun  6 14:27 liblapack.so -> liblapack.so.3
lrwxrwxrwx 1 cvmfs cvmfs      19 Jun  6 14:27 liblapack.so.3 -> liblapack.so.3.11.0
drwxr-xr-x 2 cvmfs cvmfs    4096 Jun  6 14:27 pkgconfig
-rwxr-xr-x 1 cvmfs cvmfs 7361824 Jun  6 14:27 liblapack.so.3.11.0
-rwxr-xr-x 1 cvmfs cvmfs  248720 Jun  6 14:21 libcblas.so.3.11.0
-rwxr-xr-x 1 cvmfs cvmfs  685888 Jun  6 14:21 libblas.so.3.11.0


 The variable PATH=/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/gcc/v12.3.0/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/CMake/v3.29.2/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/Python/v3.10.2/bin:/lustre/home/user/m/milias/work/software/wien2k/wien2k_23.2_intelserial_mkl:/lustre/home/user/m/milias/work/software/wien2k/wien2k_23.2_intelserial_mkl/SRC_structeditor/bin:/lustre/home/user/m/milias/work/software/wien2k/wien2k_23.2_intelserial_mkl/SRC_IRelast/script-elastic:/lustre/home/user/m/milias/work/software/ams/linux.openmpi/ams2021.107/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:.:/lustre/home/user/m/milias/work/software/wien2k/wien2k_23.2_intelserial_mkl:.:/lustre/home/user/m/milias/.local/bin:/lustre/home/user/m/milias/bin



 The variable LD_LIBRARY_PATH=/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/LAPACK/v3.12.0_gcc1230/lib64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/LAPACK/v3.12.0_gcc1230/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/gcc/v12.3.0/lib64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/Python/v3.10.2/lib:.....



 checking the workhorse executable, ldd /lustre/home/user/m/milias/work/software/dirac/trunk/build_openmpi5_lapack_blas_i4/dirac.x:
	linux-vdso.so.1 =>  (0x00007ffd231c9000)
	liblapack.so.3 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/LAPACK/v3.12.0_gcc1230/lib64/liblapack.so.3 (0x00002b6746c5b000)
	libblas.so.3 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/LAPACK/v3.12.0_gcc1230/lib64/libblas.so.3 (0x00002b674753f000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x00002b67477e2000)
	libmpi_usempif08.so.40 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib/libmpi_usempif08.so.40 (0x00002b67479fe000)
	libmpi_usempi_ignore_tkr.so.40 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib/libmpi_usempi_ignore_tkr.so.40 (0x00002b6747c3e000)
	libmpi_mpifh.so.40 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib/libmpi_mpifh.so.40 (0x00002b6747e4e000)
	libmpi.so.40 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib/libmpi.so.40 (0x00002b67480b9000)
	libgfortran.so.5 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/gcc/v12.3.0/lib64/libgfortran.so.5 (0x00002b6748616000)
	libm.so.6 => /lib64/libm.so.6 (0x00002b6748adc000)
	libgomp.so.1 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/gcc/v12.3.0/lib64/libgomp.so.1 (0x00002b6748dde000)
	libgcc_s.so.1 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/gcc/v12.3.0/lib64/libgcc_s.so.1 (0x00002b6749023000)
	libquadmath.so.0 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/gcc/v12.3.0/lib64/libquadmath.so.0 (0x00002b6749241000)
	libc.so.6 => /lib64/libc.so.6 (0x00002b6749486000)
	/lib64/ld-linux-x86-64.so.2 (0x00002b6746a37000)
	libopen-pal.so.80 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib/libopen-pal.so.80 (0x00002b6749854000)
	librt.so.1 => /lib64/librt.so.1 (0x00002b6749b5d000)
	libpmix.so.2 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib/libpmix.so.2 (0x00002b6749d65000)
	libutil.so.1 => /lib64/libutil.so.1 (0x00002b674a194000)
	libevent_core-2.1.so.7 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib/libevent_core-2.1.so.7 (0x00002b674a397000)
	libevent_pthreads-2.1.so.7 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib/libevent_pthreads-2.1.so.7 (0x00002b674a5cc000)
	libhwloc.so.15 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib/libhwloc.so.15 (0x00002b674a7cf000)
	libdl.so.2 => /lib64/libdl.so.2 (0x00002b674aa28000)

 checking pam script, /lustre/home/user/m/milias/work/software/dirac/trunk/build_openmpi5_lapack_blas_i4/pam --help  :
Usage: pam [options] --inp=*.inp --mol=*.mol [--pcm=*.pcm] [--pot=*.pot]
   or: pam [options] --inp=*.inp --mol=*.xyz [--pcm=*.pcm] [--pot=*.pot]
   or: pam [options] --fullrestart=ARCHIVE.tgz [--inp=*.inp] [--mol=*.mol | --mol=*.xyz] [--pcm=*.pcm] [--pot=*.pot]
Options are also read from ~/.diracrc

Options:
  -h, --help            show this help message and exit

  Input files - mandatory specifications:
    --inp=INP_FILE      Dirac input file containing the job directives [*.inp]
    --mol=MOL_FILE      file containing the basis set and geometry
                        specifications [*.mol or *.xyz]
    --pcm=PCM_FILE      PCMSolver input file [*.pcm]
    --pot=POT_FILE      file containing the polarizable embedding potential
                        parameters [*.pot]
    --fullrestart=ARCHIVE.tgz
                        restart from files within ARCHIVE.tgz;
                        old input files from ARCHIVE.tgz are reused unless new
                        input files are provided with one or more of "--inp",
                        "--mol", "--pcm", and "--pot".

  Output files:
    --suffix=STRING     file suffix for output [default: out]
    --rsh=STRING        Remote shell program, typically rsh or ssh [default:
                        ssh or taken from .dirarc]
    --rcp=STRING        Remote shell program, typically rcp or scp [default:
                        scp or taken from .dirarc]
    --noh5              do not create a h5 acheckpoint file [default: create
                        it]
    --noarch            do not create a tgz archive [default: create it]
    --nobackup          do not backup old outputs [default: back them up]

  Memory specification:
    --mw=INTEGER        set max memory (in megawords) for WORK array
                        for the master (for each MPI thread) [default: 64]
    --mb=INTEGER        set max memory (in MB) for WORK array
                        for the master (for each MPI thread) [default: none]
    --gb=FLOAT          set max memory (in GB) for WORK array
                        for the master (for each MPI thread) [default: none]
    --nw=INTEGER        set max memory (in megawords) for WORK array
                        for the co-workers (for each MPI thread) [default:
                        --mw set value]
    --aw=INTEGER        set max dynamically allocatable memory (in megawords)
                        (for each MPI thread) [default: 0]
    --ag=FLOAT          set max dynamically allocatable memory (in gigabytes)
                        (for each MPI thread) [default: 0]

  MPI settings:
    --mpi=INTEGER       set number of MPI processes in parallel run [default:
                        1]
    --mpirun=STRING     set the MPI run command, can have own arguments
                        [default: see "pam --show" output]
    --mpiarg=STRING     additonal options passed on to the MPI launcher;
                        useful arguments could be:
                        -envall (for MPICH2, Intel MPI)
                        -x "PATH LD_LIBRARY_PATH" (for Open MPI)
                        which will allow the user to pass the present
                        environment settings from the head node to the scratch
                        nodes.
    --mpi-workdir-argument=STRING
                        option to explicitly pass the scratch directory name
                        on to the slaves
                        ("wd" for MPICH2/Intel MPI, "wdir" for Open MPI).
    --machfile=STRING   specify a machinefile for a parallel run [default:
                        none]

  Managing data/file transfer:
    --put="file1 file2 my_file* ...", --copy="file1 file2 my_file* ..."
                        copy files to scratch directory
    --distribute="file1 file2 my_file*..."
                        copy selected files to scratch directories of working
                        nodes in parallel run
    --get="file1 file2 my_file*..."
                        get files from scratch directory
    --env="env_var1=value1 env_var2=value2 ..."
                        set environment variables
    --replace=NAME=VALUE
                        replace NAME by VALUE in the mol and inp files
    --incmo             copy CHECKPOINT to scratch directory [default: False]
    --outcmo            get CHECKPOINT from scratch directory [default: False]
    --inkrmc            copy KRMCSCF to scratch directory as KRMCOLD [default:
                        False]
    --outkrmc           get KRMCSCF and KRMCOLD from scratch directory
                        [default: False]
    --outqforce         get qforce.*.log from scratch directory [default:
                        False]
    --keep_scratch      keep the scratch directory after calculation [default:
                        delete it]

  Modify default paths and settings:
    --scratch=FULL_PATH
                        full path to scratch directory (DIRAC will append a
                        subdirectory to make it unique) [default read from
                        ~/.diracrc]
    --scratchfull=FULL_PATH
                        full path to scratch directory (DIRAC will not append
                        anything to this path; do NOT choose your /home/user
                        as scratchfull) [default: script defined full path
                        upon ~/.diracrc]
    --basis=FULL_PATH   basis set directory [default: .:/lustre/home/user/m/mi
                        lias/work/software/dirac/trunk/build_openmpi5_lapack_b
                        las_i4/basis:/lustre/home/user/m/milias/work/software/
                        dirac/trunk/build_openmpi5_lapack_blas_i4/basis_dalton
                        :/lustre/home/user/m/milias/work/software/dirac/trunk/
                        build_openmpi5_lapack_blas_i4/basis_ecp]
    --dirac=FULL_PATH   dirac executable [default: /lustre/home/user/m/milias/
                        work/software/dirac/trunk/build_openmpi5_lapack_blas_i
                        4/dirac.x]

  Advanced options for programming and debugging:
    --show              print current settings (including those from .diracrc)
                        and exit
    --silent            run pam silent with minimal output
    --debug             run in debugger [default: False]
    --summit            run with summits jsrun [default: False]
    --debugger=FULL_PATH
                        set full path to debugger [default: in .diracrc]
    --valgrind          run with valgrind [default: False]
    --profile           perform profiling [default: False]
    --profiler=FULL_PATH
                        set full path to profiler [default: none]
    --timeout=TIME_STRING
                        limit dirac.x execution time (in #d#h#m#s, or in pure
                        seconds-integer) [default: no limit; reads env.var.
                        DIRTIMEOUT]

 DIRAC_MPI_COMMAND=mpirun --bind-to :overload-allowed  --machinefile  /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/buildup_on_servers/jinr_ru/hydra_jinr_ru/trunk_openmpi5_lapack/individual_tests/nodes.cascade.2782996 -np 16


 I am in the directory: /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/buildup_on_servers/jinr_ru/hydra_jinr_ru/trunk_openmpi5_lapack/individual_tests


Running job from DIRAC test directory: /lustre/home/user/m/milias/work/software/dirac/trunk/test/benchmark_cc
DIRAC input file=cc.inp ;  mol file=C2H4Cl2_sta_c1.mol
suffix for the DIRAC output file = out.gvr.cascade.N4.n16.jid2782996.out



  DIRAC pam script running:

  user           : milias
  machine        : n02p021
  date and time  : 2024-09-09 00:32:02.882096
  input dir      : /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/buildup_on_servers/jinr_ru/hydra_jinr_ru/trunk_openmpi5_lapack/individual_tests
  pam command    : /lustre/home/user/m/milias/work/software/dirac/trunk/build_openmpi5_lapack_blas_i4/pam
  all pam args   : ['--noarch', '--gb=16', '--ag=18', '--inp=/lustre/home/user/m/milias/work/software/dirac/trunk/test/benchmark_cc/cc.inp', '--mol=/lustre/home/user/m/milias/work/software/dirac/trunk/test/benchmark_cc/C2H4Cl2_sta_c1.mol', '--suffix=out.gvr.cascade.N4.n16.jid2782996.out']
  executable     : /lustre/home/user/m/milias/work/software/dirac/trunk/build_openmpi5_lapack_blas_i4/dirac.x
  scratch dir    : /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_cc_C2H4Cl2_sta_c1_273342 (availspace=116562.135[GB])
  output file    : cc_C2H4Cl2_sta_c1.out.gvr.cascade.N4.n16.jid2782996.out
  DIRAC run      : parallel (launcher: /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/bin/mpirun)
  local disks    : False
  rsh/rcp        : ssh / scp
  machine file   : None

  Setting MKL and OPENMP environment to default values (if not set already)
   MKL_NUM_THREADS = 1
   MKL_DYNAMIC = "FALSE"
   Variabl.OMP_NUM_THREADS =  1
   OMP_DYNAMIC="FALSE"
  Setting environment variables (as specified explicitly or in .diracrc)

  Creating the scratch directory.
  Copying /lustre/home/user/m/milias/work/software/dirac/trunk/build_openmpi5_lapack_blas_i4/dirac.x to /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_cc_C2H4Cl2_sta_c1_273342/dirac.x
  Copying /lustre/home/user/m/milias/work/software/dirac/trunk/test/benchmark_cc/C2H4Cl2_sta_c1.mol to /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_cc_C2H4Cl2_sta_c1_273342/MOLECULE.MOL
  Copying /lustre/home/user/m/milias/work/software/dirac/trunk/test/benchmark_cc/cc.inp to /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_cc_C2H4Cl2_sta_c1_273342/DIRAC.INP
  Basis set libraries (default)    : /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/buildup_on_servers/jinr_ru/hydra_jinr_ru/trunk_openmpi5_lapack/individual_tests
                        /lustre/home/user/m/milias/work/software/dirac/trunk/build_openmpi5_lapack_blas_i4/basis
                        /lustre/home/user/m/milias/work/software/dirac/trunk/build_openmpi5_lapack_blas_i4/basis_dalton
                        /lustre/home/user/m/milias/work/software/dirac/trunk/build_openmpi5_lapack_blas_i4/basis_ecp
  DIRAC command  : mpirun --bind-to :overload-allowed  --machinefile  /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/buildup_on_servers/jinr_ru/hydra_jinr_ru/trunk_openmpi5_lapack/individual_tests/nodes.cascade.2782996 -np 16 /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_cc_C2H4Cl2_sta_c1_273342/dirac.x (PID=273343)
  content of the (master) scratch directory
  ------------------------------------------------------------------------------
                     name    size (MB)    last accessed
  ------------------------------------------------------------------------------
                  ft43.12     75.840   09/09/2024 01:52:21 AM
                  ft45. 5     30.208   09/09/2024 01:52:23 AM
                 MCCRES.1     75.928   09/09/2024 03:58:03 AM
                  ft41.15      7.728   09/09/2024 01:49:26 AM
                  ft41. 1      7.728   09/09/2024 01:49:23 AM
                  ft45.11     30.272   09/09/2024 01:52:23 AM
                  MRCONEE      0.384   09/09/2024 12:54:22 AM
            RELCCSD.OUT.1      0.009   09/09/2024 12:47:48 AM
           RELCCSD.OUT.14      0.009   09/09/2024 12:47:48 AM
                MCCRES.14     75.928   09/09/2024 03:58:03 AM
                   DFFOCK      7.938   09/09/2024 12:34:11 AM
                  ft43. 6     75.840   09/09/2024 01:52:22 AM
                   DFCYCL      0.002   09/09/2024 12:34:11 AM
                  ft42. 6     49.264   09/09/2024 01:52:22 AM
                  ft40. 1      0.336   09/09/2024 12:54:24 AM
               MDCINXXX14      0.000   09/09/2024 12:47:49 AM
                  ft44.11    314.176   09/09/2024 01:49:22 AM
                  ft44. 5    314.176   09/09/2024 01:49:25 AM
                  ft40.15      0.336   09/09/2024 12:54:24 AM
               MDCINXXXX8      0.000   09/09/2024 12:47:48 AM
                  ft46. 8     46.496   09/09/2024 01:51:25 AM
                  ft42.12     49.264   09/09/2024 01:52:21 AM
                  ft46. 2     46.496   09/09/2024 01:51:31 AM
                  MNF.INP      0.001   09/09/2024 12:32:04 AM
               MDCINXXXX2      0.000   09/09/2024 12:47:48 AM
        schema_labels.txt      0.017   09/09/2024 12:32:04 AM
                  ft42.15     49.264   09/09/2024 01:52:21 AM
                  ft46. 5     46.496   09/09/2024 01:51:29 AM
               MDCINXXXX5      0.000   09/09/2024 12:47:49 AM
                   MDCINT   3434.383   09/09/2024 12:54:22 AM
                  ft44. 2    314.176   09/09/2024 01:49:23 AM
             interface_ao      0.025   09/09/2024 12:32:07 AM
                  ft44. 8    314.176   09/09/2024 01:49:22 AM
                  ft40.12      0.336   09/09/2024 12:54:24 AM
                  ft40. 6      0.336   09/09/2024 12:54:24 AM
               MDCINXXX13      0.000   09/09/2024 12:47:49 AM
                  ft46.11     46.608   09/09/2024 01:51:25 AM
                   DFDENS      8.128   09/09/2024 12:34:10 AM
                  ft42. 1     49.264   09/09/2024 01:52:21 AM
                   DFFCK2      8.128   09/09/2024 12:34:10 AM
           RELCCSD.OUT.13      0.009   09/09/2024 12:47:48 AM
            RELCCSD.OUT.6      0.009   09/09/2024 12:47:48 AM
                MCCRES.13     75.928   09/09/2024 03:58:03 AM
                  ft43. 1     75.840   09/09/2024 01:52:21 AM
                  ft41. 6      7.728   09/09/2024 01:49:26 AM
                   DFEVEC      7.938   09/09/2024 12:34:04 AM
                  ft45. 2     30.208   09/09/2024 01:52:22 AM
                 MCCRES.6     75.928   09/09/2024 03:58:03 AM
                  ft41.12      7.728   09/09/2024 01:49:26 AM
                  ft45. 8     30.208   09/09/2024 01:52:22 AM
           OPERATORS.noh5      0.033   09/09/2024 12:32:04 AM
                  ft43.15     75.840   09/09/2024 01:52:21 AM
                   DFFCKT      8.128   09/09/2024 12:32:07 AM
                 MCCRES.2     75.928   09/09/2024 03:58:03 AM
                   DFCMOS      0.882   09/09/2024 12:34:11 AM
             MOLECULE.MOL      0.001   09/09/2024 12:32:04 AM
                  ft45. 6     30.208   09/09/2024 01:52:23 AM
                 MCCRES.8     75.928   09/09/2024 03:58:03 AM
                  ft43.11     75.840   09/09/2024 01:52:21 AM
            RELCCSD.OUT.8      0.009   09/09/2024 12:47:48 AM
                  ft43. 5     75.840   09/09/2024 01:52:22 AM
            RELCCSD.OUT.2      0.009   09/09/2024 12:47:48 AM
                  ft41. 8      7.728   09/09/2024 01:49:22 AM
                  ft45.12     30.272   09/09/2024 01:52:23 AM
                  AOMOMAT      5.355   09/09/2024 12:33:10 AM
                  ft41. 2      7.728   09/09/2024 01:49:23 AM
                  ft44.12    314.176   09/09/2024 01:49:25 AM
                  ft40. 8      0.336   09/09/2024 12:54:24 AM
                  ft40. 2      0.336   09/09/2024 12:54:24 AM
                  ft42. 5     49.264   09/09/2024 01:52:22 AM
                  ft46.15     46.608   09/09/2024 01:51:31 AM
               MDCINXXXX1      0.000   09/09/2024 12:47:48 AM
                  ft46. 1     46.496   09/09/2024 01:51:31 AM
                  ft42.11     49.264   09/09/2024 01:52:21 AM
                  ft44. 6    314.176   09/09/2024 01:49:26 AM
                  ft40.11      0.336   09/09/2024 12:54:24 AM
                  ft44. 1    314.176   09/09/2024 01:49:23 AM
               MDCINXXXX6      0.000   09/09/2024 12:47:49 AM
                  ft46. 6     46.496   09/09/2024 01:51:29 AM
                  ft42. 2     49.264   09/09/2024 01:52:21 AM
                   DFFCK1      8.128   09/09/2024 12:34:10 AM
                  ft46.12     46.608   09/09/2024 01:51:32 AM
                  ft42. 8     49.264   09/09/2024 01:52:21 AM
                  ft44.15    314.176   09/09/2024 01:49:25 AM
               MDCINXXX10      0.000   09/09/2024 12:47:48 AM
                  ft40. 5      0.336   09/09/2024 12:54:24 AM
                  ft45.15     30.272   09/09/2024 01:52:22 AM
                  dirac.x     72.456   09/09/2024 12:32:02 AM
                  ft41. 5      7.728   09/09/2024 01:49:26 AM
                  ft43. 2     75.840   09/09/2024 01:52:21 AM
                MCCRES.10     75.928   09/09/2024 03:58:03 AM
           RELCCSD.OUT.10      0.009   09/09/2024 12:47:48 AM
            RELCCSD.OUT.5      0.009   09/09/2024 12:47:48 AM
                  ft43. 8     75.840   09/09/2024 01:52:21 AM
                  AOMOSLR      2.678   09/09/2024 12:32:07 AM
                  ft41.11      7.728   09/09/2024 01:49:22 AM
                 MCCRES.5     75.928   09/09/2024 03:58:03 AM
                  ft45. 1     30.208   09/09/2024 01:52:22 AM
                  ft43. 0     75.840   09/09/2024 01:52:22 AM
                MCCRES.12     75.928   09/09/2024 03:58:03 AM
           RELCCSD.OUT.12      0.009   09/09/2024 12:47:48 AM
            RELCCSD.OUT.7      0.009   09/09/2024 12:47:48 AM
                   DFDIIS      0.001   09/09/2024 12:32:25 AM
                  ft41. 7      7.728   09/09/2024 01:49:24 AM
                 MCCRES.7     75.928   09/09/2024 03:58:03 AM
                  ft45. 9     30.272   09/09/2024 01:52:23 AM
                  ft41.13      7.728   09/09/2024 01:49:26 AM
                  ft45. 3     30.208   09/09/2024 01:52:22 AM
                  ft43.14     75.840   09/09/2024 01:52:21 AM
               MDCINXXXX4      0.000   09/09/2024 12:47:49 AM
                  ft46. 4     46.496   09/09/2024 01:51:31 AM
                  ft42.14     49.264   09/09/2024 01:52:21 AM
                  ft40.13      0.336   09/09/2024 12:54:24 AM
                  ft44. 9    314.176   09/09/2024 01:49:21 AM
                dirac.xml      0.004   09/09/2024 12:32:04 AM
                  ft44. 3    314.176   09/09/2024 01:49:23 AM
                  ft40. 7      0.336   09/09/2024 12:54:24 AM
               MDCINXXX12      0.000   09/09/2024 12:47:49 AM
                  ft42. 0     49.264   09/09/2024 01:52:22 AM
                  AOMOlin      2.678   09/09/2024 12:32:07 AM
                  ft46.10     46.608   09/09/2024 01:51:25 AM
                   MDPROP      1.141   09/09/2024 12:34:11 AM
                  ft42. 7     49.264   09/09/2024 01:52:22 AM
                  ft44.10    314.176   09/09/2024 01:49:21 AM
                  ft40. 0      0.336   09/09/2024 12:54:24 AM
               MDCINXXX15      0.000   09/09/2024 12:47:49 AM
                  ft40.14      0.336   09/09/2024 12:54:24 AM
                  ft44. 4    314.176   09/09/2024 01:49:23 AM
               MDCINXXXX3      0.000   09/09/2024 12:47:48 AM
                  ft46. 3     46.496   09/09/2024 01:51:31 AM
                  ft42.13     49.264   09/09/2024 01:52:21 AM
                  ft46. 9     46.608   09/09/2024 01:51:24 AM
               MDCINXXXX9      0.000   09/09/2024 12:47:48 AM
                  ft43.13     75.840   09/09/2024 01:52:21 AM
                 MCCRES.0     75.928   09/09/2024 03:58:03 AM
                  ft41.14      7.728   09/09/2024 01:49:26 AM
                  ft45. 4     30.208   09/09/2024 01:52:23 AM
                  ft45.10     30.272   09/09/2024 01:52:23 AM
                  ft41. 0      7.728   09/09/2024 01:49:26 AM
                  ft43. 7     75.840   09/09/2024 01:52:22 AM
                MCCRES.15     75.928   09/09/2024 03:58:03 AM
           RELCCSD.OUT.15      0.009   09/09/2024 12:47:48 AM
                  ft41. 4      7.728   09/09/2024 01:49:24 AM
                  ft45.14     30.272   09/09/2024 01:52:22 AM
           RELCCSD.OUT.11      0.009   09/09/2024 12:47:48 AM
            RELCCSD.OUT.4      0.009   09/09/2024 12:47:48 AM
                  ft43. 9     75.840   09/09/2024 01:52:21 AM
                MCCRES.11     75.928   09/09/2024 03:58:03 AM
                  ft43. 3     75.840   09/09/2024 01:52:21 AM
          CHECKPOINT.noh5      0.033   09/09/2024 12:32:04 AM
                  ft45. 0     30.208   09/09/2024 01:52:23 AM
                  ft41.10      7.728   09/09/2024 01:49:22 AM
                 MCCRES.4     75.928   09/09/2024 03:58:03 AM
                  ft44. 0    314.176   09/09/2024 01:50:52 AM
                  ft40.10      0.336   09/09/2024 12:54:24 AM
                  ft46. 7     46.496   09/09/2024 01:51:31 AM
               MDCINXXXX7      0.000   09/09/2024 12:47:48 AM
                  ft42. 9     49.264   09/09/2024 01:52:21 AM
                  ft46.13     46.608   09/09/2024 01:51:32 AM
                  ft42. 3     49.264   09/09/2024 01:52:21 AM
               MDCINXXX11      0.000   09/09/2024 12:47:48 AM
                  ft40. 4      0.336   09/09/2024 12:54:24 AM
                  ft44.14    314.176   09/09/2024 01:49:25 AM
                  ft40. 3      0.336   09/09/2024 12:54:24 AM
                  ft40. 9      0.336   09/09/2024 12:54:24 AM
                  ft44.13    314.176   09/09/2024 01:49:25 AM
                  ft46.14     46.608   09/09/2024 01:51:33 AM
                  ft42. 4     49.264   09/09/2024 01:52:22 AM
                DIRAC.INP      0.000   09/09/2024 12:47:48 AM
                  LOWDMAT      1.857   09/09/2024 12:32:05 AM
                  ft42.10     49.264   09/09/2024 01:52:21 AM
                  ft46. 0     46.496   09/09/2024 01:51:43 AM
                  ft44. 7    314.176   09/09/2024 01:49:24 AM
             MOLECULE.XYZ      0.001   09/09/2024 12:32:04 AM
                  ft45. 7     30.208   09/09/2024 01:52:23 AM
                 MCCRES.9     75.928   09/09/2024 03:58:03 AM
                 MCCRES.3     75.928   09/09/2024 03:58:03 AM
                  ft43.10     75.840   09/09/2024 01:52:21 AM
             interface_mo      0.000   09/09/2024 12:32:07 AM
            RELCCSD.OUT.3      0.009   09/09/2024 12:47:48 AM
            RELCCSD.OUT.9      0.009   09/09/2024 12:47:48 AM
                  ft43. 4     75.840   09/09/2024 01:52:22 AM
                  ft41. 3      7.728   09/09/2024 01:49:23 AM
                  ft45.13     30.272   09/09/2024 01:52:23 AM
                  ft41. 9      7.728   09/09/2024 01:49:22 AM
  ------------------------------------------------------------------------------
      Total size of all files :   13171.296  MB 
      Disk info:  used    available   capacity [GB]
             294036.415 115563.585   409600.000 

  Could not construct hdf5 checkpoint file, h5py is not installed
  going to delete the scratch directory ... done

  exit date      : 2024-09-09 03:58:09.066364
  elapsed time   : 03h26m06s
  exit           : normal
