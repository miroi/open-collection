  ** interface to 32-bit integer MPI enabled **

DIRAC master    (f31n09) starts by allocating     1424000000 r*8 words (  10.610 GB) of memory
DIRAC nodes 1 to 199 starts by allocating          1424000000 r*8 words (  10.610 GB) of memory
DIRAC nodes 1 to 199 to allocate at most           2048000000 r*8 words (  15.259 GB) of memory
DIRAC master    (f31n09) to allocate at most      2048000000 r*8 words (  15.259 GB) of memory
 
Note: maximum allocatable memory for master+nodes can be set by -aw (MW)/-ag (GB) flags in pam
 
 *******************************************************************************
 *                                                                             *
 *                                O U T P U T                                  *
 *                                   from                                      *
 *                                                                             *
 *                   @@@@@    @@   @@@@@     @@@@     @@@@@                    *
 *                   @@  @@        @@  @@   @@  @@   @@                        *
 *                   @@  @@   @@   @@@@@    @@@@@@   @@                        *
 *                   @@  @@   @@   @@ @@    @@  @@   @@                        *
 *                   @@@@@    @@   @@  @@   @@  @@    @@@@@                    *
 *                                                                             *
 *                                                                             *
 %}ZS)S?$=$)]S?$%%>SS$%S$ZZ6cHHMHHHHHHHHMHHM&MHbHH6$L/:<S///</:|/:|:/::!:.::--:%
 $?S$$%$$$$?%?$(SSS$SSSHMMMMMMMMMMMMMMMMMM6H&6S&SH&&k?6$r~::://///::::::.:::-::$
 (%?)Z??$$$(S%$>$)S6HMMMMMMMMMMMMMMMMMMMMMMR6M]&&$6HR$&6(i::::::|i|:::::::-:-::(
 $S?$$)$?$%?))?S/]#MMMMMMMMMMMMMMMMMMMMMMMMMMHM1HRH9R&$$$|):?:/://|:/::/:/.::.:$
 SS$%%?$%((S)?Z[6MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM&HF$$&/)S?<~::!!:::::::/:-:|.S
 SS%%%%S$%%%$$MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMHHHHHHM>?/S/:/:::`:/://:/::-::S
 ?$SSSS?%SS$)MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM/4?:S:/:::/:::/:/:::.::?
 S$(S?S$%(?$HMMMMMMMMMMMMMMMMM#&7RH99MMMMMMMMMMMMMMMMMMHHHd$/:::::/::::::-//.:.S
 (?SS(%)S&HMMMMMMMMMMMMMMMMM#S|///???$9HHMMMMMMMMMDSZ&1S/?</>?~:///::|/!:/-:-:.(
 $S?%?<H(MMMMMMMMMMMMMMMMR?`. :.:`::<>:``?/*?##*)$:/>       `((%://::/:::::/::/$
 S$($$)HdMMMMMMMMMMMMMMMP: . `   `  `    `      `-            `Z<:>?::/:::::|:iS
 c%%%&HMMMMMMMMMMMMMMMM6:                                      `$%)>%%</>!:::::c
 S?%/MMMMMMMMMMMMMMMMMMH-                                        /ZSS>?:?~:;/::S
 $SZ?MMMMMMMMMMMMMMMMMH?.                                        \"&((/?//?|:::$
 $%$%&MMMMMMMMMMMMMMMMM:.                                          ?%/S:</::/::$
 ($?}&HMMMMMMMMMMMMMMMM>:                                          $%%<?/i:|i::(
 Z$($&MMMMMMMMMMMMMMMMHk(:.  . -                                   .S/\?\?/!:/:Z
 (??$<HMMMMMMMMMMMMMMMFk|:   -.-.                                  :%/%/(:/:ii|(
 SZ(S?]MMMMMMMMMMMMMMHS?:- -  ::.:                                  |/S:</::?||S
 $%)$$(MMMMMMMMMMMMMMR):`:. :.:::`,,/bcokb/_                       :S?%?|~:/:/:$
 %$$%$)[[?$?MMMMMMMMMM: :.:-.::::$7?<&7&MMMMMMM#/           _ .. ..:</?:(:/::::%
 $$$?Z?HHH~|/MMMMMMMMM/`.-.:.:/:%%%%?dHMMMMMMMMMMH?,-   .,bMMMM6//./i~/~:<:::/:$
 $($S$M//::S?ZHMMMMMH/:.`:::.:/%S/&MMHMMMMMMMMRM&><   ,HMMMMMMMF  :::?:///:|:::$
 )[$S$S($|_i:#>::*H&?/::.::/:\"://:?>>`:&HMHSMMMM$:`-   MMHMMMMHHT .)i/?////::/)
 $$[$$>$}:dHH&$$--?S::-:.:::--/-:``./::>%Zi?)&/?`:.`   `H?$T*\" `  /%?>%:)://ii$
 $&=&/ZS}$RF<:?/-.|%r/:::/:/:`.-.-..|::S//!`\"``          >??:    `SS<S:)!/////$
 Z&]>b[Z(Z?&%:::../S$$:>:::i`.`. `-.`  `                         ,>%%%:>/>/!|:/Z
 $$&/F&1$c$?>:>?/,>?$$ZS/::/:-: ...                              |S?S)S?<~:::::$
 &$&$&$k&>>|?<:<?((/$[/?)i~/:/. - -                              S?:%%%?/:::/::&
 =[/Z[[Fp[MMH$>?Z&S$$$/$S///||..-           -.-                  /((S$:%<:///:/=
 $&>1MHHMMMM6M9MMMM$Z$}$S%/:::.`.            .:/,,,dcb>/:.       ((SSSS%:)!//i|$
 MMMMMMMMMMMR&&RRRHR&&($(?:|i::-             .:%&S&$[&H&``     ../>%;/?>??:<::>M
 MMMMMMMMMMMMS/}S$&&H&[$SS//:::.:.   . . .v</Jq1&&D]&M&<,      :/::/?%%)S>?://:M
 MMMMMMMMMMMM?}$/$$kMM&&$(%/?//:..`.  .|//1d/`://?*/*/\"` `     .:/(SS$%(S%)):%M
 MMMMMMMMMMMM(}$$>&&MMHR#$S%%:?::.:|-.`:;&&b/D/$p=qpv//b/~`   :/~~%%??$=$)Z$S+;M
 MMMMMMMMMMMM[|S$$Z1]MMMMD[$?$:>)/::: :/?:``???bD&{b<<-`     .,:/)|SS(}Z/$$?/<SM
 MMMMMMMMMMMM||$)&7k&MMMMH9]$$??Z%|!/:i::`  `` .             SS?SS?Z/]1$/&$c/$SM
 MMMMMMMMMMMM| -?>[&]HMMMMMMMH1[/7SS(?:/..-` ::/Sc,/_,     _<$?SS%$S/&c&&$&>//<M
 MMMMMMMMMMMMR  `$&&&HMM9MMMMMMM&&c$%%:/:/:.:.:/\?\?/\    _MMHk/7S/]dq&1S<&&></M
 MMMMMMMMMMMMM?  :&96MHMMMMMMMMMMMHHk[S%(<<:// `         ,MMMMMMM&/Z6H]DkH]1$&&M
 MMMMMMMMMMMMMD    99H9HMMMMMMMMMMMMMMMb&%$<:i.:....    .MMMMMMMMM6HHHRH&H&H1SFM
 MMMMMMMMMMMMMM|   `?HMMMMMMMMMMMMMMMMMMMHk6k&>$&Z$/?_.bHMMMMMMMMMMM&6HRM9H6]ZkM
 MMMMMMMMMMMMMMM/    `TMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMHMH6RH&R6&M
 MMMMMMMMMMMMMMMM    -|?HMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMFHH6HMD&&M
 MMMMMMMMMMMMMMMMk  ..:~?9MMMMMMMMMMMMM#`:MMMMMMMMMMMMMMMMMMMMMMMMMMMMM9MHkR6&FM
 MMMMMMMMMMMMMMMMM/  .-!:%$ZHMMMMMMMMMR` dMMMMMMMMMMMMMMMMMMMMMMMMMMMMM9MRMHH9&M
 MMMMMMMMMMMMMMMMMML,:.-|::/?&&MMMMMM` .MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMHRMH&&6M
 MMMMMMMMMMMMMMMMMMMc%>/:::i<:SMMMMMMHdMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMHHM&969kM
 MMMMMMMMMMMMMMMMMMMMSS/$$/(|HMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMHH&HH&M
 MMMMMMMMMMMMMMMMMMMM6S/?/MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMR96H1DR1M
 MMMMMMMMMMMMMMMMMMMMM&$MHMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMHMH691&&M
 MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMH&R&9ZM
 MMMMMMMMMMMMMMMMMMMMMMMMMRHMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMH&96][6M
 MMMMMMMMMMMMMMMMMMMMMMMMp?:MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM96HH1][FM
 MMMMMMMMMMMMMMMMMMMMMMMM> -HMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMH&1k&$&M
 *******************************************************************************
 *                                                                             *
 *         =========================================================           *
 *                     Program for Atomic and Molecular                        *
 *          Direct Iterative Relativistic All-electron Calculations            *
 *         =========================================================           *
 *                                                                             *
 *                                                                             *
 *    Written by:                                                              *
 *                                                                             *
 *    Radovan Bast             UiT The Arctic University of      Norway        *
 *    Andre S. P. Gomes        CNRS/Universite de Lille          France        *
 *    Trond Saue               Universite Toulouse III           France        *
 *    Lucas Visscher           Vrije Universiteit Amsterdam      Netherlands   *
 *    Hans Joergen Aa. Jensen  University of Southern Denmark    Denmark       *
 *                                                                             *
 *    with contributions from:                                                 *
 *                                                                             *
 *    Ignacio Agustin Aucar    CONICET/Northeastern University   Argentina     *
 *    Vebjoern Bakken          University of Oslo                Norway        *
 *    Chima Chibueze           Vrije Universiteit Amsterdam      Netherlands   *
 *    Joel Creutzberg          University of Southern Denmark    Denmark       *
 *    Kenneth G. Dyall         Schrodinger, Inc., Portland       USA           *
 *    Sebastien Dubillard      University of Strasbourg          France        *
 *    Ulf Ekstroem             University of Oslo                Norway        *
 *    Ephraim Eliav            University of Tel Aviv            Israel        *
 *    Thomas Enevoldsen        University of Southern Denmark    Denmark       *
 *    Elke Fasshauer           University of Tubingen            Germany       *
 *    Timo Fleig               Universite Toulouse III           France        *
 *    Olav Fossgaard           UiT The Arctic University of      Norway        *
 *    Loic Halbert             Universite de Lille               France        *
 *    Erik D. Hedegaard        University of Southern Denmark    Denmark       *
 *    Trygve Helgaker          University of Oslo                Norway        *
 *    Benjamin Helmich-Paris   Max Planck Institute f. Coal Res. Germany       *
 *    Johan Henriksson         Linkoeping University             Sweden        *
 *    Martin van Horn          Universite Toulouse III           France        *
 *    Miroslav Ilias           Matej Bel University              Slovakia      *
 *    Christoph R. Jacob       TU Braunschweig                   Germany       *
 *    Stefan Knecht            Algorithmiq Ltd / ETH Zuerich     Finland/CH    *
 *    Stanislav Komorovsky     Slovak Academy of Sciences        Slovakia      *
 *    Ossama Kullie            University of Kassel              Germany       *
 *    Jon K. Laerdahl          University of Oslo                Norway        *
 *    Christoffer V. Larsen    University of Southern Denmark    Denmark       *
 *    Yoon Sup Lee             KAIST, Daejeon                    South Korea   *
 *    Nanna Holmgaard List     Stockholm Inst. of Technology     Sweden        *
 *    Huliyar S. Nataraj       BME/Budapest Univ. Tech. & Econ.  Hungary       *
 *    Malaya Kumar Nayak       Bhabha Atomic Research Centre     India         *
 *    Patrick Norman           Stockholm Inst. of Technology     Sweden        *
 *    Andreas Nyvang           Aarhus University                 Denmark       *
 *    Malgorzata Olejniczak    University of Warsaw              Poland        *
 *    Jeppe Olsen              Aarhus University                 Denmark       *
 *    Jogvan Magnus H. Olsen   University of Southern Denmark    Denmark       *
 *    Anastasios Papadopoulos  Max Planck Institute f. Coal Res. Germany       *
 *    Young Choon Park         KAIST, Daejeon                    South Korea   *
 *    Jesper K. Pedersen       University of Southern Denmark    Denmark       *
 *    Markus Pernpointner      University of Heidelberg          Germany       *
 *    Johann V. Pototschnig    Technical University Graz         Austria       *
 *    Roberto Di Remigio       EuroCC National Competence Centre Sweden        *
 *    Michal Repisky           UiT The Arctic University of      Norway        *
 *    Kenneth Ruud             UiT The Arctic University of      Norway        *
 *    Pawel Salek              Stockholm Inst. of Technology     Sweden        *
 *    Bernd Schimmelpfennig    Karlsruhe Institute of Technology Germany       *
 *    Bruno Senjean            CNRS/Universite de Montpellier    France        *
 *    Avijit Shee              University of Berkeley            USA           *
 *    Jetze Sikkema            Vrije Universiteit Amsterdam      Netherlands   *
 *    Ayaki Sunaga             Kyoto University                  Japan         *
 *    Andreas J. Thorvaldsen   UiT The Arctic University of      Norway        *
 *    Joern Thyssen            University of Southern Denmark    Denmark       *
 *    Joost N. P. van Stralen  Vrije Universiteit Amsterdam      Netherlands   *
 *    Marta L. Vidal           Cardiff University                UK            *
 *    Sebastien Villaume       Linkoeping University             Sweden        *
 *    Olivier Visser           University of Groningen           Netherlands   *
 *    Toke Winther             University of Southern Denmark    Denmark       *
 *    Shigeyoshi Yamamoto      Chukyo University                 Japan         *
 *    Xiang Yuan               Universite de Lille               France        *
 *                                                                             *
 *    For more information about the DIRAC code see http://diracprogram.org    *
 *                                                                             *
 *    This is an experimental code. The authors accept no responsibility       *
 *    for the performance of the code or for the correctness of the results.   *
 *                                                                             *
 *    This program is free software; you can redistribute it and/or            *
 *    modify it under the terms of the GNU Lesser General Public               *
 *    License version 2.1 as published by the Free Software Foundation.        *
 *                                                                             *
 *    If results obtained with this code are published, an                     *
 *    appropriate citation would be:                                           *
 *                                                                             *
 *    DIRAC, a relativistic ab initio electronic structure program,            *
 *    Release DIRAC23 (2023), written by                                       *
 *    R. Bast, A. S. P. Gomes, T. Saue, L. Visscher, and H. J. Aa. Jensen,     *
 *    with contributions from I. A. Aucar, V. Bakken, C. Chibueze,             *
 *    J. Creutzberg, K. G. Dyall, S. Dubillard, U. Ekstroem, E. Eliav,         *
 *    T. Enevoldsen, E. Fasshauer, T. Fleig, O. Fossgaard, L. Halbert,         *
 *    E. D. Hedegaard, T. Helgaker, J. Henriksson, M. van Horn, M. Ilias,      *
 *    Ch. R. Jacob, S. Knecht, S. Komorovsky, O. Kullie, J. K. Laerdahl,       *
 *    C. V. Larsen, Y. S. Lee, N. H. List, H. S. Nataraj, M. K. Nayak,         *
 *    P. Norman, A. Nyvang, M. Olejniczak, J. Olsen, J. M. H. Olsen,           *
 *    A. Papadopoulos, Y. C. Park, J. K. Pedersen, M. Pernpointner,            *
 *    J. V. Pototschnig, R. Di Remigio, M. Repisky, K. Ruud, P. Salek,         *
 *    B. Schimmelpfennig, B. Senjean, A. Shee, J. Sikkema, A. Sunaga,          *
 *    A. J. Thorvaldsen, J. Thyssen, J. N. P. van Stralen, M. L. Vidal,        *
 *    S. Villaume, O. Visser, T. Winther, S. Yamamoto and X. Yuan              *
 *    (see http://diracprogram.org).                                           *
 *                                                                             *
 *    as well as our reference paper: J. Chem. Phys. 152 (2020) 204104.        *
 *                                                                             *
 *******************************************************************************


Version information
-------------------

Branch        | release-23
Commit hash   | 400e5caf4
Commit author | Hans JÃ¸rgen Aagaard Jensen
Commit date   | Wed Feb 22 13:50:08 2023 +0000


Configuration and build information
-----------------------------------

Who compiled             | gomes
Compiled on server       | login3
Operating system         | Linux-4.18.0-193.46.1.el8_2.ppc64le
CMake version            | 3.23.2
CMake generator          | Unix Makefiles
CMake build type         | release
Configuration time       | 2023-03-11 02:03:05.886577
Fortran compiler         | /sw/summit/spack-envs/base/opt/linux-rhel8-ppc64le/gcc-11.2.0/spectrum-mpi-10.4.0.3-20210112-3jdqaes3rqueekezybe4jups6zzdyxco/bin/mpif90
Fortran compiler version | 11.2.0
Fortran compiler flags   |  -g -fcray-pointer -fbacktrace -fno-range-check -DVAR_GFORTRAN -DVAR_MFDS -fallow-argument-mismatch  -fopenmp -fno-lto -g -fcray-pointer -fbacktrace -fno-range-check -DVAR_GFORTRAN -DVAR_MFDS -fallow-argument-mismatch  -fopenmp -fno-lto
C compiler               | /sw/summit/spack-envs/base/opt/linux-rhel8-ppc64le/gcc-11.2.0/spectrum-mpi-10.4.0.3-20210112-3jdqaes3rqueekezybe4jups6zzdyxco/bin/mpicc
C compiler version       | 11.2.0
C compiler flags         |  -g  -fopenmp -fno-lto -g  -fopenmp -fno-lto
C++ compiler             | /sw/summit/spack-envs/base/opt/linux-rhel8-ppc64le/gcc-11.2.0/spectrum-mpi-10.4.0.3-20210112-3jdqaes3rqueekezybe4jups6zzdyxco/bin/mpicxx
C++ compiler version     | 11.2.0
C++ compiler flags       |  -g -Wall -Wno-unknown-pragmas -Wno-sign-compare -Woverloaded-virtual -Wwrite-strings -Wno-unused  -fopenmp -fno-lto -g -Wall -Wno-unknown-pragmas -Wno-sign-compare -Woverloaded-virtual -Wwrite-strings -Wno-unused  -fopenmp -fno-lto
Static linking           | False
64-bit integers          | False
MPI parallelization      | True
MPI launcher             | /sw/summit/spack-envs/base/opt/linux-rhel8-ppc64le/gcc-11.2.0/spectrum-mpi-10.4.0.3-20210112-3jdqaes3rqueekezybe4jups6zzdyxco/bin/mpiexec
Math libraries           | /sw/summit/spack-envs/base/opt/linux-rhel8-ppc64le/gcc-11.2.0/netlib-lapack-3.9.1-ad6bana5qizkt2z2opcikceyy2ua5lrp/lib64/liblapack.so;/sw/summit/spack-envs/base/opt/linux-rhel8-ppc64le/gcc-11.2.0/netlib-lapack-3.9.1-ad6bana5qizkt2z2opcikceyy2ua5lrp/lib64/libblas.so
Builtin BLAS library     | OFF
Builtin LAPACK library   | OFF
Explicit libraries       | unknown
Compile definitions      | HAVE_MPI;HAVE_OPENMP;VAR_MPI;VAR_MPI2;USE_MPI_MOD_F90;SYS_LINUX;PRG_DIRAC;INSTALL_WRKMEM=64000000;HAS_PCMSOLVER;BUILD_GEN1INT;HAS_PELIB;HAS_STIELTJES;MOD_LAO_REARRANGED;MOD_MCSCF_spinfree;MOD_XAMFI;MOD_ESR;MOD_KRCC;MOD_SRDFT;MOD_EXACORR

EXACORR dependencies
--------------------
Exatensor source repo    | https://github.com/RelMBdev/ExaTENSOR.git
Exatensor git hash       | 35caded68340657186be190a2d68a98c9e2159bb
Exatensor configuration  | WRAP=NOWRAP BUILD_TYPE=OPT   TOOLKIT=GNU EXA_OS=LINUX GPU_CUDA=CUDA MPILIB=OPENMPI BLASLIB=ESSL


 LAPACK integer*4/8 selftest passed
 Selftest of ISO_C_BINDING Fortran - C/C++ interoperability PASSED
 MPI selftest passed with MPI_INTEGER of the size 4 bytes


 * openMP activated,  max # processes, max # threads, current # threads:       84      21       1

Execution time and host
-----------------------

 
     Date and time (Linux)  : Tue Jul 18 09:51:08 2023
     Host name              : f31n09                                  

 * Opening checkpoint file 
    - New checkpoint file created


Contents of the input file
--------------------------

**DIRAC                                                                                             
.TITLE                                                                                              
  MtH(CO)4                                                                                          
.WAVE FUNCTION                                                                                      
.PROPERTIES                                                                                         
#.ANALYZE                                                                                           
**GENERAL                                                                                           
#.PCMOUT                                                                                            
**INTEGRALS                                                                                         
*READIN                                                                                             
.UNCONTRACTED                                                                                       
**HAMILTONIAN                                                                                       
.X2C                                                                                                
#.DFT                                                                                               
#BP86                                                                                               
*AMFI                                                                                               
.AMFICH                                                                                             
+4                                                                                                  
**PROPERTIES                                                                                        
.DIPOLE                                                                                             
#.POLARIZ                                                                                           
**WAVE FUNCTION                                                                                     
.SCF                                                                                                
.EXACC                                                                                              
*SCF                                                                                                
.CLOSED SHELL                                                                                       
152                                                                                                 
.EVCCNV                                                                                             
1.0D-9  5.0D-5                                                                                      
## needs starting MO                                                                                
.MAXITR                                                                                             
6                                                                                                   
#.NODAMP                                                                                            
#**MOLTRA                                                                                           
## 42 correlated electrons, v2z                                                                     
#.ACTIVE                                                                                            
#energy -1.20   20.0  0.01                                                                          
**EXACC                                                                                             
.OCCUPIED                                                                                           
energy -18.00 -0.1  0.01                                                                            
.VIRTUAL                                                                                            
energy -0.1 5.0  0.01                                                                               
.EXATENSOR                                                                                          
.TCONVERG                                                                                           
1.0E-7                                                                                              
#.LAMBDA                                                                                            
.MOINT_SCHEME                                                                                       
4                                                                                                   
.PRINT                                                                                              
4                                                                                                   
.NCYCLES                                                                                            
90                                                                                                  
#.NOTRIPLES                                                                                         
*END OF                                                                                             


Contents of the molecule file
-----------------------------

INTGRL                                                                                              
 Mt(Hax)(CO)4 molecule;set C1 symmetry for exacorr                                                  
 DIRAC X2C-A4P,BP86,v2z CONVERGED geometry                                                          
C   4    0                                                                                          
      109.0   1                                                                                     
Mt    0.0002206562            0.0000000000            0.2668441713                                  
LARGE BASIS dyall.cv3z                                                                              
        1.0   1                                                                                     
H     0.0008142511            0.0000000000            3.4758249816                                  
LARGE BASIS dyall.cv3z                                                                              
        6.0   3                                                                                     
C     1.8641969700            3.2289050274            0.6268034176                                  
C    -3.7288704620            0.0000000000            0.6267949659                                  
C    -0.0001674099            0.0000000000           -3.5872197181                                  
LARGE BASIS dyall.cv3z                                                                              
        8.0   3                                                                                     
O     2.9523131050            5.1130069354            0.9213122963                                  
O    -5.9047160573            0.0000000000            0.9195453762                                  
O    -0.0012160724            0.0000000000           -5.7699258552                                  
LARGE BASIS dyall.cv3z                                                                              
FINISH                                                                                              


    **************************************************************************
    *******************************   MtH(CO)4 *******************************
    **************************************************************************

 Jobs in this run:
   * Wave function
   * Properties


    **************************************************************************
    ************************** General DIRAC set-up **************************
    **************************************************************************

   CODATA Recommended Values of the Fundamental Physical Constants: 2018  
             Peter J. Mohr, David B. Newell and Barry N. Taylor           
         Reviews of Modern Physics, Vol. 93, 025010 (2021)                
 * The speed of light :        137.0359992
 * Running in two-component mode
 * Parallel run with 199 slaves.
 * Direct evaluation of the following two-electron integrals:
   - LL-integrals
 * Spherical transformation embedded in MO-transformation
   for large components
 * Transformation to scalar RKB basis embedded in
   MO-transformation for small components
 * Thresholds for linear dependence:
   Large components:   1.00D-06
   Small components:   1.00D-08
 * General print level   :   0


    *************************************************************************
    ****************** Output from HERMIT input processing ******************
    *************************************************************************

 Default print level:        1
 Using default nuclear model: Gaussian charge distribution.

 Two-electron integrals not calculated.


 Ordinary (field-free non-relativistic) Hamiltonian integrals not calculated.



 Changes of defaults for *READIN
 -------------------------------


 Uncontracted basis forced, irrespective of basis input file.



   ***************************************************************************
   ****************** Output from MOLECULE input processing ******************
   ***************************************************************************



  Title Cards
  -----------

   Mt(Hax)(CO)4 molecule;set C1 symmetry for exacorr                      
   DIRAC X2C-A4P,BP86,v2z CONVERGED geometry                              
  Nuclear Gaussian exponent for atom of charge 109.000 :    1.1878723221D+08
  Nuclear Gaussian exponent for atom of charge   1.000 :    2.1248236111D+09
  Nuclear Gaussian exponent for atom of charge   6.000 :    6.8077493126D+08
  Nuclear Gaussian exponent for atom of charge   8.000 :    5.8631428213D+08


                          SYMGRP:Point group information
                          ------------------------------

Point group: C1 

   * Character table

        |  E 
   -----+-----
    A   |   1

   * Direct product table

        | A  
   -----+-----
    A   | A  


                            **************************
                            *** Output from DBLGRP ***
                            **************************

   * One fermion irrep:   E1 
   * Quaternionic group. NZ = 4
   * Direct product decomposition:
          E1  x E1  : A   + A   + A   + A  


                                 Spinor structure
                                 ----------------


   * Fermion irrep no.: 1
      La  |  A  (1)  A  (1)  |
      Sa  |  A  (1)  A  (1)  |
      Lb  |  A  (1)  A  (1)  |
      Sb  |  A  (1)  A  (1)  |


                              Quaternion symmetries
                              ---------------------

    Rep  T(+)
    -----------------------------
    A    1  i  j  k

  Nuclear repulsion energy                       :   1139.593091903316 Hartree

  Nuclear contribution to electric dipole moment :  -34.793131832100   60.277485647600  -12.872433815900 a.u.;  origin (0,0,0)


  Atoms and basis sets
  --------------------

  Number of atom types :    4
  Total number of atoms:    8

  label    atoms   charge   prim    cont     basis   
  ----------------------------------------------------------------------
  Mt          1     109     460     460      L  - [32s29p20d14f4g1h|32s29p20d14f4g1h]                            
  H           1       1      21      21      L  - [9s2p1d|9s2p1d]                                                
  C           3       6      66      66      L  - [14s8p3d1f|14s8p3d1f]                                          
  O           3       8      66      66      L  - [14s8p3d1f|14s8p3d1f]                                          
  ----------------------------------------------------------------------
                            877     877      L  - large components
  ----------------------------------------------------------------------
  total:      8     152     877     877

  Cartesian basis used.
  Threshold for integrals (to be written to file):  1.00D-15


  References for the basis sets
  -----------------------------

  Atom type   1   2   3   4
   1s-3s: K.G. Dyall, Theor. Chem. Acc. (2016) 135:128.                           
   4s-7s: K.G. Dyall, J. Phys. Chem. A. (2009) 113:12638.                         
   2p-3p: K.G. Dyall, Theor. Chem. Acc. (2016) 135:128.                           
   4p-6p: K.G. Dyall, Theor. Chem. Acc. (2002) 108:335;                           
          revision K.G. Dyall, Theor. Chem. Acc. (2006) 115:441.                  
      7p: K.G. Dyall, Theor. Chem. Acc. (2012) 131:1172.                          
      3d: K.G. Dyall and A.S.P. Gomes, unpublished.                               
      4d: K.G. Dyall, Theor. Chem. Acc. (2007) 117:483.                           
      5d: K.G. Dyall, Theor. Chem. Acc. (2004) 112:403;                           
          revision  K.G. Dyall and A.S.P. Gomes, Theor. Chem. Acc. (2009) 125:97. 


  Cartesian Coordinates (bohr)
  ----------------------------

  Total number of coordinates: 24


   1   Mt       x      0.0002206562
   2            y      0.0000000000
   3            z      0.2668441713

   4   H        x      0.0008142511
   5            y      0.0000000000
   6            z      3.4758249816

   7   C        x      1.8641969700
   8            y      3.2289050274
   9            z      0.6268034176

  10   C        x     -3.7288704620
  11            y      0.0000000000
  12            z      0.6267949659

  13   C        x     -0.0001674099
  14            y      0.0000000000
  15            z     -3.5872197181

  16   O        x      2.9523131050
  17            y      5.1130069354
  18            z      0.9213122963

  19   O        x     -5.9047160573
  20            y      0.0000000000
  21            z      0.9195453762

  22   O        x     -0.0012160724
  23            y      0.0000000000
  24            z     -5.7699258552



  Cartesian coordinates in XYZ format (Angstrom)
  ----------------------------------------------

    8

Mt     0.0001167662   0.0000000000   0.1412078543
H      0.0004308831   0.0000000000   1.8393273694
C      0.9864905532   1.7086629567   0.3316900843
C     -1.9732332709   0.0000000000   0.3316856119
C     -0.0000885895   0.0000000000  -1.8982749253
O      1.5622968146   2.7056867494   0.4875374713
O     -3.1246411744   0.0000000000   0.4866024575
O     -0.0006435178   0.0000000000  -3.0533132712


   Interatomic separations (in Angstroms):
   ---------------------------------------

            Mt          H           C           C           C           O   

   Mt      0.000000
   H       1.698120    0.000000
   C       1.982106    2.482904    0.000000
   C       1.982522    2.483613    3.417528    0.000000
   C       2.039483    3.737602    2.977518    2.977587    0.000000
   O       3.143420    3.404042    1.161851    4.454773    3.931144    0.000000
   O       3.143789    3.405281    4.454765    1.161783    3.930708    5.411851
   O       3.194521    4.892641    3.918190    3.917822    1.155038    4.722409

            O           O   

   O       0.000000
   O       4.721267    0.000000




  Bond distances (angstroms):
  ---------------------------

                  atom 1     atom 2                           distance
                  ------     ------                           --------
  bond distance:    H          Mt                             1.698120
  bond distance:    C          Mt                             1.982106
  bond distance:    C          Mt                             1.982522
  bond distance:    C          Mt                             2.039483
  bond distance:    O          C                              1.161851
  bond distance:    O          C                              1.161783
  bond distance:    O          C                              1.155038


  Bond angles (degrees):
  ----------------------

                  atom 1     atom 2     atom 3                   angle
                  ------     ------     ------                   -----
  bond angle:       C          Mt         H                     84.480
  bond angle:       C          Mt         H                     84.497
  bond angle:       C          Mt         C                    119.085
  bond angle:       C          Mt         H                    179.995
  bond angle:       C          Mt         C                     95.518
  bond angle:       C          Mt         C                     95.508
  bond angle:       O          C          Mt                   177.806
  bond angle:       O          C          Mt                   177.851
  bond angle:       O          C          Mt                   179.978



   Nuclear repulsion energy                          : 1139.593091903316 Hartree

                       * Total mass:   352.130570 amu
                       * Natural abundance:  96.027 %


* Center-of-mass coordinates (a.u.):      -0.197543796855493   0.342284884992898  -0.045611848133124
* Center-of-mass coordinates (A)   :      -0.104535675451179   0.181129360774796  -0.024136750579218


                                GETLAB: AO-labels
                                -----------------

   * Large components:  186
     1  L Mt  1 s        2  L Mt  1 px       3  L Mt  1 py       4  L Mt  1 pz       5  L Mt  1 dxx      6  L Mt  1 dxy 
     7  L Mt  1 dxz      8  L Mt  1 dyy      9  L Mt  1 dyz     10  L Mt  1 dzz     11  L Mt  1 fxxx    12  L Mt  1 fxxy
    13  L Mt  1 fxxz    14  L Mt  1 fxyy    15  L Mt  1 fxyz    16  L Mt  1 fxzz    17  L Mt  1 fyyy    18  L Mt  1 fyyz
    19  L Mt  1 fyzz    20  L Mt  1 fzzz    21  L Mt  1 g400    22  L Mt  1 g310    23  L Mt  1 g301    24  L Mt  1 g220
    25  L Mt  1 g211    26  L Mt  1 g202    27  L Mt  1 g130    28  L Mt  1 g121    29  L Mt  1 g112    30  L Mt  1 g103
    31  L Mt  1 g040    32  L Mt  1 g031    33  L Mt  1 g022    34  L Mt  1 g013    35  L Mt  1 g004    36  L Mt  1 h500
    37  L Mt  1 h410    38  L Mt  1 h401    39  L Mt  1 h320    40  L Mt  1 h311    41  L Mt  1 h302    42  L Mt  1 h230
    43  L Mt  1 h221    44  L Mt  1 h212    45  L Mt  1 h203    46  L Mt  1 h140    47  L Mt  1 h131    48  L Mt  1 h122
    49  L Mt  1 h113    50  L Mt  1 h104    51  L Mt  1 h050    52  L Mt  1 h041    53  L Mt  1 h032    54  L Mt  1 h023
    55  L Mt  1 h014    56  L Mt  1 h005    57  L H   1 s       58  L H   1 px      59  L H   1 py      60  L H   1 pz  
    61  L H   1 dxx     62  L H   1 dxy     63  L H   1 dxz     64  L H   1 dyy     65  L H   1 dyz     66  L H   1 dzz 
    67  L C   1 s       68  L C   1 px      69  L C   1 py      70  L C   1 pz      71  L C   1 dxx     72  L C   1 dxy 
    73  L C   1 dxz     74  L C   1 dyy     75  L C   1 dyz     76  L C   1 dzz     77  L C   1 fxxx    78  L C   1 fxxy
    79  L C   1 fxxz    80  L C   1 fxyy    81  L C   1 fxyz    82  L C   1 fxzz    83  L C   1 fyyy    84  L C   1 fyyz
    85  L C   1 fyzz    86  L C   1 fzzz    87  L C   1 s       88  L C   1 px      89  L C   1 py      90  L C   1 pz  
    91  L C   1 dxx     92  L C   1 dxy     93  L C   1 dxz     94  L C   1 dyy     95  L C   1 dyz     96  L C   1 dzz 
    97  L C   1 fxxx    98  L C   1 fxxy    99  L C   1 fxxz   100  L C   1 fxyy   101  L C   1 fxyz   102  L C   1 fxzz
   103  L C   1 fyyy   104  L C   1 fyyz   105  L C   1 fyzz   106  L C   1 fzzz   107  L C   1 s      108  L C   1 px  
   109  L C   1 py     110  L C   1 pz     111  L C   1 dxx    112  L C   1 dxy    113  L C   1 dxz    114  L C   1 dyy 
   115  L C   1 dyz    116  L C   1 dzz    117  L C   1 fxxx   118  L C   1 fxxy   119  L C   1 fxxz   120  L C   1 fxyy
   121  L C   1 fxyz   122  L C   1 fxzz   123  L C   1 fyyy   124  L C   1 fyyz   125  L C   1 fyzz   126  L C   1 fzzz
   127  L O   1 s      128  L O   1 px     129  L O   1 py     130  L O   1 pz     131  L O   1 dxx    132  L O   1 dxy 
   133  L O   1 dxz    134  L O   1 dyy    135  L O   1 dyz    136  L O   1 dzz    137  L O   1 fxxx   138  L O   1 fxxy
   139  L O   1 fxxz   140  L O   1 fxyy   141  L O   1 fxyz   142  L O   1 fxzz   143  L O   1 fyyy   144  L O   1 fyyz
   145  L O   1 fyzz   146  L O   1 fzzz   147  L O   1 s      148  L O   1 px     149  L O   1 py     150  L O   1 pz  
   151  L O   1 dxx    152  L O   1 dxy    153  L O   1 dxz    154  L O   1 dyy    155  L O   1 dyz    156  L O   1 dzz 
   157  L O   1 fxxx   158  L O   1 fxxy   159  L O   1 fxxz   160  L O   1 fxyy   161  L O   1 fxyz   162  L O   1 fxzz
   163  L O   1 fyyy   164  L O   1 fyyz   165  L O   1 fyzz   166  L O   1 fzzz   167  L O   1 s      168  L O   1 px  
   169  L O   1 py     170  L O   1 pz     171  L O   1 dxx    172  L O   1 dxy    173  L O   1 dxz    174  L O   1 dyy 
   175  L O   1 dyz    176  L O   1 dzz    177  L O   1 fxxx   178  L O   1 fxxy   179  L O   1 fxxz   180  L O   1 fxyy
   181  L O   1 fxyz   182  L O   1 fxzz   183  L O   1 fyyy   184  L O   1 fyyz   185  L O   1 fyzz   186  L O   1 fzzz
   * Small components:    0



  Symmetry Orbitals
  -----------------

  Number of orbitals in each symmetry:          877
  Number of large orbitals in each symmetry:    877
  Number of small orbitals in each symmetry:      0

* Large component functions

  Symmetry  A  ( 1)

       32 functions:    Mt s   
       29 functions:    Mt px  
       29 functions:    Mt py  
       29 functions:    Mt pz  
       20 functions:    Mt dxx 
       20 functions:    Mt dxy 
       20 functions:    Mt dxz 
       20 functions:    Mt dyy 
       20 functions:    Mt dyz 
       20 functions:    Mt dzz 
       14 functions:    Mt fxxx
       14 functions:    Mt fxxy
       14 functions:    Mt fxxz
       14 functions:    Mt fxyy
       14 functions:    Mt fxyz
       14 functions:    Mt fxzz
       14 functions:    Mt fyyy
       14 functions:    Mt fyyz
       14 functions:    Mt fyzz
       14 functions:    Mt fzzz
        4 functions:    Mt g400
        4 functions:    Mt g310
        4 functions:    Mt g301
        4 functions:    Mt g220
        4 functions:    Mt g211
        4 functions:    Mt g202
        4 functions:    Mt g130
        4 functions:    Mt g121
        4 functions:    Mt g112
        4 functions:    Mt g103
        4 functions:    Mt g040
        4 functions:    Mt g031
        4 functions:    Mt g022
        4 functions:    Mt g013
        4 functions:    Mt g004
        1 functions:    Mt h500
        1 functions:    Mt h410
        1 functions:    Mt h401
        1 functions:    Mt h320
        1 functions:    Mt h311
        1 functions:    Mt h302
        1 functions:    Mt h230
        1 functions:    Mt h221
        1 functions:    Mt h212
        1 functions:    Mt h203
        1 functions:    Mt h140
        1 functions:    Mt h131
        1 functions:    Mt h122
        1 functions:    Mt h113
        1 functions:    Mt h104
        1 functions:    Mt h050
        1 functions:    Mt h041
        1 functions:    Mt h032
        1 functions:    Mt h023
        1 functions:    Mt h014
        1 functions:    Mt h005
        9 functions:    H  s   
        2 functions:    H  px  
        2 functions:    H  py  
        2 functions:    H  pz  
        1 functions:    H  dxx 
        1 functions:    H  dxy 
        1 functions:    H  dxz 
        1 functions:    H  dyy 
        1 functions:    H  dyz 
        1 functions:    H  dzz 
       14 functions:    C  s   
        8 functions:    C  px  
        8 functions:    C  py  
        8 functions:    C  pz  
        3 functions:    C  dxx 
        3 functions:    C  dxy 
        3 functions:    C  dxz 
        3 functions:    C  dyy 
        3 functions:    C  dyz 
        3 functions:    C  dzz 
        1 functions:    C  fxxx
        1 functions:    C  fxxy
        1 functions:    C  fxxz
        1 functions:    C  fxyy
        1 functions:    C  fxyz
        1 functions:    C  fxzz
        1 functions:    C  fyyy
        1 functions:    C  fyyz
        1 functions:    C  fyzz
        1 functions:    C  fzzz
       14 functions:    C  s   
        8 functions:    C  px  
        8 functions:    C  py  
        8 functions:    C  pz  
        3 functions:    C  dxx 
        3 functions:    C  dxy 
        3 functions:    C  dxz 
        3 functions:    C  dyy 
        3 functions:    C  dyz 
        3 functions:    C  dzz 
        1 functions:    C  fxxx
        1 functions:    C  fxxy
        1 functions:    C  fxxz
        1 functions:    C  fxyy
        1 functions:    C  fxyz
        1 functions:    C  fxzz
        1 functions:    C  fyyy
        1 functions:    C  fyyz
        1 functions:    C  fyzz
        1 functions:    C  fzzz
       14 functions:    C  s   
        8 functions:    C  px  
        8 functions:    C  py  
        8 functions:    C  pz  
        3 functions:    C  dxx 
        3 functions:    C  dxy 
        3 functions:    C  dxz 
        3 functions:    C  dyy 
        3 functions:    C  dyz 
        3 functions:    C  dzz 
        1 functions:    C  fxxx
        1 functions:    C  fxxy
        1 functions:    C  fxxz
        1 functions:    C  fxyy
        1 functions:    C  fxyz
        1 functions:    C  fxzz
        1 functions:    C  fyyy
        1 functions:    C  fyyz
        1 functions:    C  fyzz
        1 functions:    C  fzzz
       14 functions:    O  s   
        8 functions:    O  px  
        8 functions:    O  py  
        8 functions:    O  pz  
        3 functions:    O  dxx 
        3 functions:    O  dxy 
        3 functions:    O  dxz 
        3 functions:    O  dyy 
        3 functions:    O  dyz 
        3 functions:    O  dzz 
        1 functions:    O  fxxx
        1 functions:    O  fxxy
        1 functions:    O  fxxz
        1 functions:    O  fxyy
        1 functions:    O  fxyz
        1 functions:    O  fxzz
        1 functions:    O  fyyy
        1 functions:    O  fyyz
        1 functions:    O  fyzz
        1 functions:    O  fzzz
       14 functions:    O  s   
        8 functions:    O  px  
        8 functions:    O  py  
        8 functions:    O  pz  
        3 functions:    O  dxx 
        3 functions:    O  dxy 
        3 functions:    O  dxz 
        3 functions:    O  dyy 
        3 functions:    O  dyz 
        3 functions:    O  dzz 
        1 functions:    O  fxxx
        1 functions:    O  fxxy
        1 functions:    O  fxxz
        1 functions:    O  fxyy
        1 functions:    O  fxyz
        1 functions:    O  fxzz
        1 functions:    O  fyyy
        1 functions:    O  fyyz
        1 functions:    O  fyzz
        1 functions:    O  fzzz
       14 functions:    O  s   
        8 functions:    O  px  
        8 functions:    O  py  
        8 functions:    O  pz  
        3 functions:    O  dxx 
        3 functions:    O  dxy 
        3 functions:    O  dxz 
        3 functions:    O  dyy 
        3 functions:    O  dyz 
        3 functions:    O  dzz 
        1 functions:    O  fxxx
        1 functions:    O  fxxy
        1 functions:    O  fxxz
        1 functions:    O  fxyy
        1 functions:    O  fxyz
        1 functions:    O  fxzz
        1 functions:    O  fyyy
        1 functions:    O  fyyz
        1 functions:    O  fyzz
        1 functions:    O  fzzz


   ***************************************************************************
   *************************** Hamiltonian defined ***************************
   ***************************************************************************


 One-electron operator origins:
 - General operator origin (a.u.)       :   0.000000000000000   0.000000000000000   0.000000000000000
 - Magnetic gauge origin (a.u.)         :   0.000000000000000   0.000000000000000   0.000000000000000
 - Dipole (and multipole) origin (a.u.) :   0.000000000000000   0.000000000000000   0.000000000000000
  - BSS with properties !
 * Print level:    0
 * Exact-Two-Component (X2C) Hamiltonian
   Reference: 
    M. Ilias and T. Saue:
    "Implementation of an infinite-order two-component relativistic Hamiltonian 
    by a simple one-step transformation." 
    J. Chem. Phys., 126 (2007) 064102.
   additional reference for the new X2C module:
    S. Knecht and T. Saue:
    manuscript in preparation, Strasbourg 2010.

 * Running in two-component mode
 * Default integral flags passed to all modules
   - LL-integrals:     1
   - LS-integrals:     0
   - SS-integrals:     0
   - GT-integrals:     0


 *******************************************************************************
 ************************** AMFI/RELSCF input reading **************************
 *******************************************************************************

===========================================================================
   Set-up for AMFI/RELSCF calculations
===========================================================================
 * AMFI   code print level:    0
 * RELSCF code print level:    0
 * RELSCF maximum number of iterations:   50
 * AMFI mean-field summations on individual atoms are modified due to the artificial charge of the system:   4
 * order of AMFI contributions to the X2C Hamiltonian:  2
  --> adding spin-same orbit MFSSO2 terms.


    **************************************************************************
    ************************** Wave function module **************************
    **************************************************************************

 Wave function types requested (in input order):
     HF        
     EXA_CC    

 Wave function jobs in execution order (expanded):
 * Hartree-Fock calculation
===========================================================================
 *SCF: Set-up for Hartree-Fock calculation:
===========================================================================
 * Number of fermion irreps: 1
 * Closed shell SCF calculation with   152 electrons in   76 orbitals.
 * Charge of molecule : 0
 * Sum of atomic potentials used for start guess
 * General print level   :   0

 ***** INITIAL TRIAL SCF FUNCTION *****
 * Trial vectors read from CHECKPOINT
 * Scaling of active-active block correction to open shell Fock operator    0.500000
   to improve convergence (default value).

 ***** SCF CONVERGENCE CRITERIA *****
 * Convergence on norm of error vector (gradient).
   Desired convergence:1.000D-09
   Allowed convergence:5.000D-05

 ***** CONVERGENCE CONTROL *****
 * Fock matrix constructed using differential density matrix
    with optimal parameter.
 * DIIS (in MO basis)
 * DIIS will be activated when convergence reaches : 1.00D+20
   - Maximum size of B-matrix:   10
 * Damping of Fock matrix when DIIS is not activated. 
   Weight of old matrix    : 0.250
 * Maximum number of SCF iterations  :    6
 * No quadratic convergent Hartree-Fock
 * Contributions from 2-electron integrals to Fock matrix:
   LL-integrals.
    ---> this is default setting from Hamiltonian input
 * NB!!! No e-p rotations in 2nd order optimization.
 ***** OUTPUT CONTROL *****
 * Only electron eigenvalues written out.


   ***************************************************************************
   ***************************** Property module *****************************
   ***************************************************************************

 * Print level:   0
 * Input label: **PROPE
 * Properties calculated for the following wave functions:
     1: DHF 
 These initial settings of center and origins might be changed later:
 * Operator center (a.u.):      0.0000000000      0.0000000000      0.0000000000
 * Gauge origin    (a.u.):      0.0000000000      0.0000000000      0.0000000000
 * Dipole origin   (a.u.):      0.0000000000      0.0000000000      0.0000000000
 * Perform 4c->2c picture change transformation of the four-component property operators
===========================================================================
 Dipole moment
===========================================================================


 ********************************************************************************
 *************************** Input consistency checks ***************************
 ********************************************************************************



    *************************************************************************
    ************************ End of input processing ************************
    *************************************************************************



   ***************************************************************************
   ****************** Output from MOLECULE input processing ******************
   ***************************************************************************



  Title Cards
  -----------

   Mt(Hax)(CO)4 molecule;set C1 symmetry for exacorr                      
   DIRAC X2C-A4P,BP86,v2z CONVERGED geometry                              
  Nuclear Gaussian exponent for atom of charge 109.000 :    1.1878723221D+08
  Nuclear Gaussian exponent for atom of charge   1.000 :    2.1248236111D+09
  Nuclear Gaussian exponent for atom of charge   6.000 :    6.8077493126D+08
  Nuclear Gaussian exponent for atom of charge   8.000 :    5.8631428213D+08


                          SYMGRP:Point group information
                          ------------------------------

Point group: C1 

   * Character table

        |  E 
   -----+-----
    A   |   1

   * Direct product table

        | A  
   -----+-----
    A   | A  


                            **************************
                            *** Output from DBLGRP ***
                            **************************

   * One fermion irrep:   E1 
   * Quaternionic group. NZ = 4
   * Direct product decomposition:
          E1  x E1  : A   + A   + A   + A  


                                 Spinor structure
                                 ----------------


   * Fermion irrep no.: 1
      La  |  A  (1)  A  (1)  |
      Sa  |  A  (1)  A  (1)  |
      Lb  |  A  (1)  A  (1)  |
      Sb  |  A  (1)  A  (1)  |


                              Quaternion symmetries
                              ---------------------

    Rep  T(+)
    -----------------------------
    A    1  i  j  k

  Nuclear repulsion energy                       :   1139.593091903316 Hartree

  Nuclear contribution to electric dipole moment :  -34.793131832100   60.277485647600  -12.872433815900 a.u.;  origin (0,0,0)


  Atoms and basis sets
  --------------------

  Number of atom types :    4
  Total number of atoms:    8

  label    atoms   charge   prim    cont     basis   
  ----------------------------------------------------------------------
  Mt          1     109     460     460      L  - [32s29p20d14f4g1h|32s29p20d14f4g1h]                            
  H           1       1      21      21      L  - [9s2p1d|9s2p1d]                                                
  C           3       6      66      66      L  - [14s8p3d1f|14s8p3d1f]                                          
  O           3       8      66      66      L  - [14s8p3d1f|14s8p3d1f]                                          
  ----------------------------------------------------------------------
                            877     877      L  - large components
                           2022    2022      S  - small components
  ----------------------------------------------------------------------
  total:      8     152    2899    2899

  Cartesian basis used.
  Threshold for integrals (to be written to file):  1.00D-15


  References for the basis sets
  -----------------------------

  Atom type   1   2   3   4
   1s-3s: K.G. Dyall, Theor. Chem. Acc. (2016) 135:128.                           
   4s-7s: K.G. Dyall, J. Phys. Chem. A. (2009) 113:12638.                         
   2p-3p: K.G. Dyall, Theor. Chem. Acc. (2016) 135:128.                           
   4p-6p: K.G. Dyall, Theor. Chem. Acc. (2002) 108:335;                           
          revision K.G. Dyall, Theor. Chem. Acc. (2006) 115:441.                  
      7p: K.G. Dyall, Theor. Chem. Acc. (2012) 131:1172.                          
      3d: K.G. Dyall and A.S.P. Gomes, unpublished.                               
      4d: K.G. Dyall, Theor. Chem. Acc. (2007) 117:483.                           
      5d: K.G. Dyall, Theor. Chem. Acc. (2004) 112:403;                           
          revision  K.G. Dyall and A.S.P. Gomes, Theor. Chem. Acc. (2009) 125:97. 


  Cartesian Coordinates (bohr)
  ----------------------------

  Total number of coordinates: 24


   1   Mt       x      0.0002206562
   2            y      0.0000000000
   3            z      0.2668441713

   4   H        x      0.0008142511
   5            y      0.0000000000
   6            z      3.4758249816

   7   C        x      1.8641969700
   8            y      3.2289050274
   9            z      0.6268034176

  10   C        x     -3.7288704620
  11            y      0.0000000000
  12            z      0.6267949659

  13   C        x     -0.0001674099
  14            y      0.0000000000
  15            z     -3.5872197181

  16   O        x      2.9523131050
  17            y      5.1130069354
  18            z      0.9213122963

  19   O        x     -5.9047160573
  20            y      0.0000000000
  21            z      0.9195453762

  22   O        x     -0.0012160724
  23            y      0.0000000000
  24            z     -5.7699258552



  Cartesian coordinates in XYZ format (Angstrom)
  ----------------------------------------------

    8

Mt     0.0001167662   0.0000000000   0.1412078543
H      0.0004308831   0.0000000000   1.8393273694
C      0.9864905532   1.7086629567   0.3316900843
C     -1.9732332709   0.0000000000   0.3316856119
C     -0.0000885895   0.0000000000  -1.8982749253
O      1.5622968146   2.7056867494   0.4875374713
O     -3.1246411744   0.0000000000   0.4866024575
O     -0.0006435178   0.0000000000  -3.0533132712


   Interatomic separations (in Angstroms):
   ---------------------------------------

            Mt          H           C           C           C           O   

   Mt      0.000000
   H       1.698120    0.000000
   C       1.982106    2.482904    0.000000
   C       1.982522    2.483613    3.417528    0.000000
   C       2.039483    3.737602    2.977518    2.977587    0.000000
   O       3.143420    3.404042    1.161851    4.454773    3.931144    0.000000
   O       3.143789    3.405281    4.454765    1.161783    3.930708    5.411851
   O       3.194521    4.892641    3.918190    3.917822    1.155038    4.722409

            O           O   

   O       0.000000
   O       4.721267    0.000000




  Bond distances (angstroms):
  ---------------------------

                  atom 1     atom 2                           distance
                  ------     ------                           --------
  bond distance:    H          Mt                             1.698120
  bond distance:    C          Mt                             1.982106
  bond distance:    C          Mt                             1.982522
  bond distance:    C          Mt                             2.039483
  bond distance:    O          C                              1.161851
  bond distance:    O          C                              1.161783
  bond distance:    O          C                              1.155038


  Bond angles (degrees):
  ----------------------

                  atom 1     atom 2     atom 3                   angle
                  ------     ------     ------                   -----
  bond angle:       C          Mt         H                     84.480
  bond angle:       C          Mt         H                     84.497
  bond angle:       C          Mt         C                    119.085
  bond angle:       C          Mt         H                    179.995
  bond angle:       C          Mt         C                     95.518
  bond angle:       C          Mt         C                     95.508
  bond angle:       O          C          Mt                   177.806
  bond angle:       O          C          Mt                   177.851
  bond angle:       O          C          Mt                   179.978



   Nuclear repulsion energy                          : 1139.593091903316 Hartree

                       * Total mass:   352.130570 amu
                       * Natural abundance:  96.027 %


* Center-of-mass coordinates (a.u.):      -0.197543796855493   0.342284884992898  -0.045611848133124
* Center-of-mass coordinates (A)   :      -0.104535675451179   0.181129360774796  -0.024136750579218


                      Nuclear contribution to dipole moments
                      --------------------------------------

                               au             Debye

                    x    -34.79313183    -88.43613870
                    y     60.27748565    153.21150469
                    z    -12.87243382    -32.71876610

                        1 Debye =   2.54177000 a.u. 


                       Generating Lowdin canonical matrix:
                       -----------------------------------

   L   A     * Deleted:        133(Proj:        133, Lindep:          0) Smin: 0.13E-05
   S   A     * Deleted:        432(Proj:        409, Lindep:         23) Smin: 0.12E-12
*** WARNING *** : 23 functions deleted due to numerical linear dependence.
 >>> CPU  time used in Lwdn_a is  27.27 seconds
 >>> WALL time used in Lwdn_a is  27.27 seconds

                   *********************************************************************
                   ***   Entering the Exact-Two-Component (X2C) interface in DIRAC   ***
                   ***                                                               ***
                   *** library version:  1.2 (August  2013)                          ***
                   ***                                                               ***
                   *** authors:          - Stefan Knecht                             ***
                   ***                   - Trond Saue                                ***
                   *** contributors:     - Hans Joergen Aagaard Jensen               ***
                   ***                   - Michal Repisky                            ***
                   ***                   - Miroslav Ilias                            ***
                   *** features:         - X2C                                       ***
                   ***                   - X2C-atomic/fragment (X2C-LU)              ***
                   ***                   - X2C-spinfree                              ***
                   ***                   - X2C-molecular-mean-field (X2Cmmf)         ***
                   ***                                                               ***
                   ***                      Universities of                          ***
                   ***     Zuerich, Toulouse, Odense, Banska Bystrica and Tromsoe    ***
                   ***                                                               ***
                   *** contact: stefan.knecht@gmail.com                              ***
                   *********************************************************************


                   *** chosen path in X2C module: molecular X2C (with spin-orbit contributions)      


                                Output from AMFIIN
                                ------------------

warning: old amfi routines in use: zero AMFI contributions from h, i, k, ... shells 
 The total nonzero charge of the system:           4
  factor is :   2.6315789473684209E-002
           1 .atom-nucleus charge:         109   partial charge:   2.8684210526315788     
           2 .atom-nucleus charge:           1   partial charge:   2.6315789473684209E-002
           3 .atom-nucleus charge:           6   partial charge:  0.15789473684210525     
           4 .atom-nucleus charge:           6   partial charge:  0.15789473684210525     
           5 .atom-nucleus charge:           6   partial charge:  0.15789473684210525     
           6 .atom-nucleus charge:           8   partial charge:  0.21052631578947367     
           7 .atom-nucleus charge:           8   partial charge:  0.21052631578947367     
           8 .atom-nucleus charge:           8   partial charge:  0.21052631578947367     
      Sum of all charges (real):   3.9999999999999991     
 Total charge of the system is :           4

  *** number of unique nuclei (from file MNF.INP): 4

  *** calculate AMFI for atom type 1 with atomic charge   109
  *** number of nuclei with identical atom type:   1
   unique nuclei index: 1
  *** file with AMFI integrals for this center: AOPROPER_MNF.109.1  


                         ATOMIC NO-PAIR SO-MF CODE starts
                         --------------------------------

   Douglas-Kroll type operators 
  charge on the calculated atom:  3
  Mean-field summation for electrons #: 106
    ...electronic occupation of Sg: [Rn]7s^2 7p^0 6d^4 5f^14  
 ****  Written to the file TOSCF for "relscf" ****
        charge: ******
      nprimit:  32 29 20 14
   closed sh.:   7  5  3  2
     open sh.:   0  0  4  0


                       *** PROGRAM AT34 - ALLIANT - @V ***
                       -----------------------------------


      SYMMETRY SPECIES            S       P       D       F
      NUMBER OF BASIS FUNCTIONS: 32      29      20      14
      NUMBER OF CLOSED SHELLS  :  7       5       3       2
      OPEN SHELL OCCUPATION    :  0       0       4       0
  ### SCF ITERATIONS           ###
  ### NON-RELATIVISTIC APPROX. ###
    1. iteration,  total energy:             0.000000000000
    2. iteration,  total energy:        -30560.490252983767
    3. iteration,  total energy:        -36671.234719795131
    4. iteration,  total energy:        -35858.353497918979
    5. iteration,  total energy:        -37622.709092002791
    6. iteration,  total energy:        -38301.355958661850
    7. iteration,  total energy:        -38154.203751100133
    8. iteration,  total energy:        -38289.258286515636
    9. iteration,  total energy:        -38321.976888091551
   10. iteration,  total energy:        -38375.727037073339
   11. iteration,  total energy:        -38379.616921302055
   12. iteration,  total energy:        -38380.684695003365
   13. iteration,  total energy:        -38381.738526996160
   14. iteration,  total energy:        -38381.749365666969
   15. iteration,  total energy:        -38381.754586315663
   16. iteration,  total energy:        -38381.756488360428
   17. iteration,  total energy:        -38381.759478592692
   18. iteration,  total energy:        -38381.759538748709
   19. iteration,  total energy:        -38381.759514578938
   20. iteration,  total energy:        -38381.759547370777
   21. iteration,  total energy:        -38381.759547370399
   22. iteration,  total energy:        -38381.759542881802
   23. iteration,  total energy:        -38381.759544802735
   24. iteration,  total energy:        -38381.759543431086
   25. iteration,  total energy:        -38381.759545927554
   25. iteration,  total energy:        -38381.759544100612
  ### NON-RELATIVISTIC APPROX. ###
        25      -0.3838175954D+05      -0.7676324973D+05       0.3838149018D+05      -0.2000007018D+01
  ### SCF ITERATIONS           ###
  ### EV APPROX.               ###
    1. iteration,  total energy:        -40228.594798920123
    2. iteration,  total energy:        -43616.929894038629
    3. iteration,  total energy:        -43623.939475971034
    4. iteration,  total energy:        -43625.718978227604
    5. iteration,  total energy:        -43628.964663146391
    6. iteration,  total energy:        -43628.991172278635
    7. iteration,  total energy:        -43628.995860920295
    8. iteration,  total energy:        -43628.998668175162
    9. iteration,  total energy:        -43629.005292925118
   10. iteration,  total energy:        -43629.005301566154
   11. iteration,  total energy:        -43629.005311606918
   12. iteration,  total energy:        -43629.005316882634
   13. iteration,  total energy:        -43629.005336009912
   14. iteration,  total energy:        -43629.005332351640
   15. iteration,  total energy:        -43629.005332316054
   16. iteration,  total energy:        -43629.005332378314
   17. iteration,  total energy:        -43629.005336109112
   18. iteration,  total energy:        -43629.005332388864
   19. iteration,  total energy:        -43629.005332391440
   20. iteration,  total energy:        -43629.005332389395
   21. iteration,  total energy:        -43629.005336108392
   21. iteration,  total energy:        -43629.005332390341
  ### EV  OPERATOR RESULT      ###
        21      -0.4362900533D+05      -0.1047755057D+06       0.6114650035D+05      -0.1713515983D+01
      *** AMFIIN: ADDING nucleus    1 with charge 109 to the BSSn Hamiltonian.

  *** calculate AMFI for atom type 2 with atomic charge     1
  *** number of nuclei with identical atom type:   1
  no 2e-SO corrections for hydrogen or hydrogen-like 1e-systems. AMFI is skipped.
   unique nuclei index: 2

       *** This (AMFI) unique nuclei is not to be calculated ! Only pass (to read input basis) through the AMFI routine.



                         ATOMIC NO-PAIR SO-MF CODE starts
                         --------------------------------

   Douglas-Kroll type operators 
  skip explicit AMFI - reading AMFI integrals from file AOPROPER_MNF.xxx!

  *** calculate AMFI for atom type 3 with atomic charge     6
  *** number of nuclei with identical atom type:   3
   unique nuclei index: 3
  *** file with AMFI integrals for this center: AOPROPER_MNF.6.3    


                         ATOMIC NO-PAIR SO-MF CODE starts
                         --------------------------------

   Douglas-Kroll type operators 
  charge on the calculated atom:  0
  Mean-field summation for electrons #:   6
    ...electronic occupation of  C: [He]2s^2 2p^2             
 ****  Written to the file TOSCF for "relscf" ****
        charge:  6.000
      nprimit:  14  8  3  1
   closed sh.:   2  0  0  0
     open sh.:   0  2  0  0


                       *** PROGRAM AT34 - ALLIANT - @V ***
                       -----------------------------------


      SYMMETRY SPECIES            S       P       D       F
      NUMBER OF BASIS FUNCTIONS: 14       8       3       1
      NUMBER OF CLOSED SHELLS  :  2       0       0       0
      OPEN SHELL OCCUPATION    :  0       2       0       0
  ### SCF ITERATIONS           ###
  ### NON-RELATIVISTIC APPROX. ###
    1. iteration,  total energy:                        NaN
    2. iteration,  total energy:           -35.230624061565
    3. iteration,  total energy:           -37.665089816850
    4. iteration,  total energy:           -37.688370787410
    5. iteration,  total energy:           -37.688501744902
    6. iteration,  total energy:           -37.688567388672
    7. iteration,  total energy:           -37.688570924274
    8. iteration,  total energy:           -37.688571241099
    9. iteration,  total energy:           -37.688571257322
   10. iteration,  total energy:           -37.688571279770
   11. iteration,  total energy:           -37.688571282034
   12. iteration,  total energy:           -37.688571282372
   13. iteration,  total energy:           -37.688571276495
   14. iteration,  total energy:           -37.688571282430
   14. iteration,  total energy:           -37.688571282434
  ### NON-RELATIVISTIC APPROX. ###
        14      -0.3768857128D+02      -0.7537710971D+02       0.3768853843D+02      -0.2000000872D+01
  ### SCF ITERATIONS           ###
  ### EV APPROX.               ###
    1. iteration,  total energy:           -37.702596710067
    2. iteration,  total energy:           -37.703574333863
    3. iteration,  total energy:           -37.703574435097
    4. iteration,  total energy:           -37.703574442460
    5. iteration,  total energy:           -37.703574437247
    6. iteration,  total energy:           -37.703574443731
    7. iteration,  total energy:           -37.703574443823
    8. iteration,  total energy:           -37.703574443838
    9. iteration,  total energy:           -37.703574437923
   10. iteration,  total energy:           -37.703574443841
   10. iteration,  total energy:           -37.703574443841
  ### EV  OPERATOR RESULT      ###
        10      -0.3770357444D+02      -0.7543887065D+02       0.3773529620D+02      -0.1999159361D+01
      *** AMFIIN: ADDING nucleus    3 with charge   6 to the BSSn Hamiltonian.
      *** AMFIIN: ADDING nucleus    4 with charge   6 to the BSSn Hamiltonian.
      *** AMFIIN: ADDING nucleus    5 with charge   6 to the BSSn Hamiltonian.

  *** calculate AMFI for atom type 4 with atomic charge     8
  *** number of nuclei with identical atom type:   3
   unique nuclei index: 6
  *** file with AMFI integrals for this center: AOPROPER_MNF.8.6    


                         ATOMIC NO-PAIR SO-MF CODE starts
                         --------------------------------

   Douglas-Kroll type operators 
  charge on the calculated atom:  0
  Mean-field summation for electrons #:   8
    ...electronic occupation of  O: [He]2s^2 2p^4             
 ****  Written to the file TOSCF for "relscf" ****
        charge:  8.000
      nprimit:  14  8  3  1
   closed sh.:   2  0  0  0
     open sh.:   0  4  0  0


                       *** PROGRAM AT34 - ALLIANT - @V ***
                       -----------------------------------


      SYMMETRY SPECIES            S       P       D       F
      NUMBER OF BASIS FUNCTIONS: 14       8       3       1
      NUMBER OF CLOSED SHELLS  :  2       0       0       0
      OPEN SHELL OCCUPATION    :  0       4       0       0
  ### SCF ITERATIONS           ###
  ### NON-RELATIVISTIC APPROX. ###
    1. iteration,  total energy:          1557.577811566396
    2. iteration,  total energy:           -59.802847723695
    3. iteration,  total energy:           -73.161538292972
    4. iteration,  total energy:           -74.546619541738
    5. iteration,  total energy:           -74.773349843904
    6. iteration,  total energy:           -74.806356200088
    7. iteration,  total energy:           -74.809029945169
    8. iteration,  total energy:           -74.809239497570
    9. iteration,  total energy:           -74.809255006623
   10. iteration,  total energy:           -74.809257469509
   11. iteration,  total energy:           -74.809257662167
   12. iteration,  total energy:           -74.809257677547
   13. iteration,  total energy:           -74.809257683989
   14. iteration,  total energy:           -74.809257678873
   14. iteration,  total energy:           -74.809257678890
  ### NON-RELATIVISTIC APPROX. ###
        14      -0.7480925768D+02      -0.1496184763D+03       0.7480921865D+02      -0.2000000522D+01
  ### SCF ITERATIONS           ###
  ### EV APPROX.               ###
    1. iteration,  total energy:           -74.857236351220
    2. iteration,  total energy:           -74.861592444954
    3. iteration,  total energy:           -74.861593678664
    4. iteration,  total energy:           -74.861593766983
    5. iteration,  total energy:           -74.861593778615
    6. iteration,  total energy:           -74.861593774648
    7. iteration,  total energy:           -74.861593774756
    8. iteration,  total energy:           -74.861593774764
    9. iteration,  total energy:           -74.861593780075
    9. iteration,  total energy:           -74.861593774765
  ### EV  OPERATOR RESULT      ###
         9      -0.7486159377D+02      -0.1498349790D+03       0.7497338518D+02      -0.1998508919D+01
      *** AMFIIN: ADDING nucleus    6 with charge   8 to the BSSn Hamiltonian.
      *** AMFIIN: ADDING nucleus    7 with charge   8 to the BSSn Hamiltonian.
      *** AMFIIN: ADDING nucleus    8 with charge   8 to the BSSn Hamiltonian.

                   *********************************************************************
                   ***               X2C transformation ended properly.              ***
                   ***          Calculation continues in two-component mode.         ***
                   *********************************************************************

 >>> CPU  time used in mk_h2c is 26 minutes 45 seconds
 >>> WALL time used in mk_h2c is 25 minutes 55 seconds
  Nuclear Gaussian exponent for atom of charge 109.000 :    1.1878723221D+08
  Nuclear Gaussian exponent for atom of charge   1.000 :    2.1248236111D+09
  Nuclear Gaussian exponent for atom of charge   6.000 :    6.8077493126D+08
  Nuclear Gaussian exponent for atom of charge   8.000 :    5.8631428213D+08


                      Nuclear contribution to dipole moments
                      --------------------------------------

                               au             Debye

                    x    -34.79313183    -88.43613870
                    y     60.27748565    153.21150469
                    z    -12.87243382    -32.71876610

                        1 Debye =   2.54177000 a.u. 


                       Generating Lowdin canonical matrix:
                       -----------------------------------

   L   A     * Deleted:        133(Proj:        133, Lindep:          0) Smin: 0.13E-05
 >>> CPU  time used in Lwdn_b is   2.31 seconds
 >>> WALL time used in Lwdn_b is   2.31 seconds


      **********************************************************************
      ************************* Orbital dimensions *************************
      **********************************************************************

No. of positive energy orbitals (NESH):   744
No. of negative energy orbitals (NPSH):     0
Total no. of orbitals           (NORB):   744
 >>> CPU  time used in PAMSET is 27 minutes 18 seconds
 >>> WALL time used in PAMSET is 26 minutes 32 seconds
===========================================================================
* PCMOIN: Coefficients read from formatted DFPCMO 
          and written to CHECKPOINT
===========================================================================


 *******************************************************************************
 *********************** X2C relativistic HF calculation ***********************
 *******************************************************************************


########## START ITERATION NO.   1 ##########   Tue Jul 18 10:17:42 2023


* REACMO: Coefficients read from CHECKPOINT  - Total energy: -44236.8349105433517
* Heading : Data read from the checkpoint file                                       

* GETGAB: label "GABAO0XX" not found; calling GABGEN.
SCR        scr.thr.    Step1    Step2  Coulomb  Exchange   WALL-time
AOfock:LL  1.00D-12    6.52%   64.94%    0.01%    0.02%   5.14800000s
 >>> CPU  time used in AO Fock is   4.07 seconds
 >>> WALL time used in AO Fock is   8.61 seconds
E_HOMO...E_LUMO, symmetry 1:                                 76  -0.31421   77  -0.01365
It.    1    -44236.83491053      4.42D+04  0.00D+00  1.84D-06              24.80135300s   LL             Tue Jul 18

########## START ITERATION NO.   2 ##########   Tue Jul 18 10:18:07 2023

SCR        scr.thr.    Step1    Step2  Coulomb  Exchange   WALL-time
AOfock:LL  1.00D-12    6.52%   64.94%    0.01%    0.02%   4.27600000s
 >>> CPU  time used in AO Fock is   0.25 seconds
 >>> WALL time used in AO Fock is   4.28 seconds
E_HOMO...E_LUMO, symmetry 1:                                 76  -0.31421   77  -0.01365
>>> Total wall time: 15.71000000s, and total CPU time : 17.28742900s

########## END ITERATION NO.   2 ##########   Tue Jul 18 10:18:23 2023

It.    2    -44236.83491053      3.42D-10 -1.79D-07  7.49D-08              15.71000000s   LL             Tue Jul 18

########## START ITERATION NO.   3 ##########   Tue Jul 18 10:18:23 2023

    3 *** Differential density matrix. DCOVLP     = 1.0000
SCR        scr.thr.    Step1    Step2  Coulomb  Exchange   WALL-time
AOfock:LL  1.00D-12   30.69%   64.91%    0.09%    0.20%   2.57400000s
 >>> CPU  time used in AO Fock is   0.18 seconds
 >>> WALL time used in AO Fock is   2.58 seconds
E_HOMO...E_LUMO, symmetry 1:                                 76  -0.31421   77  -0.01365
>>> Total wall time: 14.72700000s, and total CPU time : 17.25757700s

########## END ITERATION NO.   3 ##########   Tue Jul 18 10:18:37 2023

It.    3    -44236.83491054      3.85D-09  9.48D-06  1.72D-07   DIIS   2   14.72700000s   LL             Tue Jul 18

########## START ITERATION NO.   4 ##########   Tue Jul 18 10:18:37 2023

    4 *** Differential density matrix. DCOVLP     = 1.0000
SCR        scr.thr.    Step1    Step2  Coulomb  Exchange   WALL-time
AOfock:LL  1.00D-12   41.84%   56.42%    0.23%    0.64%   2.19900000s
 >>> CPU  time used in AO Fock is   0.17 seconds
 >>> WALL time used in AO Fock is   2.21 seconds
E_HOMO...E_LUMO, symmetry 1:                                 76  -0.31421   77  -0.01365
>>> Total wall time: 15.71700000s, and total CPU time : 17.24963000s

########## END ITERATION NO.   4 ##########   Tue Jul 18 10:18:53 2023

It.    4    -44236.83491053     -1.15D-08 -5.10D-06  2.04D-07   DIIS   3   15.71700000s   LL             Tue Jul 18

########## START ITERATION NO.   5 ##########   Tue Jul 18 10:18:53 2023

    5 *** Differential density matrix. DCOVLP     = 1.0000
SCR        scr.thr.    Step1    Step2  Coulomb  Exchange   WALL-time
AOfock:LL  1.00D-12   40.36%   57.56%    0.20%    0.54%   2.21100000s
 >>> CPU  time used in AO Fock is   0.16 seconds
 >>> WALL time used in AO Fock is   2.22 seconds
E_HOMO...E_LUMO, symmetry 1:                                 76  -0.31421   77  -0.01365
>>> Total wall time: 15.73500000s, and total CPU time : 17.25046500s

########## END ITERATION NO.   5 ##########   Tue Jul 18 10:19:09 2023

It.    5    -44236.83491053     -5.38D-10  4.35D-06  2.11D-07   DIIS   4   15.73500000s   LL             Tue Jul 18

########## START ITERATION NO.   6 ##########   Tue Jul 18 10:19:09 2023

    6 *** Differential density matrix. DCOVLP     = 1.0000
SCR        scr.thr.    Step1    Step2  Coulomb  Exchange   WALL-time
AOfock:LL  1.00D-12   37.36%   60.10%    0.16%    0.43%   2.33700000s
 >>> CPU  time used in AO Fock is   0.17 seconds
 >>> WALL time used in AO Fock is   2.35 seconds
E_HOMO...E_LUMO, symmetry 1:                                 76  -0.31421   77  -0.01365
>>> Total wall time: 14.06300000s, and total CPU time : 17.27862600s

########## END ITERATION NO.   6 ##########   Tue Jul 18 10:19:23 2023

It.    6    -44236.83491052     -5.50D-09  2.72D-06  3.32D-07   DIIS   5   14.06300000s   LL             Tue Jul 18

 ** Exit SCF because maximum number of iterations reached.


                                   SCF - CYCLE
                                   -----------

* Convergence on norm of error vector (gradient).
  Desired convergence:1.000D-09
  Allowed convergence:5.000D-05

* ERGVAL - convergence in total energy
* FCKVAL - convergence in maximum change in total Fock matrix
* EVCVAL - convergence in error vector (gradient)
--------------------------------------------------------------------------------------------------------------------------------
           Energy               ERGVAL    FCKVAL    EVCVAL      Conv.acc    CPU          Integrals   Time stamp
--------------------------------------------------------------------------------------------------------------------------------
It.    1    -44236.83491053      4.42D+04  0.00D+00  1.84D-06              24.80135300s   LL             Tue Jul 18
It.    2    -44236.83491053      3.42D-10 -1.79D-07  7.49D-08              15.71000000s   LL             Tue Jul 18
It.    3    -44236.83491054      3.85D-09  9.48D-06  1.72D-07   DIIS   2   14.72700000s   LL             Tue Jul 18
It.    4    -44236.83491053     -1.15D-08 -5.10D-06  2.04D-07   DIIS   3   15.71700000s   LL             Tue Jul 18
It.    5    -44236.83491053     -5.38D-10  4.35D-06  2.11D-07   DIIS   4   15.73500000s   LL             Tue Jul 18
It.    6    -44236.83491052     -5.50D-09  2.72D-06  3.32D-07   DIIS   5   14.06300000s   LL             Tue Jul 18
--------------------------------------------------------------------------------------------------------------------------------
* Desired convergence limit not reached after    6 iterations but the current convergence is acceptable.
* Average elapsed time per iteration: 
      LL           :   16.81266667s


                                   TOTAL ENERGY
                                   ------------

   Charge of molecule : 0

   Electronic energy                        :    -45376.428002424509

   Other contributions to the total energy
   Nuclear repulsion energy                 :      1139.593091903316

   Sum of all contributions to the energy
   Total energy                             :    -44236.834910521189


                                   Eigenvalues
                                   -----------


* Fermion symmetry E1 
  * Closed shell, f = 1.0000
   -6557.338497922  ( 2)      -1322.961813585  ( 2)      -1285.376185967  ( 2)       -944.067093447  ( 2)       -944.066477283  ( 2)
    -355.506717358  ( 2)       -337.574058477  ( 2)       -254.962720455  ( 2)       -254.961110620  ( 2)       -227.331857090  ( 2)
    -227.330744606  ( 2)       -212.701121507  ( 2)       -212.700334060  ( 2)       -212.699622765  ( 2)       -101.799214100  ( 2)
     -93.114666730  ( 2)        -69.616754596  ( 2)        -69.614151031  ( 2)        -56.101640583  ( 2)        -56.100112090  ( 2)
     -52.145655141  ( 2)        -52.144945950  ( 2)        -52.143437653  ( 2)        -34.350631656  ( 2)        -34.349047051  ( 2)
     -34.347929332  ( 2)        -33.208369647  ( 2)        -33.207403904  ( 2)        -33.206089497  ( 2)        -33.205328285  ( 2)
     -26.300184154  ( 2)        -22.431658391  ( 2)        -20.695333540  ( 2)        -20.678071668  ( 2)        -20.677973339  ( 2)
     -15.931306892  ( 2)        -15.927133311  ( 2)        -11.432485131  ( 2)        -11.430385607  ( 2)        -11.430341610  ( 2)
     -10.214177854  ( 2)        -10.212488043  ( 2)         -9.245008276  ( 2)         -9.243654895  ( 2)         -9.240898918  ( 2)
      -4.775741726  ( 2)         -3.384355618  ( 2)         -2.474944428  ( 2)         -2.470642587  ( 2)         -2.469143599  ( 2)
      -2.281281700  ( 2)         -2.277969163  ( 2)         -2.274756120  ( 2)         -2.273745419  ( 2)         -2.062182771  ( 2)
      -2.049729308  ( 2)         -1.524750654  ( 2)         -1.505211801  ( 2)         -1.504932969  ( 2)         -0.868904000  ( 2)
      -0.831584710  ( 2)         -0.820388040  ( 2)         -0.744224897  ( 2)         -0.705411848  ( 2)         -0.687993070  ( 2)
      -0.665463482  ( 2)         -0.660544347  ( 2)         -0.650615239  ( 2)         -0.645146765  ( 2)         -0.641634500  ( 2)
      -0.638754015  ( 2)         -0.482657775  ( 2)         -0.446788313  ( 2)         -0.437750090  ( 2)         -0.382070606  ( 2)
      -0.314206756  ( 2)
  * Virtual eigenvalues, f = 0.0000
      -0.013649294  ( 2)          0.058948282  ( 2)          0.059724282  ( 2)          0.070257804  ( 2)          0.074719265  ( 2)
       0.101974018  ( 2)          0.105215216  ( 2)          0.124055993  ( 2)          0.130814744  ( 2)          0.145063969  ( 2)
       0.151222237  ( 2)          0.168037066  ( 2)          0.187556145  ( 2)          0.192301037  ( 2)          0.198529118  ( 2)
       0.216979900  ( 2)          0.229516360  ( 2)          0.238933641  ( 2)          0.258237282  ( 2)          0.259841916  ( 2)
       0.261498942  ( 2)          0.276443293  ( 2)          0.282144415  ( 2)          0.288721111  ( 2)          0.309192497  ( 2)
       0.316381890  ( 2)          0.323700511  ( 2)          0.349544673  ( 2)          0.371368125  ( 2)          0.385521958  ( 2)
       0.402016138  ( 2)          0.432597379  ( 2)          0.445282265  ( 2)          0.456349558  ( 2)          0.469428979  ( 2)
       0.478647995  ( 2)          0.488164007  ( 2)          0.500795856  ( 2)          0.515841501  ( 2)          0.518519322  ( 2)
       0.525805099  ( 2)          0.535916865  ( 2)          0.538026792  ( 2)          0.549577456  ( 2)          0.556888531  ( 2)
       0.572020094  ( 2)          0.579099741  ( 2)          0.590874055  ( 2)          0.596566675  ( 2)          0.636070618  ( 2)
       0.686753792  ( 2)          0.759449330  ( 2)          0.776046578  ( 2)          0.778931928  ( 2)          0.819867403  ( 2)
       0.839981375  ( 2)          0.843862891  ( 2)          0.871737917  ( 2)          0.893090949  ( 2)          0.901092184  ( 2)
       0.907213241  ( 2)          0.910274338  ( 2)          0.914054887  ( 2)          0.923288083  ( 2)          0.930566992  ( 2)
       0.941797213  ( 2)          0.951062468  ( 2)          0.963955189  ( 2)          0.969923754  ( 2)          0.990336951  ( 2)
       0.995963558  ( 2)          1.004019720  ( 2)          1.016114513  ( 2)          1.017717749  ( 2)          1.040614095  ( 2)
       1.051766000  ( 2)          1.057053918  ( 2)          1.076216816  ( 2)          1.091722987  ( 2)          1.101980141  ( 2)
       1.121147399  ( 2)          1.141936730  ( 2)          1.150829054  ( 2)          1.177350098  ( 2)          1.190909090  ( 2)
       1.199445845  ( 2)          1.212122914  ( 2)          1.214332945  ( 2)          1.246510762  ( 2)          1.265358817  ( 2)
       1.289254506  ( 2)          1.306425970  ( 2)          1.358292351  ( 2)          1.383631905  ( 2)          1.408792729  ( 2)
       1.425417833  ( 2)          1.505737315  ( 2)          1.540433838  ( 2)          1.595904604  ( 2)          1.606909937  ( 2)
       1.614531974  ( 2)          1.622609244  ( 2)          1.641631848  ( 2)          1.654327239  ( 2)          1.668874517  ( 2)
       1.674142809  ( 2)          1.684991701  ( 2)          1.725476084  ( 2)          1.747833681  ( 2)          1.755968623  ( 2)
       1.779909426  ( 2)          1.812982420  ( 2)          1.818827741  ( 2)          1.820513260  ( 2)          1.847535893  ( 2)
       1.881259398  ( 2)          1.908916570  ( 2)          1.913208674  ( 2)          1.919609844  ( 2)          1.938622992  ( 2)
       1.949266281  ( 2)          1.957563998  ( 2)          2.042655526  ( 2)          2.144705434  ( 2)          2.335344938  ( 2)
       2.369013671  ( 2)          2.378421000  ( 2)          2.391550816  ( 2)          2.429700829  ( 2)          2.461788110  ( 2)
       2.482111559  ( 2)          2.511296513  ( 2)          2.532178789  ( 2)          2.539338659  ( 2)          2.560899598  ( 2)
       2.575846062  ( 2)          2.620211993  ( 2)          2.636202906  ( 2)          2.640640772  ( 2)          2.650245082  ( 2)
       2.659838868  ( 2)          2.679321374  ( 2)          2.682871879  ( 2)          2.697644849  ( 2)          2.711599481  ( 2)
       2.724168398  ( 2)          2.728678682  ( 2)          2.734768441  ( 2)          2.754733576  ( 2)          2.758469939  ( 2)
       2.769188577  ( 2)          2.776519846  ( 2)          2.801296587  ( 2)          2.812231554  ( 2)          2.829261147  ( 2)
       2.831057498  ( 2)          2.874418055  ( 2)          2.904268069  ( 2)          2.929887710  ( 2)          2.967719825  ( 2)
       2.986671193  ( 2)          3.002306184  ( 2)          3.009160734  ( 2)          3.027482551  ( 2)          3.037530147  ( 2)
       3.047183528  ( 2)          3.067563214  ( 2)          3.079818160  ( 2)          3.104545733  ( 2)          3.111793382  ( 2)
       3.125137286  ( 2)          3.142929105  ( 2)          3.170816121  ( 2)          3.176984474  ( 2)          3.198364996  ( 2)
       3.206593332  ( 2)          3.216699907  ( 2)          3.225677106  ( 2)          3.246497878  ( 2)          3.261510520  ( 2)
       3.290073256  ( 2)          3.325563454  ( 2)          3.346505693  ( 2)          3.370202968  ( 2)          3.426284015  ( 2)
       3.448356266  ( 2)          3.477677697  ( 2)          3.511401729  ( 2)          3.535313597  ( 2)          3.561190935  ( 2)
       3.607191287  ( 2)          3.645064602  ( 2)          3.658271942  ( 2)          3.701664872  ( 2)          3.739045246  ( 2)
       3.760477813  ( 2)          3.778371518  ( 2)          3.796583457  ( 2)          3.812608317  ( 2)          3.892218197  ( 2)
       3.903480404  ( 2)          3.941318689  ( 2)          3.948196916  ( 2)          4.023400211  ( 2)          4.036070321  ( 2)
       4.104213817  ( 2)          4.120217394  ( 2)          4.140832401  ( 2)          4.170546161  ( 2)          4.185919336  ( 2)
       4.286212995  ( 2)          4.313904800  ( 2)          4.388729742  ( 2)          4.654361531  ( 2)          4.815422156  ( 2)
       4.829264572  ( 2)          4.831457019  ( 2)          4.839964371  ( 2)          4.849748919  ( 2)          4.857718389  ( 2)
       4.914645384  ( 2)          5.158557452  ( 2)          5.325866090  ( 2)          5.433024376  ( 2)          5.433958663  ( 2)
       5.449534688  ( 2)          5.450791737  ( 2)          5.451410587  ( 2)          5.451916629  ( 2)          5.591967958  ( 2)
       5.615716132  ( 2)          5.641693985  ( 2)          5.809090350  ( 2)          5.811772752  ( 2)          5.831380044  ( 2)
       5.841828808  ( 2)          5.850188790  ( 2)          5.855261703  ( 2)          6.087551212  ( 2)          6.095313410  ( 2)
       6.102792892  ( 2)          6.104472956  ( 2)          6.108981483  ( 2)          6.148570923  ( 2)          6.159023877  ( 2)
       6.277612995  ( 2)          6.338547890  ( 2)          6.402914835  ( 2)          6.484274538  ( 2)          6.498644530  ( 2)
       6.600190503  ( 2)          6.603240647  ( 2)          6.611633876  ( 2)          6.624405027  ( 2)          6.627745045  ( 2)
       6.631114149  ( 2)          6.632612010  ( 2)          6.740382471  ( 2)          6.762859552  ( 2)          6.799247037  ( 2)
       6.827419614  ( 2)          6.889348790  ( 2)          6.901584979  ( 2)          7.012055978  ( 2)          7.219156653  ( 2)
       7.225761859  ( 2)          7.243997432  ( 2)          7.253440576  ( 2)          7.264466391  ( 2)          7.281383901  ( 2)
       7.366595337  ( 2)          7.376734809  ( 2)          7.383619223  ( 2)          7.389143830  ( 2)          7.422974055  ( 2)
       7.460921133  ( 2)          7.544895702  ( 2)          7.617098385  ( 2)          7.677909248  ( 2)          7.697215515  ( 2)
       7.704316526  ( 2)          7.782127365  ( 2)          7.983378218  ( 2)          8.072364148  ( 2)          8.203226888  ( 2)
       8.953380162  ( 2)          8.969727123  ( 2)          9.013580450  ( 2)          9.201560727  ( 2)          9.338359563  ( 2)
       9.373218577  ( 2)          9.491642337  ( 2)          9.683709409  ( 2)          9.827467417  ( 2)          9.888973337  ( 2)
       9.936778981  ( 2)         10.000972478  ( 2)         10.252505611  ( 2)         10.343868622  ( 2)         10.506109856  ( 2)
      10.705889329  ( 2)         10.832813112  ( 2)         11.165991561  ( 2)         13.286131209  ( 2)         13.302857745  ( 2)
      13.304613989  ( 2)         13.311724858  ( 2)         13.319804009  ( 2)         13.325802767  ( 2)         14.211354507  ( 2)
      14.232650199  ( 2)         14.267019496  ( 2)         16.237655873  ( 2)         16.272111681  ( 2)         16.290501012  ( 2)
      16.305441216  ( 2)         16.352961425  ( 2)         16.373834560  ( 2)         16.422089990  ( 2)         16.533614427  ( 2)
      16.653205590  ( 2)         16.981170294  ( 2)         17.014542736  ( 2)         18.334965036  ( 2)         18.435790282  ( 2)
      18.446411348  ( 2)         18.491776827  ( 2)         18.495794343  ( 2)         18.497673465  ( 2)         18.499500811  ( 2)
      18.503700049  ( 2)         18.533481894  ( 2)         18.536251754  ( 2)         18.538967886  ( 2)         18.540155716  ( 2)
      18.541916493  ( 2)         18.545673206  ( 2)         20.938887191  ( 2)         21.099613995  ( 2)         21.177503329  ( 2)
      21.180995493  ( 2)         21.186130411  ( 2)         21.206598753  ( 2)         21.208856793  ( 2)         21.255150682  ( 2)
      21.923298799  ( 2)         21.943588191  ( 2)         21.984433315  ( 2)         22.509986733  ( 2)         23.525538367  ( 2)
      23.670331747  ( 2)         23.764728427  ( 2)         23.939259975  ( 2)         24.031409928  ( 2)         24.226144577  ( 2)
      24.280732340  ( 2)         26.122629364  ( 2)         26.236774596  ( 2)         27.276065380  ( 2)         27.415405082  ( 2)
      27.756983389  ( 2)         29.716466197  ( 2)         29.720749982  ( 2)         29.749999285  ( 2)         38.857859810  ( 2)
      38.873646651  ( 2)         38.881090953  ( 2)         38.885580801  ( 2)         38.901758481  ( 2)         38.906473062  ( 2)
      39.597438350  ( 2)         39.636936344  ( 2)         39.666445804  ( 2)         40.208846120  ( 2)         45.444701756  ( 2)
      49.481675997  ( 2)         49.559443140  ( 2)         49.585045017  ( 2)         55.833092706  ( 2)         55.971834274  ( 2)
      56.282902051  ( 2)         56.414688810  ( 2)         56.511380961  ( 2)         57.088259376  ( 2)         57.148561802  ( 2)
      57.305442122  ( 2)         57.341423523  ( 2)         59.566690428  ( 2)         59.569493833  ( 2)         59.570247520  ( 2)
      59.590298628  ( 2)         59.592672839  ( 2)         59.594047969  ( 2)         59.645925632  ( 2)         59.648285590  ( 2)
      59.649599592  ( 2)         59.659624797  ( 2)         59.660913699  ( 2)         59.663563501  ( 2)         59.941955302  ( 2)
      59.947335352  ( 2)         59.948446856  ( 2)         65.799374241  ( 2)         65.922369492  ( 2)         68.613848520  ( 2)
      68.723405847  ( 2)         68.958416992  ( 2)         69.281772288  ( 2)         69.285454407  ( 2)         69.301648715  ( 2)
      69.320072391  ( 2)         69.328760806  ( 2)         69.359508319  ( 2)         69.887183565  ( 2)         69.899194289  ( 2)
      69.937039662  ( 2)         75.689266282  ( 2)         81.967700064  ( 2)         81.969608620  ( 2)         81.996994644  ( 2)
      91.288580072  ( 2)        106.706538407  ( 2)        106.723941744  ( 2)        106.724838644  ( 2)        106.764861835  ( 2)
     106.780568837  ( 2)        106.782193686  ( 2)        106.792992014  ( 2)        106.810599937  ( 2)        106.811024522  ( 2)
     106.832130989  ( 2)        106.847822428  ( 2)        106.849211484  ( 2)        106.898513685  ( 2)        106.914138298  ( 2)
     106.915629147  ( 2)        116.166622916  ( 2)        127.047593347  ( 2)        127.063273624  ( 2)        127.068701990  ( 2)
     127.177848143  ( 2)        127.194167263  ( 2)        127.198332459  ( 2)        127.725641717  ( 2)        127.733487355  ( 2)
     127.761659346  ( 2)        128.380255959  ( 2)        128.484417578  ( 2)        128.560225881  ( 2)        130.192171422  ( 2)
     130.239497915  ( 2)        130.355474614  ( 2)        130.392240134  ( 2)        131.241010171  ( 2)        131.306758144  ( 2)
     131.332819207  ( 2)        141.473414694  ( 2)        141.615272217  ( 2)        157.019064716  ( 2)        157.131211299  ( 2)
     163.410399982  ( 2)        163.499116877  ( 2)        163.683870958  ( 2)        200.213836876  ( 2)        220.673724827  ( 2)
     220.676190760  ( 2)        220.701419163  ( 2)        283.178570547  ( 2)        284.526270725  ( 2)        284.531799415  ( 2)
     284.547983821  ( 2)        284.801815570  ( 2)        284.880130049  ( 2)        284.937281836  ( 2)        285.017335003  ( 2)
     285.021545579  ( 2)        285.037259147  ( 2)        285.338403043  ( 2)        285.345764742  ( 2)        285.370224233  ( 2)
     289.161213880  ( 2)        289.198008085  ( 2)        289.289057097  ( 2)        289.316246296  ( 2)        306.441222817  ( 2)
     341.732835430  ( 2)        341.858035046  ( 2)        351.120346347  ( 2)        351.182171272  ( 2)        351.205149800  ( 2)
     356.456792048  ( 2)        356.551476459  ( 2)        370.426481807  ( 2)        370.498650904  ( 2)        370.645944381  ( 2)
     422.024935800  ( 2)        520.970298232  ( 2)        520.985563375  ( 2)        520.989944943  ( 2)        522.756865098  ( 2)
     522.772801625  ( 2)        522.775776629  ( 2)        523.053479564  ( 2)        523.064161098  ( 2)        523.079233158  ( 2)
     594.910974921  ( 2)        594.914537110  ( 2)        594.937478236  ( 2)        626.318483324  ( 2)        626.379404603  ( 2)
     626.423380016  ( 2)        637.242095930  ( 2)        637.269895290  ( 2)        637.338166644  ( 2)        637.358466147  ( 2)
     659.720401795  ( 2)        774.298514595  ( 2)        774.377133153  ( 2)        789.254867706  ( 2)        789.360943149  ( 2)
     804.759910061  ( 2)        804.818738770  ( 2)        804.935979140  ( 2)        847.929531971  ( 2)        968.751424239  ( 2)
     968.808268863  ( 2)        968.827864663  ( 2)       1393.251251309  ( 2)       1393.294918778  ( 2)       1393.326380619  ( 2)
    1423.001245653  ( 2)       1423.021154234  ( 2)       1423.069949309  ( 2)       1423.084456567  ( 2)       1467.065802885  ( 2)
    1625.351182987  ( 2)       1625.410506321  ( 2)       1625.988423223  ( 2)       1636.860468125  ( 2)       1636.865448056  ( 2)
    1636.885660774  ( 2)       1693.063321512  ( 2)       1693.108837053  ( 2)       1693.201906643  ( 2)       1721.484911992  ( 2)
    1741.267455247  ( 2)       1741.355367539  ( 2)       2785.385485486  ( 2)       2785.434255523  ( 2)       2785.450158136  ( 2)
    2997.678013677  ( 2)       3095.337337786  ( 2)       3220.866591892  ( 2)       3220.894175892  ( 2)       3220.913841025  ( 2)
    3313.368102792  ( 2)       3313.380607721  ( 2)       3313.411295160  ( 2)       3313.420241332  ( 2)       3323.633609233  ( 2)
    3323.680326023  ( 2)       3476.763168398  ( 2)       3476.797551958  ( 2)       3476.867496988  ( 2)       3650.431463110  ( 2)
    3650.500394239  ( 2)       4608.638901929  ( 2)       4608.646109722  ( 2)       4608.662301144  ( 2)       5345.642872570  ( 2)
    6161.689686193  ( 2)       6648.123721442  ( 2)       6648.156332388  ( 2)       6995.623771557  ( 2)       6995.647598528  ( 2)
    6995.696094293  ( 2)       7236.399752916  ( 2)       7236.451012955  ( 2)       8131.055809343  ( 2)       8131.068527375  ( 2)
    8131.077523886  ( 2)       8259.752647198  ( 2)       8259.789007298  ( 2)       8259.799589956  ( 2)       8496.574970509  ( 4)
    8496.594629806  ( 4)       9215.999285350  ( 2)      11572.816168136  ( 2)      13018.800197381  ( 4)      13018.821252325  ( 2)
   13054.623183945  ( 2)      13054.643578146  ( 2)      13558.146988422  ( 2)      13558.182871715  ( 2)      13838.210922907  ( 2)
   13838.225523639  ( 2)      13838.255451182  ( 2)      15323.796532058  ( 2)      20640.822629269  ( 2)      24142.187743475  ( 4)
   24614.338656208  ( 2)      24662.721653284  ( 4)      24662.747519514  ( 2)      25431.753538043  ( 4)      27205.789807021  ( 6)
   35335.729096064  ( 2)      36429.578274350  ( 6)      38429.669558078  ( 2)      41269.093178097  ( 4)      50429.460408174  ( 4)
   54622.393838115  ( 6)      58766.376507096  ( 2)      58816.290523867  ( 2)      68514.011098404  ( 4)      77928.898405806  ( 6)
   89054.064651450  ( 2)      96026.753291635  ( 2)     108318.858020028  ( 4)     109497.731584153  ( 6)     111667.784907253  ( 4)
  119976.793212389  ( 6)     134597.427826074  ( 2)     155727.799440202  ( 2)     180318.169322341  ( 4)     204898.280562911  ( 2)
  252881.472304995  ( 2)     290656.849173520  ( 4)     317285.022837505  ( 2)     414857.789652770  ( 2)     470809.206848503  ( 4)
  506428.721401219  ( 2)     693756.051610238  ( 2)     771504.324921892  ( 4)     850925.229226952  ( 2)    1199655.212021140  ( 2)
 1294803.739323562  ( 4)    1574673.472287096  ( 2)    2156936.061369189  ( 2)    2259175.352228879  ( 4)
* HOMO - LUMO gap:

    E(LUMO) :    -0.01364929 au (symmetry E1 )
  - E(HOMO) :    -0.31420676 au (symmetry E1 )
  ------------------------------------------
    gap     :     0.30055746 au

 
 
                    *****************************************************
                    ***     Entering the ExaCorr module in DIRAC      ***
                    ***                                               ***
                    ***  Authors:         - Lucas Visscher            ***
                    ***                   - Anastasios Papadopoulos   ***
                    ***  Contributors:    - Johann Pototschnig        ***
                    ***                   - Michal Repisky            ***
                    ***                   - Andre Severo Pereira Gomes***
                    ***                   - Loic Halbert              ***
                    ***                   - Xiang Yuan                ***
                    ***                   - Chima Chibueze            ***
                    ***  Features:        - CCD/CC2/CCSD              ***
                    ***                   - MP2 NOs                   ***
                    ***                   - 1DM                       ***
                    ***                   - ReSpect interface         ***
                    ***                                               ***
                    ***  Relativistic Coupled Cluster code using the  ***
                    ***  TALSH and ExaTensor libraries developed by   ***
                    ***  Dmitry Lyakh                                 ***
                    ***                                               ***
                    ***  Reference DOI: 10.1021/acs.jctc.1c00260      ***
                    ***                                               ***
                    *****************************************************
 
 
  18 Jul 23 10:19:23 Found Dirac SCF orbitals file. using it.
  18 Jul 23 10:19:23 scf data read from the checkpoint file
  18 Jul 23 10:19:23 retrieved basis set information
  18 Jul 23 10:19:23 Initialized global data
  --- read active occupied spinors --- 
    #             E   
      71  -0.159313069E+02
      72  -0.159313069E+02
      73  -0.159271333E+02
      74  -0.159271333E+02
      75  -0.114324851E+02
      76  -0.114324851E+02
      77  -0.114303856E+02
      78  -0.114303856E+02
      79  -0.114303416E+02
      80  -0.114303416E+02
      81  -0.102141779E+02
      82  -0.102141779E+02
      83  -0.102124880E+02
      84  -0.102124880E+02
      85  -0.924500828E+01
      86  -0.924500828E+01
      87  -0.924365490E+01
      88  -0.924365490E+01
      89  -0.924089892E+01
      90  -0.924089892E+01
      91  -0.477574173E+01
      92  -0.477574173E+01
      93  -0.338435562E+01
      94  -0.338435562E+01
      95  -0.247494443E+01
      96  -0.247494443E+01
      97  -0.247064259E+01
      98  -0.247064259E+01
      99  -0.246914360E+01
     100  -0.246914360E+01
     101  -0.228128170E+01
     102  -0.228128170E+01
     103  -0.227796916E+01
     104  -0.227796916E+01
     105  -0.227475612E+01
     106  -0.227475612E+01
     107  -0.227374542E+01
     108  -0.227374542E+01
     109  -0.206218277E+01
     110  -0.206218277E+01
     111  -0.204972931E+01
     112  -0.204972931E+01
     113  -0.152475065E+01
     114  -0.152475065E+01
     115  -0.150521180E+01
     116  -0.150521180E+01
     117  -0.150493297E+01
     118  -0.150493297E+01
     119  -0.868904000E+00
     120  -0.868904000E+00
     121  -0.831584710E+00
     122  -0.831584710E+00
     123  -0.820388040E+00
     124  -0.820388040E+00
     125  -0.744224897E+00
     126  -0.744224897E+00
     127  -0.705411848E+00
     128  -0.705411848E+00
     129  -0.687993070E+00
     130  -0.687993070E+00
     131  -0.665463482E+00
     132  -0.665463482E+00
     133  -0.660544347E+00
     134  -0.660544347E+00
     135  -0.650615239E+00
     136  -0.650615239E+00
     137  -0.645146765E+00
     138  -0.645146765E+00
     139  -0.641634500E+00
     140  -0.641634500E+00
     141  -0.638754015E+00
     142  -0.638754015E+00
     143  -0.482657775E+00
     144  -0.482657775E+00
     145  -0.446788313E+00
     146  -0.446788313E+00
     147  -0.437750090E+00
     148  -0.437750090E+00
     149  -0.382070606E+00
     150  -0.382070606E+00
     151  -0.314206756E+00
     152  -0.314206756E+00
  --- read active virtual spinors --- 
    #             E   
     153  -0.136492943E-01
     154  -0.136492943E-01
     155   0.589482820E-01
     156   0.589482820E-01
     157   0.597242820E-01
     158   0.597242820E-01
     159   0.702578035E-01
     160   0.702578035E-01
     161   0.747192645E-01
     162   0.747192645E-01
     163   0.101974018E+00
     164   0.101974018E+00
     165   0.105215216E+00
     166   0.105215216E+00
     167   0.124055993E+00
     168   0.124055993E+00
     169   0.130814744E+00
     170   0.130814744E+00
     171   0.145063969E+00
     172   0.145063969E+00
     173   0.151222237E+00
     174   0.151222237E+00
     175   0.168037066E+00
     176   0.168037066E+00
     177   0.187556145E+00
     178   0.187556145E+00
     179   0.192301037E+00
     180   0.192301037E+00
     181   0.198529118E+00
     182   0.198529118E+00
     183   0.216979900E+00
     184   0.216979900E+00
     185   0.229516360E+00
     186   0.229516360E+00
     187   0.238933641E+00
     188   0.238933641E+00
     189   0.258237282E+00
     190   0.258237282E+00
     191   0.259841916E+00
     192   0.259841916E+00
     193   0.261498942E+00
     194   0.261498942E+00
     195   0.276443293E+00
     196   0.276443293E+00
     197   0.282144415E+00
     198   0.282144415E+00
     199   0.288721111E+00
     200   0.288721111E+00
     201   0.309192497E+00
     202   0.309192497E+00
     203   0.316381890E+00
     204   0.316381890E+00
     205   0.323700511E+00
     206   0.323700511E+00
     207   0.349544673E+00
     208   0.349544673E+00
     209   0.371368125E+00
     210   0.371368125E+00
     211   0.385521958E+00
     212   0.385521958E+00
     213   0.402016138E+00
     214   0.402016138E+00
     215   0.432597379E+00
     216   0.432597379E+00
     217   0.445282265E+00
     218   0.445282265E+00
     219   0.456349558E+00
     220   0.456349558E+00
     221   0.469428979E+00
     222   0.469428979E+00
     223   0.478647995E+00
     224   0.478647995E+00
     225   0.488164007E+00
     226   0.488164007E+00
     227   0.500795856E+00
     228   0.500795856E+00
     229   0.515841501E+00
     230   0.515841501E+00
     231   0.518519322E+00
     232   0.518519322E+00
     233   0.525805099E+00
     234   0.525805099E+00
     235   0.535916865E+00
     236   0.535916865E+00
     237   0.538026792E+00
     238   0.538026792E+00
     239   0.549577456E+00
     240   0.549577456E+00
     241   0.556888531E+00
     242   0.556888531E+00
     243   0.572020094E+00
     244   0.572020094E+00
     245   0.579099741E+00
     246   0.579099741E+00
     247   0.590874055E+00
     248   0.590874055E+00
     249   0.596566675E+00
     250   0.596566675E+00
     251   0.636070618E+00
     252   0.636070618E+00
     253   0.686753792E+00
     254   0.686753792E+00
     255   0.759449330E+00
     256   0.759449330E+00
     257   0.776046578E+00
     258   0.776046578E+00
     259   0.778931928E+00
     260   0.778931928E+00
     261   0.819867403E+00
     262   0.819867403E+00
     263   0.839981375E+00
     264   0.839981375E+00
     265   0.843862891E+00
     266   0.843862891E+00
     267   0.871737917E+00
     268   0.871737917E+00
     269   0.893090949E+00
     270   0.893090949E+00
     271   0.901092184E+00
     272   0.901092184E+00
     273   0.907213241E+00
     274   0.907213241E+00
     275   0.910274338E+00
     276   0.910274338E+00
     277   0.914054887E+00
     278   0.914054887E+00
     279   0.923288083E+00
     280   0.923288083E+00
     281   0.930566992E+00
     282   0.930566992E+00
     283   0.941797213E+00
     284   0.941797213E+00
     285   0.951062468E+00
     286   0.951062468E+00
     287   0.963955189E+00
     288   0.963955189E+00
     289   0.969923754E+00
     290   0.969923754E+00
     291   0.990336951E+00
     292   0.990336951E+00
     293   0.995963558E+00
     294   0.995963558E+00
     295   0.100401972E+01
     296   0.100401972E+01
     297   0.101611451E+01
     298   0.101611451E+01
     299   0.101771775E+01
     300   0.101771775E+01
     301   0.104061410E+01
     302   0.104061410E+01
     303   0.105176600E+01
     304   0.105176600E+01
     305   0.105705392E+01
     306   0.105705392E+01
     307   0.107621682E+01
     308   0.107621682E+01
     309   0.109172299E+01
     310   0.109172299E+01
     311   0.110198014E+01
     312   0.110198014E+01
     313   0.112114740E+01
     314   0.112114740E+01
     315   0.114193673E+01
     316   0.114193673E+01
     317   0.115082905E+01
     318   0.115082905E+01
     319   0.117735010E+01
     320   0.117735010E+01
     321   0.119090909E+01
     322   0.119090909E+01
     323   0.119944584E+01
     324   0.119944584E+01
     325   0.121212291E+01
     326   0.121212291E+01
     327   0.121433294E+01
     328   0.121433294E+01
     329   0.124651076E+01
     330   0.124651076E+01
     331   0.126535882E+01
     332   0.126535882E+01
     333   0.128925451E+01
     334   0.128925451E+01
     335   0.130642597E+01
     336   0.130642597E+01
     337   0.135829235E+01
     338   0.135829235E+01
     339   0.138363190E+01
     340   0.138363190E+01
     341   0.140879273E+01
     342   0.140879273E+01
     343   0.142541783E+01
     344   0.142541783E+01
     345   0.150573731E+01
     346   0.150573731E+01
     347   0.154043384E+01
     348   0.154043384E+01
     349   0.159590460E+01
     350   0.159590460E+01
     351   0.160690994E+01
     352   0.160690994E+01
     353   0.161453197E+01
     354   0.161453197E+01
     355   0.162260924E+01
     356   0.162260924E+01
     357   0.164163185E+01
     358   0.164163185E+01
     359   0.165432724E+01
     360   0.165432724E+01
     361   0.166887452E+01
     362   0.166887452E+01
     363   0.167414281E+01
     364   0.167414281E+01
     365   0.168499170E+01
     366   0.168499170E+01
     367   0.172547608E+01
     368   0.172547608E+01
     369   0.174783368E+01
     370   0.174783368E+01
     371   0.175596862E+01
     372   0.175596862E+01
     373   0.177990943E+01
     374   0.177990943E+01
     375   0.181298242E+01
     376   0.181298242E+01
     377   0.181882774E+01
     378   0.181882774E+01
     379   0.182051326E+01
     380   0.182051326E+01
     381   0.184753589E+01
     382   0.184753589E+01
     383   0.188125940E+01
     384   0.188125940E+01
     385   0.190891657E+01
     386   0.190891657E+01
     387   0.191320867E+01
     388   0.191320867E+01
     389   0.191960984E+01
     390   0.191960984E+01
     391   0.193862299E+01
     392   0.193862299E+01
     393   0.194926628E+01
     394   0.194926628E+01
     395   0.195756400E+01
     396   0.195756400E+01
     397   0.204265553E+01
     398   0.204265553E+01
     399   0.214470543E+01
     400   0.214470543E+01
     401   0.233534494E+01
     402   0.233534494E+01
     403   0.236901367E+01
     404   0.236901367E+01
     405   0.237842100E+01
     406   0.237842100E+01
     407   0.239155082E+01
     408   0.239155082E+01
     409   0.242970083E+01
     410   0.242970083E+01
     411   0.246178811E+01
     412   0.246178811E+01
     413   0.248211156E+01
     414   0.248211156E+01
     415   0.251129651E+01
     416   0.251129651E+01
     417   0.253217879E+01
     418   0.253217879E+01
     419   0.253933866E+01
     420   0.253933866E+01
     421   0.256089960E+01
     422   0.256089960E+01
     423   0.257584606E+01
     424   0.257584606E+01
     425   0.262021199E+01
     426   0.262021199E+01
     427   0.263620291E+01
     428   0.263620291E+01
     429   0.264064077E+01
     430   0.264064077E+01
     431   0.265024508E+01
     432   0.265024508E+01
     433   0.265983887E+01
     434   0.265983887E+01
     435   0.267932137E+01
     436   0.267932137E+01
     437   0.268287188E+01
     438   0.268287188E+01
     439   0.269764485E+01
     440   0.269764485E+01
     441   0.271159948E+01
     442   0.271159948E+01
     443   0.272416840E+01
     444   0.272416840E+01
     445   0.272867868E+01
     446   0.272867868E+01
     447   0.273476844E+01
     448   0.273476844E+01
     449   0.275473358E+01
     450   0.275473358E+01
     451   0.275846994E+01
     452   0.275846994E+01
     453   0.276918858E+01
     454   0.276918858E+01
     455   0.277651985E+01
     456   0.277651985E+01
     457   0.280129659E+01
     458   0.280129659E+01
     459   0.281223155E+01
     460   0.281223155E+01
     461   0.282926115E+01
     462   0.282926115E+01
     463   0.283105750E+01
     464   0.283105750E+01
     465   0.287441806E+01
     466   0.287441806E+01
     467   0.290426807E+01
     468   0.290426807E+01
     469   0.292988771E+01
     470   0.292988771E+01
     471   0.296771982E+01
     472   0.296771982E+01
     473   0.298667119E+01
     474   0.298667119E+01
     475   0.300230618E+01
     476   0.300230618E+01
     477   0.300916073E+01
     478   0.300916073E+01
     479   0.302748255E+01
     480   0.302748255E+01
     481   0.303753015E+01
     482   0.303753015E+01
     483   0.304718353E+01
     484   0.304718353E+01
     485   0.306756321E+01
     486   0.306756321E+01
     487   0.307981816E+01
     488   0.307981816E+01
     489   0.310454573E+01
     490   0.310454573E+01
     491   0.311179338E+01
     492   0.311179338E+01
     493   0.312513729E+01
     494   0.312513729E+01
     495   0.314292910E+01
     496   0.314292910E+01
     497   0.317081612E+01
     498   0.317081612E+01
     499   0.317698447E+01
     500   0.317698447E+01
     501   0.319836500E+01
     502   0.319836500E+01
     503   0.320659333E+01
     504   0.320659333E+01
     505   0.321669991E+01
     506   0.321669991E+01
     507   0.322567711E+01
     508   0.322567711E+01
     509   0.324649788E+01
     510   0.324649788E+01
     511   0.326151052E+01
     512   0.326151052E+01
     513   0.329007326E+01
     514   0.329007326E+01
     515   0.332556345E+01
     516   0.332556345E+01
     517   0.334650569E+01
     518   0.334650569E+01
     519   0.337020297E+01
     520   0.337020297E+01
     521   0.342628402E+01
     522   0.342628402E+01
     523   0.344835627E+01
     524   0.344835627E+01
     525   0.347767770E+01
     526   0.347767770E+01
     527   0.351140173E+01
     528   0.351140173E+01
     529   0.353531360E+01
     530   0.353531360E+01
     531   0.356119093E+01
     532   0.356119093E+01
     533   0.360719129E+01
     534   0.360719129E+01
     535   0.364506460E+01
     536   0.364506460E+01
     537   0.365827194E+01
     538   0.365827194E+01
     539   0.370166487E+01
     540   0.370166487E+01
     541   0.373904525E+01
     542   0.373904525E+01
     543   0.376047781E+01
     544   0.376047781E+01
     545   0.377837152E+01
     546   0.377837152E+01
     547   0.379658346E+01
     548   0.379658346E+01
     549   0.381260832E+01
     550   0.381260832E+01
     551   0.389221820E+01
     552   0.389221820E+01
     553   0.390348040E+01
     554   0.390348040E+01
     555   0.394131869E+01
     556   0.394131869E+01
     557   0.394819692E+01
     558   0.394819692E+01
     559   0.402340021E+01
     560   0.402340021E+01
     561   0.403607032E+01
     562   0.403607032E+01
     563   0.410421382E+01
     564   0.410421382E+01
     565   0.412021739E+01
     566   0.412021739E+01
     567   0.414083240E+01
     568   0.414083240E+01
     569   0.417054616E+01
     570   0.417054616E+01
     571   0.418591934E+01
     572   0.418591934E+01
     573   0.428621299E+01
     574   0.428621299E+01
     575   0.431390480E+01
     576   0.431390480E+01
     577   0.438872974E+01
     578   0.438872974E+01
     579   0.465436153E+01
     580   0.465436153E+01
     581   0.481542216E+01
     582   0.481542216E+01
     583   0.482926457E+01
     584   0.482926457E+01
     585   0.483145702E+01
     586   0.483145702E+01
     587   0.483996437E+01
     588   0.483996437E+01
     589   0.484974892E+01
     590   0.484974892E+01
     591   0.485771839E+01
     592   0.485771839E+01
     593   0.491464538E+01
     594   0.491464538E+01
  18 Jul 23 10:19:23  Starting CC calculation with exatensor
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node   1 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  62 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node   2 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
number of blocks =       144; number of MPI processes =      200
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

  Master node : --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---         

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  14 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  63 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  31 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 126 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  15 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node   6 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 127 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  78 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  70 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 159 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  10 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 191 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 134 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  30 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  66 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node   7 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 190 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  71 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  95 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 130 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 128 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node   3 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  64 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  79 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  23 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  11 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  46 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 135 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 142 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  67 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 129 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  94 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  65 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  18 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 131 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  38 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node   8 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 174 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 158 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  34 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node   4 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 110 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  16 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 150 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 143 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  54 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  47 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  32 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  40 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  12 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 166 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 175 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  50 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  86 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  39 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 103 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 146 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 198 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 162 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  19 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  96 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 145 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  33 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node   9 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 151 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 195 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  55 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  82 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 132 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 192 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  74 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  17 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  99 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 160 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  35 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node   5 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 167 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  87 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  42 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  20 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 102 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  41 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  22 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 138 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  13 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  97 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 193 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  80 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 111 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 199 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 144 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 194 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  98 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  51 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 106 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 163 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  73 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 183 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 161 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  68 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  26 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 119 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 147 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  48 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 136 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  81 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 154 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  43 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  83 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 178 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  24 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 139 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 114 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 171 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  58 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  75 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  90 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 176 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  69 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 118 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 133 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  27 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  49 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 137 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 182 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  57 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  36 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 115 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 112 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 107 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  72 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 179 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  91 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 152 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 177 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 101 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  59 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 170 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 155 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 104 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  25 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 122 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 123 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 186 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 168 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  84 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 105 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 113 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 187 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  56 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  21 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  44 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  88 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 169 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 100 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 153 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 196 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 164 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  77 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  52 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  89 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  85 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  45 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 197 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 141 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 120 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 165 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 148 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 184 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 121 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 185 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  37 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  53 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  76 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 149 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  28 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 180 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 172 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  60 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 181 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 116 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  29 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 117 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 173 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 109 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 140 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  93 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 156 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  61 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 108 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node  92 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 157 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 124 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 188 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 189 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
In the current implementation there is a scaling limit.
Using your configuration there would be nodes doing no work.
Therefore decrease the number of MPI processes or .EXA_BLOCKSIZE

 Slave node 125 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      

 Date and time (Linux) : Tue Jul 18 10:19:23 2023
 Not enough work for MPI processes
DIRAC pam run in /autofs/nccs-svm1_home1/milias/work/projects/open-collection/theoretical_chemistry/software_runs/dirac/servers/ornl/summit/Mt-H-CO_4

 ====  below this line is the stderr stream  ====
Warning, specify multiple instances of the same option to jsrun, take the last one

 Slave node   1 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  62 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

  Master node : --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---         
 Not enough work for MPI processes

 Slave node  31 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  63 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 126 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node   3 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  14 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 127 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  78 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 159 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 191 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  15 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  30 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 190 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  95 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node   6 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  79 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  70 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  46 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node   2 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  94 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 142 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 134 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 128 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node   7 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  66 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  71 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  65 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 158 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 130 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  23 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  10 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 135 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 129 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  67 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  34 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 110 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  64 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 143 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  47 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 131 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 174 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  39 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  11 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 198 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 111 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  18 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  32 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 195 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 150 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 162 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  54 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 175 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  38 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 146 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node   9 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  16 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  35 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  87 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 167 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  74 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 103 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 151 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  22 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  33 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  55 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 192 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  82 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 199 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 166 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 102 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  19 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  86 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 138 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 194 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  99 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 132 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 174 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  17 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 163 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 193 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 127 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 62 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node   5 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  96 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 30 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 78 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 95 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 183 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  75 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 14 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 46 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node   8 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 158 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  98 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 145 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 167 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 160 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 119 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 175 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 110 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 126 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 63 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  50 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 31 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 190 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  97 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 143 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 79 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 94 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 106 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 182 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  73 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 22 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 147 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 70 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 134 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 7 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 47 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 15 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 38 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  40 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 159 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  83 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  12 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  80 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 54 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 166 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 161 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 178 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 111 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 144 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  51 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 191 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 139 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 142 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node   4 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 23 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 71 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 102 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 135 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 6 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 39 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 150 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  42 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 55 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 86 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  20 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  81 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 154 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 118 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 198 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 171 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  41 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 103 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 115 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 151 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 87 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 118 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  43 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 179 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 199 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  26 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 107 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  48 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 136 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 114 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 112 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 119 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  90 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  68 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 170 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  27 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  72 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 133 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  58 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 155 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 176 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  91 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  59 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  49 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 137 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 177 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 182 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 122 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  13 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 183 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 123 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  36 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 50 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 113 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 18 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 186 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 51 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 154 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  69 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 19 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 187 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 155 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 2 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 3 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 104 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 106 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  24 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 98 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 107 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 99 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 146 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 147 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 168 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 74 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 105 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 75 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 162 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 196 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 169 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 114 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 163 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 153 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 90 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 194 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  25 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  57 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 66 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  21 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 115 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 139 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  37 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 195 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  88 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 152 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 67 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  77 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 138 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 91 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 59 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 101 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 82 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 58 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  89 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 122 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  56 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 197 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 83 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 123 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 130 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 131 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 10 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 184 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 42 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 121 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  84 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 186 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 11 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 43 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 170 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 26 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 120 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  44 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 171 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 187 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 185 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 27 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 100 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 164 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 149 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 141 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 178 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node 165 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  85 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 179 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

 Slave node  45 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  52 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  76 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 148 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  53 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 181 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 172 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 116 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 180 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 117 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 140 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 109 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 173 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  28 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 108 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  60 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  29 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  93 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 156 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  92 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 157 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node  61 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 124 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 188 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 125 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes

 Slave node 189 :  --- SEVERE ERROR, PROGRAM WILL BE ABORTED ---      
 Not enough work for MPI processes
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 128 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 129 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 12 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 13 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 68 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 24 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 69 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 21 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 25 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 20 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 85 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 116 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 84 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 117 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 73 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 48 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 49 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 72 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 109 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 96 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 16 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 136 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 132 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 17 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 97 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 137 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 44 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 168 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 133 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 108 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 169 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 45 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 4 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 5 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 52 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 53 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 164 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 192 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 40 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 196 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 120 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 165 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 28 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 64 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 197 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 32 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 41 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 193 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 144 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 121 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 29 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 33 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 65 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 160 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 145 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 140 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 56 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 161 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 112 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 88 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 141 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 113 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 57 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 176 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 36 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 89 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 177 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 8 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 80 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 37 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 9 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 149 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 81 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 180 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 184 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 124 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 104 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 148 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 185 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 105 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 156 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 100 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 181 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 152 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 101 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 153 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 157 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 77 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 60 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 76 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 125 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 61 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 188 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 189 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 92 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 93 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 172 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 173 in communicator MPI_COMM_WORLD
with errorcode -9999.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
