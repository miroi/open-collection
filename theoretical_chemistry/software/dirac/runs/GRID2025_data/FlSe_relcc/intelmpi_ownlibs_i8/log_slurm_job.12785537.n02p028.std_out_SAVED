
Running on host n02p028
Time is Thu Jul  3 23:10:00 MSK 2025 


 The total memory at the node (in GB)
              total        used        free      shared  buff/cache   available
Mem:            187          17         140           0          29         169
Swap:             3           0           3
Total:          191          17         144


 Job user is milias and his job FlSeCC has assigned ID 12785537
This job was submitted from the computer SLURM_SUBMIT_HOST=space02.hydra.local

This job was submitted from the home directory:
SLURM_SUBMIT_DIR=/lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/runs/GRID2025_data/FlSe_relcc/intelmpi_ownlibs_i8

This job is running on the cluster compute node:
SLURM_CLUSTER_NAME=gvr

This job is employing SLURM_JOB_NUM_NODES=2 node/nodes:
SLURM_JOB_NODELIST = n02p[028-029]

Job's partition is  SLURM_JOB_PARTITION=cascade

This job reserved SLURM_CPUS_ON_NODE=4 threads per node.
This job wants, in total, SLURM_NTASKS=6 threads . 


The master's node CPU model name	: Intel(R) Xeon(R) Platinum 8268 CPU @ 2.90GHz
This node has total NPROCS=96 CPUs available for an master EXCLUSIVE job.


 Generated machinefile for MPI, MACHINEFILE=nodes.cascade.12785537
-rw-r--r-- 1 milias hybrilit 48 Jul  3 23:10 /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/runs/GRID2025_data/FlSe_relcc/intelmpi_ownlibs_i8/nodes.cascade.12785537
MACHINEFILE contains these nodes:
n02p028	n02p028	n02p028	n02p029	n02p029	n02p029


 Loaded modules:
Currently Loaded Modulefiles:
  1) GVR/v1.0-1       3) Python/v3.10.2
  2) BASE/1.0         4) intel/v2021.1


Python ? :/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/intelpython/latest/bin/python
Python 3.7.9 :: Intel Corporation
  mpiifort ? :/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mpi/2021.1.1/bin/mpiifort
Intel(R) Fortran Intel(R) 64 Compiler Classic for applications running on Intel(R) 64, Version 2021.1 Build 20201112_000000
Copyright (C) 1985-2020 Intel Corporation.  All rights reserved.

  mpiicc ? :/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mpi/2021.1.1/bin/mpiicc
Intel(R) C Intel(R) 64 Compiler Classic for applications running on Intel(R) 64, Version 2021.1 Build 20201112_000000
Copyright (C) 1985-2020 Intel Corporation.  All rights reserved.

  mpiicc ? :/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mpi/2021.1.1/bin/mpiicpc
Intel(R) C++ Intel(R) 64 Compiler Classic for applications running on Intel(R) 64, Version 2021.1 Build 20201112_000000
Copyright (C) 1985-2020 Intel Corporation.  All rights reserved.

  OpenMPI v5.0.7  mpirun ? :/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/intelpython/latest/bin/mpirun
Intel(R) MPI Library for Linux* OS, Version 2021.1 Build 20201112 (id: b9c9d2fc5)
Copyright 2003-2020, Intel Corporation.


 The variable PATH=/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/dev-utilities/2021.1.1/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/intelpython/latest/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/intelpython/latest/condabin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/clck/2021.1.1/bin/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mkl/latest/bin/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/debugger/10.0.0/gdb/intel64/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/vpl/2021.1.1/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/compiler/2021.1.1/linux/bin/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/compiler/2021.1.1/linux/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/compiler/2021.1.1/linux/ioc/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/advisor/2021.1.1/bin64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/inspector/2021.1.1/bin64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mpi/2021.1.1/libfabric/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mpi/2021.1.1/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/itac/2021.1.1/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/vtune/2021.1.1/bin64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/dpcpp-ct/2021.1.1/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/Python/v3.10.2/bin:/lustre/home/user/m/milias/work/software/ams/linux.openmpi/ams2021.107/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:.:/lustre/home/user/m/milias/.local/bin:/lustre/home/user/m/milias/bin



 The variable LD_LIBRARY_PATH=/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mkl/latest/lib/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/debugger/10.0.0/dep/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/debugger/10.0.0/libipt/intel64/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/debugger/10.0.0/gdb/intel64/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/tbb/2021.1.1/env/../lib/intel64/gcc4.8:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/ippcp/2021.1.1/lib/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/vpl/2021.1.1/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/ccl/2021.1.1/lib/cpu_gpu_dpcpp:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/compiler/2021.1.1/linux/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/compiler/2021.1.1/linux/lib/x64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/compiler/2021.1.1/linux/lib/emu:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/compiler/2021.1.1/linux/compiler/lib/intel64_lin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/compiler/2021.1.1/linux/compiler/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mpi/2021.1.1//libfabric/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mpi/2021.1.1//lib/release:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mpi/2021.1.1//lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/itac/2021.1.1/slib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/dal/2021.1.1/lib/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/dnnl/2021.1.1/cpu_dpcpp_gpu_dpcpp/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/ipp/2021.1.1/lib/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/Python/v3.10.2/lib



 Checking the workhorse executable, ldd /lustre/home/user/m/milias/work/software/dirac/trunk_study/build_intelmpi_ownlibs_i8/dirac.x:
	linux-vdso.so.1 =>  (0x00007fff13d09000)
	libhdf5.so.8 => /lib64/libhdf5.so.8 (0x00002af0eac7b000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x00002af0eb121000)
	libirng.so => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/compiler/2021.1.1/linux/compiler/lib/intel64_lin/libirng.so (0x00002af0eb33d000)
	libstdc++.so.6 => /lib64/libstdc++.so.6 (0x00002af0eb6a7000)
	libmpi_ilp64.so => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mpi/2021.1.1//lib/release/libmpi_ilp64.so (0x00002af0eb9ae000)
	libmpifort.so.12 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mpi/2021.1.1//lib/libmpifort.so.12 (0x00002af0ebbf4000)
	libmpi.so.12 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mpi/2021.1.1//lib/release/libmpi.so.12 (0x00002af0ebfb2000)
	libdl.so.2 => /lib64/libdl.so.2 (0x00002af0ed32b000)
	librt.so.1 => /lib64/librt.so.1 (0x00002af0ed52f000)
	libm.so.6 => /lib64/libm.so.6 (0x00002af0ed737000)
	libiomp5.so => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/compiler/2021.1.1/linux/compiler/lib/intel64_lin/libiomp5.so (0x00002af0eda39000)
	libc.so.6 => /lib64/libc.so.6 (0x00002af0ede40000)
	libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00002af0ee20e000)
	libsz.so.2 => /lib64/libsz.so.2 (0x00002af0ee424000)
	libz.so.1 => /lib64/libz.so.1 (0x00002af0ee627000)
	/lib64/ld-linux-x86-64.so.2 (0x00002af0eaa57000)
	libintlc.so.5 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/compiler/2021.1.1/linux/compiler/lib/intel64_lin/libintlc.so.5 (0x00002af0ee83d000)
	libfabric.so.1 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/mpi/2021.1.1//libfabric/lib/libfabric.so.1 (0x00002af0eeab5000)
	libaec.so.0 => /lib64/libaec.so.0 (0x00002af0eecfb000)

 checking pam script, /lustre/home/user/m/milias/work/software/dirac/trunk_study/build_intelmpi_ownlibs_i8/pam --help  :
Usage: pam [options] --inp=*.inp --mol=*.mol [--pcm=*.pcm] [--pot=*.pot]
   or: pam [options] --inp=*.inp --mol=*.xyz [--pcm=*.pcm] [--pot=*.pot]
   or: pam [options] --fullrestart=ARCHIVE.tgz [--inp=*.inp] [--mol=*.mol | --mol=*.xyz] [--pcm=*.pcm] [--pot=*.pot]
Options are also read from ~/.diracrc

Options:
  -h, --help            show this help message and exit

  Input files - mandatory specifications:
    --inp=INP_FILE      Dirac input file containing the job directives [*.inp]
    --mol=MOL_FILE      file containing the basis set and geometry
                        specifications [*.mol or *.xyz]
    --pcm=PCM_FILE      PCMSolver input file [*.pcm]
    --pot=POT_FILE      file containing the polarizable embedding potential
                        parameters [*.pot]
    --fullrestart=ARCHIVE.tgz
                        restart from files within ARCHIVE.tgz;
                        old input files from ARCHIVE.tgz are reused unless new
                        input files are provided with one or more of "--inp",
                        "--mol", "--pcm", and "--pot".

  Output files:
    --suffix=STRING     file suffix for output [default: out]
    --rsh=STRING        Remote shell program, typically rsh or ssh [default:
                        ssh or taken from .dirarc]
    --rcp=STRING        Remote shell program, typically rcp or scp [default:
                        scp or taken from .dirarc]
    --noh5              do not create a h5 acheckpoint file [default: create
                        it]
    --noarch            do not create a tgz archive [default: create it]
    --nobackup          do not backup old outputs [default: back them up]

  Memory specification:
    --mw=INTEGER        set max memory (in megawords) for WORK array
                        for the master (for each MPI thread) [default: 64]
    --mb=INTEGER        set max memory (in MB) for WORK array
                        for the master (for each MPI thread) [default: none]
    --gb=FLOAT          set max memory (in GB) for WORK array
                        for the master (for each MPI thread) [default: none]
    --nw=INTEGER        set max memory (in megawords) for WORK array
                        for the co-workers (for each MPI thread) [default:
                        --mw set value]
    --aw=INTEGER        set max dynamically allocatable memory (in megawords)
                        (for each MPI thread) [default: 0]
    --ag=FLOAT          set max dynamically allocatable memory (in gigabytes)
                        (for each MPI thread) [default: 0]

  MPI settings:
    --mpi=INTEGER       set number of MPI processes in parallel run [default:
                        1]
    --mpirun=STRING     set the MPI run command, can have own arguments
                        [default: see "pam --show" output]
    --mpiarg=STRING     additonal options passed on to the MPI launcher;
                        useful arguments could be:
                        -envall (for MPICH2, Intel MPI)
                        -x "PATH LD_LIBRARY_PATH" (for Open MPI)
                        which will allow the user to pass the present
                        environment settings from the head node to the scratch
                        nodes.
    --mpi-workdir-argument=STRING
                        option to explicitly pass the scratch directory name
                        on to the slaves
                        ("wd" for MPICH2/Intel MPI, "wdir" for Open MPI).
    --machfile=STRING   specify a machinefile for a parallel run [default:
                        none]

  Managing data/file transfer:
    --put="file1 file2 my_file* ...", --copy="file1 file2 my_file* ..."
                        copy files to scratch directory
    --distribute="file1 file2 my_file*..."
                        copy selected files to scratch directories of working
                        nodes in parallel run
    --get="file1 file2 my_file*..."
                        get files from scratch directory
    --env="env_var1=value1 env_var2=value2 ..."
                        set environment variables
    --replace=NAME=VALUE
                        replace NAME by VALUE in the mol and inp files
    --incmo             copy CHECKPOINT to scratch directory [default: False]
    --outcmo            get CHECKPOINT from scratch directory [default: False]
    --inkrmc            copy KRMCSCF to scratch directory as KRMCOLD [default:
                        False]
    --outkrmc           get KRMCSCF and KRMCOLD from scratch directory
                        [default: False]
    --outqforce         get qforce.*.log from scratch directory [default:
                        False]
    --keep_scratch      keep the scratch directory after calculation [default:
                        delete it]

  Modify default paths and settings:
    --scratch=FULL_PATH
                        full path to scratch directory (DIRAC will append a
                        subdirectory to make it unique) [default read from
                        ~/.diracrc]
    --scratchfull=FULL_PATH
                        full path to scratch directory (DIRAC will not append
                        anything to this path; do NOT choose your /home/user
                        as scratchfull) [default: script defined full path
                        upon ~/.diracrc]
    --basis=FULL_PATH   basis set directory [default: .:/lustre/home/user/m/mi
                        lias/work/software/dirac/trunk_study/build_intelmpi_ow
                        nlibs_i8/basis:/lustre/home/user/m/milias/work/softwar
                        e/dirac/trunk_study/build_intelmpi_ownlibs_i8/basis_da
                        lton:/lustre/home/user/m/milias/work/software/dirac/tr
                        unk_study/build_intelmpi_ownlibs_i8/basis_ecp]
    --dirac=FULL_PATH   dirac executable [default: /lustre/home/user/m/milias/
                        work/software/dirac/trunk_study/build_intelmpi_ownlibs
                        _i8/dirac.x]

  Advanced options for programming and debugging:
    --show              print current settings (including those from .diracrc)
                        and exit
    --silent            run pam silent with minimal output
    --debug             run in debugger [default: False]
    --summit            run with summits jsrun [default: False]
    --debugger=FULL_PATH
                        set full path to debugger [default: in .diracrc]
    --valgrind          run with valgrind [default: False]
    --profile           perform profiling [default: False]
    --profiler=FULL_PATH
                        set full path to profiler [default: none]
    --timeout=TIME_STRING
                        limit dirac.x execution time (in #d#h#m#s, or in pure
                        seconds-integer) [default: no limit; reads env.var.
                        DIRTIMEOUT]


 DIRAC_MPI_COMMAND=mpirun  -f /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/runs/GRID2025_data/FlSe_relcc/intelmpi_ownlibs_i8/nodes.cascade.12785537 -np 6


 I am in the directory: /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/runs/GRID2025_data/FlSe_relcc/intelmpi_ownlibs_i8


 Running parallel DIRAC job with --mpi=6 : 


  DIRAC pam script running:

  user           : milias
  machine        : n02p028
  date and time  : 2025-07-03 23:10:01.162757
  input dir      : /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/runs/GRID2025_data/FlSe_relcc/intelmpi_ownlibs_i8
  pam command    : /lustre/home/user/m/milias/work/software/dirac/trunk_study/build_intelmpi_ownlibs_i8/pam
  all pam args   : ['--put=../DFPCMO.FlSe.acv3z_x2c_scf_lsym=DFPCMO', '--noarch', '--gb=42.0', '--ag=43.0', '--inp=../x2c-ach0.scf_relcc_m3.20-vir50.00.inp', '--mol=../FlSe.dyall_acv3z_lsym.mol', '--suffix=out.gvr.cascade.N2.n6.jid12785537.out']
  executable     : /lustre/home/user/m/milias/work/software/dirac/trunk_study/build_intelmpi_ownlibs_i8/dirac.x
  scratch dir    : /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_x2c-ach0.scf_relcc_m3.20-vir50.00_FlSe.dyall_acv3z_lsym_123652 (availspace=127567.666[GB])
  output file    : x2c-ach0.scf_relcc_m3.20-vir50.00_FlSe.dyall_acv3z_lsym.out.gvr.cascade.N2.n6.jid12785537.out
  DIRAC run      : parallel (launcher: /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/v2021.1/intelpython/latest/bin/mpirun)
  local disks    : False
  rsh/rcp        : ssh / scp
  machine file   : None

  Setting MKL and OPENMP environment to default values (if not set already)
   Variabl.MKL_NUM_THREADS =  1
   Variabl.MKL_DYNAMIC =  FALSE
   OMP_NUM_THREADS = 1
   OMP_DYNAMIC="FALSE"
  Setting environment variables (as specified explicitly or in .diracrc)

  Creating the scratch directory.
  Copying /lustre/home/user/m/milias/work/software/dirac/trunk_study/build_intelmpi_ownlibs_i8/dirac.x to /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_x2c-ach0.scf_relcc_m3.20-vir50.00_FlSe.dyall_acv3z_lsym_123652/dirac.x
  Copying ../FlSe.dyall_acv3z_lsym.mol to /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_x2c-ach0.scf_relcc_m3.20-vir50.00_FlSe.dyall_acv3z_lsym_123652/MOLECULE.MOL
  Copying ../x2c-ach0.scf_relcc_m3.20-vir50.00.inp to /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_x2c-ach0.scf_relcc_m3.20-vir50.00_FlSe.dyall_acv3z_lsym_123652/DIRAC.INP
  Copying ../DFPCMO.FlSe.acv3z_x2c_scf_lsym to /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_x2c-ach0.scf_relcc_m3.20-vir50.00_FlSe.dyall_acv3z_lsym_123652/DFPCMO
  Basis set libraries (default)    : /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/runs/GRID2025_data/FlSe_relcc/intelmpi_ownlibs_i8
                        /lustre/home/user/m/milias/work/software/dirac/trunk_study/build_intelmpi_ownlibs_i8/basis
                        /lustre/home/user/m/milias/work/software/dirac/trunk_study/build_intelmpi_ownlibs_i8/basis_dalton
                        /lustre/home/user/m/milias/work/software/dirac/trunk_study/build_intelmpi_ownlibs_i8/basis_ecp
  DIRAC command  : mpirun  -f /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/runs/GRID2025_data/FlSe_relcc/intelmpi_ownlibs_i8/nodes.cascade.12785537 -np 6 /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_x2c-ach0.scf_relcc_m3.20-vir50.00_FlSe.dyall_acv3z_lsym_123652/dirac.x (PID=123662)
  content of the (master) scratch directory
  ------------------------------------------------------------------------------
                     name    size (MB)    last accessed
  ------------------------------------------------------------------------------
                  MRCONEE      3.127   07/04/2025 12:11:15 AM
                   MDCINT  26128.059   07/04/2025 12:11:11 AM
                   ft46.0   1031.776   07/04/2025 12:26:14 AM
                 MCCRES.3     47.248   07/04/2025 12:43:48 AM
                   ft45.3    181.952   07/04/2025 12:26:18 AM
                   ft44.4    193.952   07/04/2025 12:25:44 AM
               MDCINXXXX4      0.000   07/03/2025 11:56:48 PM
                   ft43.5     47.200   07/04/2025 12:26:18 AM
                   DFDENS      2.888   07/03/2025 11:13:01 PM
                   DFFCK2      2.888   07/03/2025 11:13:01 PM
                   ft42.2      8.512   07/04/2025 12:26:17 AM
                   ft41.1      0.384   07/04/2025 12:25:44 AM
                  MNF.INP      0.004   07/03/2025 11:10:18 PM
                   DFEVEC     18.074   07/03/2025 11:13:01 PM
                   DFFCKT      2.888   07/03/2025 11:10:30 PM
              RELSCF_COEF      0.005   07/03/2025 11:10:28 PM
                   MCCRES     47.248   07/04/2025 12:43:49 AM
                   DFFOCK     18.074   07/03/2025 11:13:01 PM
                   DFCYCL      0.003   07/03/2025 11:13:01 PM
                DIRAC.INP      0.001   07/03/2025 11:56:48 PM
                   ft40.1      0.288   07/04/2025 12:11:15 AM
                   ft43.2     47.200   07/04/2025 12:26:17 AM
                   ft42.5      8.512   07/04/2025 12:26:18 AM
                 MCCRES.4     47.248   07/04/2025 12:43:49 AM
                   ft45.4    181.952   07/04/2025 12:26:18 AM
                   ft44.3    193.952   07/04/2025 12:25:43 AM
               MDCINXXXX3      0.000   07/03/2025 11:56:48 PM
                   ft45.0    181.888   07/04/2025 12:26:18 AM
                  AOMOMAT      4.817   07/03/2025 11:12:39 PM
                   ft46.3   1032.128   07/04/2025 12:26:04 AM
                   ft40.5      0.288   07/04/2025 12:11:15 AM
             MOLECULE.XYZ      0.000   07/03/2025 11:10:02 PM
                   ft41.2      0.384   07/04/2025 12:25:43 AM
                   ft42.1      8.512   07/04/2025 12:26:18 AM
                   DFFCK1      2.888   07/03/2025 11:13:01 PM
                dirac.xml      0.013   07/03/2025 11:10:02 PM
            RELCCSD.OUT.3      0.011   07/03/2025 11:56:48 PM
             interface_mo      0.000   07/03/2025 11:10:30 PM
                    TOSCF      0.001   07/03/2025 11:10:28 PM
                   DFCMOS      2.008   07/03/2025 11:13:01 PM
            RELCCSD.OUT.4      0.011   07/03/2025 11:56:48 PM
        AOPROPER_MNF.34.2      0.361   07/03/2025 11:10:29 PM
                   ft43.1     47.200   07/04/2025 12:26:18 AM
                  AOMOSLR     15.677   07/03/2025 11:10:05 PM
                   ft40.2      0.288   07/04/2025 12:11:15 AM
                   ft41.5      0.384   07/04/2025 12:25:44 AM
                   ft46.4   1032.128   07/04/2025 12:26:14 AM
                   ft44.0    193.952   07/04/2025 12:25:53 AM
                  dirac.x     85.951   07/03/2025 11:10:01 PM
                   ft40.0      0.288   07/04/2025 12:11:15 AM
             interface_ao      0.014   07/03/2025 11:10:30 PM
                   ft42.4      8.512   07/04/2025 12:26:18 AM
                   ft43.3     47.200   07/04/2025 12:26:17 AM
       AOPROPER_MNF.114.1      1.311   07/03/2025 11:10:28 PM
               MDCINXXXX2      0.000   07/03/2025 11:56:48 PM
                   ft44.2    193.952   07/04/2025 12:25:43 AM
                   ft45.5    181.952   07/04/2025 12:26:18 AM
                 MCCRES.5     47.248   07/04/2025 12:43:49 AM
                  AOMOlin     15.677   07/03/2025 11:10:05 PM
        soc-contributions      0.733   07/03/2025 11:10:30 PM
            RELCCSD.OUT.1      0.011   07/03/2025 11:56:48 PM
            CHECKPOINT.h5     90.950   07/04/2025 12:43:50 AM
                   ft46.1   1031.872   07/04/2025 12:26:14 AM
             OPERATORS.h5    222.722   07/04/2025 12:43:50 AM
               MDCINXXXX5      0.000   07/03/2025 11:56:48 PM
                   ft44.5    193.952   07/04/2025 12:25:44 AM
        schema_labels.txt      0.017   07/03/2025 11:10:02 PM
                   ft45.2    181.952   07/04/2025 12:26:18 AM
                 MCCRES.2     47.248   07/04/2025 12:43:49 AM
                   ft42.3      8.512   07/04/2025 12:26:17 AM
                   MDPROP      9.241   07/03/2025 11:13:02 PM
                   ft43.4     47.200   07/04/2025 12:26:18 AM
                   ft41.0      0.384   07/04/2025 12:25:44 AM
                   DFPCMO      6.685   07/03/2025 11:10:30 PM
                 AOPROPER      0.000   07/03/2025 11:10:05 PM
                   ft43.0     47.200   07/04/2025 12:26:18 AM
                   ft41.4      0.384   07/04/2025 12:25:44 AM
                   ft40.3      0.288   07/04/2025 12:11:15 AM
                   ft46.5   1032.128   07/04/2025 12:26:14 AM
                   X2CMAT      0.000   07/03/2025 11:10:03 PM
                   ft44.1    193.952   07/04/2025 12:25:44 AM
               MDCINXXXX1      0.000   07/03/2025 11:56:48 PM
            RELCCSD.OUT.5      0.011   07/03/2025 11:56:48 PM
            RELCCSD.OUT.2      0.011   07/03/2025 11:56:48 PM
                  LOWDMAT      2.408   07/03/2025 11:10:30 PM
             MOLECULE.MOL      0.000   07/03/2025 11:10:31 PM
                 MCCRES.1     47.248   07/04/2025 12:43:49 AM
                   ft45.1    181.920   07/04/2025 12:26:18 AM
                   ft46.2   1032.064   07/04/2025 12:26:04 AM
                   ft41.3      0.384   07/04/2025 12:25:43 AM
                   ft40.4      0.288   07/04/2025 12:11:15 AM
                   ft42.0      8.512   07/04/2025 12:26:18 AM
  ------------------------------------------------------------------------------
      Total size of all files :   35706.755  MB 
      Disk info:  used    available   capacity [GB]
             537844.820 127755.180   665600.000 

  Copying /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_x2c-ach0.scf_relcc_m3.20-vir50.00_FlSe.dyall_acv3z_lsym_123652/CHECKPOINT.h5 to /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/runs/GRID2025_data/FlSe_relcc/intelmpi_ownlibs_i8/x2c-ach0.scf_relcc_m3.20-vir50.00_FlSe.dyall_acv3z_lsym.h5
  Native hdf5 checkpoint file was retrieved from scratch directory
  going to delete the scratch directory ... done

  exit date      : 2025-07-04 00:44:02.475678
  elapsed time   : 01h34m01s
  exit           : normal

  output lines containing warnings:
*** WARNING *** : 20 functions deleted due to numerical linear dependence.

