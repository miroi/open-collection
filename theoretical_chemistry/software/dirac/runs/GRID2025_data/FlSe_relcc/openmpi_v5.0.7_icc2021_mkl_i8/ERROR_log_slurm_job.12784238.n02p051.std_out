
Running on host n02p051
Time is Thu Jul  3 13:21:46 MSK 2025 


 The total memory at the node (in GB)
              total        used        free      shared  buff/cache   available
Mem:            187          19         150           0          17         166
Swap:             3           0           3
Total:          191          19         154


 Job user is milias and his job FlSeCC has assigned ID 12784238
This job was submitted from the computer SLURM_SUBMIT_HOST=space02.hydra.local

This job was submitted from the home directory:
SLURM_SUBMIT_DIR=/lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/runs/GRID2025_data/FlSe_relcc/openmpi_v5.0.7_icc2021_mkl_i8

This job is running on the cluster compute node:
SLURM_CLUSTER_NAME=gvr

This job is employing SLURM_JOB_NUM_NODES=1 node/nodes:
SLURM_JOB_NODELIST = n02p051

Job's partition is  SLURM_JOB_PARTITION=cascade

This job reserved SLURM_CPUS_ON_NODE=6 threads per node.
This job wants, in total, SLURM_NTASKS=6 threads . 


The master's node CPU model name	: Intel(R) Xeon(R) Platinum 8268 CPU @ 2.90GHz
This node has total NPROCS=96 CPUs available for an master EXCLUSIVE job.


 Generated machinefile for MPI, MACHINEFILE=nodes.cascade.12784238
-rw-r--r-- 1 milias hybrilit 48 Jul  3 13:21 /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/runs/GRID2025_data/FlSe_relcc/openmpi_v5.0.7_icc2021_mkl_i8/nodes.cascade.12784238
MACHINEFILE contains these nodes:
n02p051	n02p051	n02p051	n02p051	n02p051	n02p051


 Loaded modules:
Currently Loaded Modulefiles:
  1) GVR/v1.0-1               4) intel/oneapi
  2) BASE/1.0                 5) openmpi/v5.0.7_icc2021
  3) Python/v3.10.2


Python ? :/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/Python/v3.10.2/bin/python
Python 3.10.2
  mpiifort ? :/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mpi/latest/bin/mpiifort
Intel(R) Fortran Intel(R) 64 Compiler Classic for applications running on Intel(R) 64, Version 2021.4.0 Build 20210910_000000
Copyright (C) 1985-2021 Intel Corporation.  All rights reserved.

  mpiicc ? :/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mpi/latest/bin/mpiicc
Intel(R) C Intel(R) 64 Compiler Classic for applications running on Intel(R) 64, Version 2021.4.0 Build 20210910_000000
Copyright (C) 1985-2021 Intel Corporation.  All rights reserved.

  mpiicc ? :/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mpi/latest/bin/mpiicpc
Intel(R) C++ Intel(R) 64 Compiler Classic for applications running on Intel(R) 64, Version 2021.4.0 Build 20210910_000000
Copyright (C) 1985-2021 Intel Corporation.  All rights reserved.

  OpenMPI v5.0.7  mpirun ? :/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.7_icc2021/bin/mpirun
mpirun (Open MPI) 5.0.7

Report bugs to https://www.open-mpi.org/community/help/

 Intel MKL library ? MKLROOT=/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mkl/latest
total 3890313
drwxr-xr-x 3 cvmfs cvmfs       4096 Nov 29  2021 locale
lrwxrwxrwx 1 cvmfs cvmfs         32 Sep 24  2021 libmkl_blacs_intelmpi_ilp64.so -> libmkl_blacs_intelmpi_ilp64.so.1
lrwxrwxrwx 1 cvmfs cvmfs         31 Sep 24  2021 libmkl_blacs_intelmpi_lp64.so -> libmkl_blacs_intelmpi_lp64.so.1
lrwxrwxrwx 1 cvmfs cvmfs         31 Sep 24  2021 libmkl_blacs_openmpi_ilp64.so -> libmkl_blacs_openmpi_ilp64.so.1
lrwxrwxrwx 1 cvmfs cvmfs         30 Sep 24  2021 libmkl_blacs_openmpi_lp64.so -> libmkl_blacs_openmpi_lp64.so.1
lrwxrwxrwx 1 cvmfs cvmfs         30 Sep 24  2021 libmkl_blacs_sgimpt_ilp64.so -> libmkl_blacs_sgimpt_ilp64.so.1
lrwxrwxrwx 1 cvmfs cvmfs         29 Sep 24  2021 libmkl_blacs_sgimpt_lp64.so -> libmkl_blacs_sgimpt_lp64.so.1
lrwxrwxrwx 1 cvmfs cvmfs         21 Sep 24  2021 libmkl_cdft_core.so -> libmkl_cdft_core.so.1
lrwxrwxrwx 1 cvmfs cvmfs         16 Sep 24  2021 libmkl_core.so -> libmkl_core.so.1
lrwxrwxrwx 1 cvmfs cvmfs         20 Sep 24  2021 libmkl_gf_ilp64.so -> libmkl_gf_ilp64.so.1
lrwxrwxrwx 1 cvmfs cvmfs         19 Sep 24  2021 libmkl_gf_lp64.so -> libmkl_gf_lp64.so.1
lrwxrwxrwx 1 cvmfs cvmfs         22 Sep 24  2021 libmkl_gnu_thread.so -> libmkl_gnu_thread.so.1
lrwxrwxrwx 1 cvmfs cvmfs         23 Sep 24  2021 libmkl_intel_ilp64.so -> libmkl_intel_ilp64.so.1
lrwxrwxrwx 1 cvmfs cvmfs         22 Sep 24  2021 libmkl_intel_lp64.so -> libmkl_intel_lp64.so.1
lrwxrwxrwx 1 cvmfs cvmfs         24 Sep 24  2021 libmkl_intel_thread.so -> libmkl_intel_thread.so.1
lrwxrwxrwx 1 cvmfs cvmfs         22 Sep 24  2021 libmkl_pgi_thread.so -> libmkl_pgi_thread.so.1
lrwxrwxrwx 1 cvmfs cvmfs         14 Sep 24  2021 libmkl_rt.so -> libmkl_rt.so.1
lrwxrwxrwx 1 cvmfs cvmfs         27 Sep 24  2021 libmkl_scalapack_ilp64.so -> libmkl_scalapack_ilp64.so.1
lrwxrwxrwx 1 cvmfs cvmfs         26 Sep 24  2021 libmkl_scalapack_lp64.so -> libmkl_scalapack_lp64.so.1
lrwxrwxrwx 1 cvmfs cvmfs         22 Sep 24  2021 libmkl_sequential.so -> libmkl_sequential.so.1
lrwxrwxrwx 1 cvmfs cvmfs         16 Sep 24  2021 libmkl_sycl.so -> libmkl_sycl.so.1
lrwxrwxrwx 1 cvmfs cvmfs         22 Sep 24  2021 libmkl_tbb_thread.so -> libmkl_tbb_thread.so.1
-rwxr-xr-x 1 cvmfs cvmfs   16210632 Sep  4  2021 libmkl_vml_avx512_mic.so.1
-rwxr-xr-x 1 cvmfs cvmfs    7756240 Sep  4  2021 libmkl_vml_cmpt.so.1
-rwxr-xr-x 1 cvmfs cvmfs    8766704 Sep  4  2021 libmkl_vml_def.so.1
-rwxr-xr-x 1 cvmfs cvmfs   14619984 Sep  4  2021 libmkl_vml_mc2.so.1
-rwxr-xr-x 1 cvmfs cvmfs   14628344 Sep  4  2021 libmkl_vml_mc3.so.1
-rwxr-xr-x 1 cvmfs cvmfs   14775632 Sep  4  2021 libmkl_vml_mc.so.1
-rwxr-xr-x 1 cvmfs cvmfs 1210424992 Sep  4  2021 libmkl_sycl.so.1
-rw-r--r-- 1 cvmfs cvmfs  111739374 Sep  4  2021 libmkl_tbb_thread.a
-rwxr-xr-x 1 cvmfs cvmfs   40608320 Sep  4  2021 libmkl_tbb_thread.so.1
-rwxr-xr-x 1 cvmfs cvmfs   15038944 Sep  4  2021 libmkl_vml_avx2.so.1
-rwxr-xr-x 1 cvmfs cvmfs   14364224 Sep  4  2021 libmkl_vml_avx512.so.1
-rwxr-xr-x 1 cvmfs cvmfs   15887632 Sep  4  2021 libmkl_vml_avx.so.1
-rw-r--r-- 1 cvmfs cvmfs  737840908 Sep  4  2021 libmkl_sycl.a
-rw-r--r-- 1 cvmfs cvmfs   12334148 Sep  4  2021 libmkl_scalapack_lp64.a
-rwxr-xr-x 1 cvmfs cvmfs    7736496 Sep  4  2021 libmkl_scalapack_lp64.so.1
-rw-r--r-- 1 cvmfs cvmfs   38515522 Sep  4  2021 libmkl_sequential.a
-rwxr-xr-x 1 cvmfs cvmfs   28992400 Sep  4  2021 libmkl_sequential.so.1
-rw-r--r-- 1 cvmfs cvmfs   29243364 Sep  4  2021 libmkl_gf_lp64.a
-rwxr-xr-x 1 cvmfs cvmfs   13556800 Sep  4  2021 libmkl_gf_lp64.so.1
-rw-r--r-- 1 cvmfs cvmfs   44055086 Sep  4  2021 libmkl_gnu_thread.a
-rwxr-xr-x 1 cvmfs cvmfs   30945536 Sep  4  2021 libmkl_gnu_thread.so.1
-rw-r--r-- 1 cvmfs cvmfs   28200418 Sep  4  2021 libmkl_intel_ilp64.a
-rwxr-xr-x 1 cvmfs cvmfs   12914912 Sep  4  2021 libmkl_intel_ilp64.so.1
-rw-r--r-- 1 cvmfs cvmfs   29247844 Sep  4  2021 libmkl_intel_lp64.a
-rwxr-xr-x 1 cvmfs cvmfs   13560968 Sep  4  2021 libmkl_intel_lp64.so.1
-rw-r--r-- 1 cvmfs cvmfs   90381052 Sep  4  2021 libmkl_intel_thread.a
-rwxr-xr-x 1 cvmfs cvmfs   64778624 Sep  4  2021 libmkl_intel_thread.so.1
-rw-r--r-- 1 cvmfs cvmfs    7513600 Sep  4  2021 libmkl_lapack95_ilp64.a
-rw-r--r-- 1 cvmfs cvmfs    7441512 Sep  4  2021 libmkl_lapack95_lp64.a
-rwxr-xr-x 1 cvmfs cvmfs   50321448 Sep  4  2021 libmkl_mc3.so.1
-rwxr-xr-x 1 cvmfs cvmfs   48746808 Sep  4  2021 libmkl_mc.so.1
-rw-r--r-- 1 cvmfs cvmfs   51340484 Sep  4  2021 libmkl_pgi_thread.a
-rwxr-xr-x 1 cvmfs cvmfs   38032808 Sep  4  2021 libmkl_pgi_thread.so.1
-rwxr-xr-x 1 cvmfs cvmfs    7177832 Sep  4  2021 libmkl_rt.so.1
-rw-r--r-- 1 cvmfs cvmfs   12244638 Sep  4  2021 libmkl_scalapack_ilp64.a
-rwxr-xr-x 1 cvmfs cvmfs    7718648 Sep  4  2021 libmkl_scalapack_ilp64.so.1
-rw-r--r-- 1 cvmfs cvmfs  680137838 Sep  4  2021 libmkl_core.a
-rwxr-xr-x 1 cvmfs cvmfs   74757224 Sep  4  2021 libmkl_core.so.1
-rwxr-xr-x 1 cvmfs cvmfs   42420592 Sep  4  2021 libmkl_def.so.1
-rw-r--r-- 1 cvmfs cvmfs   28196008 Sep  4  2021 libmkl_gf_ilp64.a
-rwxr-xr-x 1 cvmfs cvmfs   12906640 Sep  4  2021 libmkl_gf_ilp64.so.1
-rwxr-xr-x 1 cvmfs cvmfs   50137384 Sep  4  2021 libmkl_avx2.so.1
-rwxr-xr-x 1 cvmfs cvmfs   67375088 Sep  4  2021 libmkl_avx512_mic.so.1
-rwxr-xr-x 1 cvmfs cvmfs   66658392 Sep  4  2021 libmkl_avx512.so.1
-rwxr-xr-x 1 cvmfs cvmfs   53034328 Sep  4  2021 libmkl_avx.so.1
-rw-r--r-- 1 cvmfs cvmfs    1273566 Sep  4  2021 libmkl_blacs_intelmpi_ilp64.a
-rwxr-xr-x 1 cvmfs cvmfs     523704 Sep  4  2021 libmkl_blacs_intelmpi_ilp64.so.1
-rw-r--r-- 1 cvmfs cvmfs     754342 Sep  4  2021 libmkl_blacs_intelmpi_lp64.a
-rwxr-xr-x 1 cvmfs cvmfs     320552 Sep  4  2021 libmkl_blacs_intelmpi_lp64.so.1
-rw-r--r-- 1 cvmfs cvmfs    1292782 Sep  4  2021 libmkl_blacs_openmpi_ilp64.a
-rwxr-xr-x 1 cvmfs cvmfs     532928 Sep  4  2021 libmkl_blacs_openmpi_ilp64.so.1
-rw-r--r-- 1 cvmfs cvmfs     773558 Sep  4  2021 libmkl_blacs_openmpi_lp64.a
-rwxr-xr-x 1 cvmfs cvmfs     321552 Sep  4  2021 libmkl_blacs_openmpi_lp64.so.1
-rw-r--r-- 1 cvmfs cvmfs    1273006 Sep  4  2021 libmkl_blacs_sgimpt_ilp64.a
-rwxr-xr-x 1 cvmfs cvmfs     523960 Sep  4  2021 libmkl_blacs_sgimpt_ilp64.so.1
-rw-r--r-- 1 cvmfs cvmfs     753782 Sep  4  2021 libmkl_blacs_sgimpt_lp64.a
-rwxr-xr-x 1 cvmfs cvmfs     316680 Sep  4  2021 libmkl_blacs_sgimpt_lp64.so.1
-rw-r--r-- 1 cvmfs cvmfs     658452 Sep  4  2021 libmkl_blas95_ilp64.a
-rw-r--r-- 1 cvmfs cvmfs     657020 Sep  4  2021 libmkl_blas95_lp64.a
-rw-r--r-- 1 cvmfs cvmfs     217682 Sep  4  2021 libmkl_cdft_core.a
-rwxr-xr-x 1 cvmfs cvmfs     168912 Sep  4  2021 libmkl_cdft_core.so.1


 The variable PATH=/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.7_icc2021/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/vtune/latest/bin64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/vpl/latest/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mpi/latest/libfabric/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mpi/latest/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/itac/latest/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/inspector/latest/bin64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/dpcpp-ct/latest/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/dev-utilities/latest/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/debugger/latest/gdb/intel64/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/bin/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/clck/latest/bin/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/advisor/latest/bin64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/Python/v3.10.2/bin:/lustre/home/user/m/milias/work/software/ams/linux.openmpi/ams2021.107/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:.:/lustre/home/user/m/milias/.local/bin:/lustre/home/user/m/milias/bin



 The variable LD_LIBRARY_PATH=/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.7_icc2021/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/vpl/latest/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/tbb/latest/lib/intel64/gcc4.8:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mpi/latest/libfabric/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mpi/latest/lib/release:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mpi/latest/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mkl/latest/lib/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/itac/latest/slib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/ippcp/latest/lib/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/ipp/latest/lib/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/dnnl/latest/cpu_dpcpp_gpu_dpcpp/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/debugger/latest/dep//lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/debugger/10.1.2/libipt/intel64/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/debugger/10.1.2/gdb/intel64/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/dal/latest/lib/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/lib/x64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/lib/emu:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/compiler/lib/intel64_lin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/clck/latest/lib/intel64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/ccl/latest/lib/cpu_gpu_dpcpp:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/Python/v3.10.2/lib



 Checking the workhorse executable, ldd /lustre/home/user/m/milias/work/software/dirac/trunk_study/build_openmpi-v5.0.7_icc2021_mkl_i8/dirac.x:
	linux-vdso.so.1 =>  (0x00007ffd25db1000)
	libmkl_intel_ilp64.so.1 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mkl/latest/lib/intel64/libmkl_intel_ilp64.so.1 (0x00002b0138079000)
	libmkl_intel_thread.so.1 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mkl/latest/lib/intel64/libmkl_intel_thread.so.1 (0x00002b0138b7b000)
	libmkl_core.so.1 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/mkl/latest/lib/intel64/libmkl_core.so.1 (0x00002b013c2ca000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x00002b0140738000)
	libm.so.6 => /lib64/libm.so.6 (0x00002b0140954000)
	libirng.so => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/compiler/lib/intel64_lin/libirng.so (0x00002b0140c56000)
	libstdc++.so.6 => /lib64/libstdc++.so.6 (0x00002b0140fc0000)
	libmpi_usempif08.so.40 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.7_icc2021/lib/libmpi_usempif08.so.40 (0x00002b01412c8000)
	libmpi_usempi_ignore_tkr.so.40 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.7_icc2021/lib/libmpi_usempi_ignore_tkr.so.40 (0x00002b0141504000)
	libmpi_mpifh.so.40 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.7_icc2021/lib/libmpi_mpifh.so.40 (0x00002b0141710000)
	libmpi.so.40 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.7_icc2021/lib/libmpi.so.40 (0x00002b014198f000)
	libiomp5.so => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/compiler/lib/intel64_lin/libiomp5.so (0x00002b0141f6c000)
	libdl.so.2 => /lib64/libdl.so.2 (0x00002b014238c000)
	libc.so.6 => /lib64/libc.so.6 (0x00002b0142590000)
	/lib64/ld-linux-x86-64.so.2 (0x00002b0137e55000)
	libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00002b014295e000)
	libintlc.so.5 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/compiler/lib/intel64_lin/libintlc.so.5 (0x00002b0142b74000)
	libopen-pal.so.80 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.7_icc2021/lib/libopen-pal.so.80 (0x00002b0142dec000)
	librt.so.1 => /lib64/librt.so.1 (0x00002b01430ed000)
	libpmix.so.2 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.7_icc2021/lib/libpmix.so.2 (0x00002b01432f5000)
	libevent_core-2.1.so.7 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.7_icc2021/lib/libevent_core-2.1.so.7 (0x00002b014375c000)
	libevent_pthreads-2.1.so.7 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.7_icc2021/lib/libevent_pthreads-2.1.so.7 (0x00002b014399b000)
	libhwloc.so.15 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.7_icc2021/lib/libhwloc.so.15 (0x00002b0143b9e000)
	libifport.so.5 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/compiler/lib/intel64_lin/libifport.so.5 (0x00002b0143e17000)
	libifcoremt.so.5 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/compiler/lib/intel64_lin/libifcoremt.so.5 (0x00002b0137e91000)
	libimf.so => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/compiler/lib/intel64_lin/libimf.so (0x00002b0144045000)
	libsvml.so => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/intel/oneapi/compiler/latest/linux/compiler/lib/intel64_lin/libsvml.so (0x00002b0144776000)

 checking pam script, /lustre/home/user/m/milias/work/software/dirac/trunk_study/build_openmpi-v5.0.7_icc2021_mkl_i8/pam --help  :
Usage: pam [options] --inp=*.inp --mol=*.mol [--pcm=*.pcm] [--pot=*.pot]
   or: pam [options] --inp=*.inp --mol=*.xyz [--pcm=*.pcm] [--pot=*.pot]
   or: pam [options] --fullrestart=ARCHIVE.tgz [--inp=*.inp] [--mol=*.mol | --mol=*.xyz] [--pcm=*.pcm] [--pot=*.pot]
Options are also read from ~/.diracrc

Options:
  -h, --help            show this help message and exit

  Input files - mandatory specifications:
    --inp=INP_FILE      Dirac input file containing the job directives [*.inp]
    --mol=MOL_FILE      file containing the basis set and geometry
                        specifications [*.mol or *.xyz]
    --pcm=PCM_FILE      PCMSolver input file [*.pcm]
    --pot=POT_FILE      file containing the polarizable embedding potential
                        parameters [*.pot]
    --fullrestart=ARCHIVE.tgz
                        restart from files within ARCHIVE.tgz;
                        old input files from ARCHIVE.tgz are reused unless new
                        input files are provided with one or more of "--inp",
                        "--mol", "--pcm", and "--pot".

  Output files:
    --suffix=STRING     file suffix for output [default: out]
    --rsh=STRING        Remote shell program, typically rsh or ssh [default:
                        ssh or taken from .dirarc]
    --rcp=STRING        Remote shell program, typically rcp or scp [default:
                        scp or taken from .dirarc]
    --noh5              do not create a h5 acheckpoint file [default: create
                        it]
    --noarch            do not create a tgz archive [default: create it]
    --nobackup          do not backup old outputs [default: back them up]

  Memory specification:
    --mw=INTEGER        set max memory (in megawords) for WORK array
                        for the master (for each MPI thread) [default: 64]
    --mb=INTEGER        set max memory (in MB) for WORK array
                        for the master (for each MPI thread) [default: none]
    --gb=FLOAT          set max memory (in GB) for WORK array
                        for the master (for each MPI thread) [default: none]
    --nw=INTEGER        set max memory (in megawords) for WORK array
                        for the co-workers (for each MPI thread) [default:
                        --mw set value]
    --aw=INTEGER        set max dynamically allocatable memory (in megawords)
                        (for each MPI thread) [default: 0]
    --ag=FLOAT          set max dynamically allocatable memory (in gigabytes)
                        (for each MPI thread) [default: 0]

  MPI settings:
    --mpi=INTEGER       set number of MPI processes in parallel run [default:
                        1]
    --mpirun=STRING     set the MPI run command, can have own arguments
                        [default: see "pam --show" output]
    --mpiarg=STRING     additonal options passed on to the MPI launcher;
                        useful arguments could be:
                        -envall (for MPICH2, Intel MPI)
                        -x "PATH LD_LIBRARY_PATH" (for Open MPI)
                        which will allow the user to pass the present
                        environment settings from the head node to the scratch
                        nodes.
    --mpi-workdir-argument=STRING
                        option to explicitly pass the scratch directory name
                        on to the slaves
                        ("wd" for MPICH2/Intel MPI, "wdir" for Open MPI).
    --machfile=STRING   specify a machinefile for a parallel run [default:
                        none]

  Managing data/file transfer:
    --put="file1 file2 my_file* ...", --copy="file1 file2 my_file* ..."
                        copy files to scratch directory
    --distribute="file1 file2 my_file*..."
                        copy selected files to scratch directories of working
                        nodes in parallel run
    --get="file1 file2 my_file*..."
                        get files from scratch directory
    --env="env_var1=value1 env_var2=value2 ..."
                        set environment variables
    --replace=NAME=VALUE
                        replace NAME by VALUE in the mol and inp files
    --incmo             copy CHECKPOINT to scratch directory [default: False]
    --outcmo            get CHECKPOINT from scratch directory [default: False]
    --inkrmc            copy KRMCSCF to scratch directory as KRMCOLD [default:
                        False]
    --outkrmc           get KRMCSCF and KRMCOLD from scratch directory
                        [default: False]
    --outqforce         get qforce.*.log from scratch directory [default:
                        False]
    --keep_scratch      keep the scratch directory after calculation [default:
                        delete it]

  Modify default paths and settings:
    --scratch=FULL_PATH
                        full path to scratch directory (DIRAC will append a
                        subdirectory to make it unique) [default read from
                        ~/.diracrc]
    --scratchfull=FULL_PATH
                        full path to scratch directory (DIRAC will not append
                        anything to this path; do NOT choose your /home/user
                        as scratchfull) [default: script defined full path
                        upon ~/.diracrc]
    --basis=FULL_PATH   basis set directory [default: .:/lustre/home/user/m/mi
                        lias/work/software/dirac/trunk_study/build_openmpi-
                        v5.0.7_icc2021_mkl_i8/basis:/lustre/home/user/m/milias
                        /work/software/dirac/trunk_study/build_openmpi-
                        v5.0.7_icc2021_mkl_i8/basis_dalton:/lustre/home/user/m
                        /milias/work/software/dirac/trunk_study/build_openmpi-
                        v5.0.7_icc2021_mkl_i8/basis_ecp]
    --dirac=FULL_PATH   dirac executable [default: /lustre/home/user/m/milias/
                        work/software/dirac/trunk_study/build_openmpi-
                        v5.0.7_icc2021_mkl_i8/dirac.x]

  Advanced options for programming and debugging:
    --show              print current settings (including those from .diracrc)
                        and exit
    --silent            run pam silent with minimal output
    --debug             run in debugger [default: False]
    --summit            run with summits jsrun [default: False]
    --debugger=FULL_PATH
                        set full path to debugger [default: in .diracrc]
    --valgrind          run with valgrind [default: False]
    --profile           perform profiling [default: False]
    --profiler=FULL_PATH
                        set full path to profiler [default: none]
    --timeout=TIME_STRING
                        limit dirac.x execution time (in #d#h#m#s, or in pure
                        seconds-integer) [default: no limit; reads env.var.
                        DIRTIMEOUT]


 DIRAC_MPI_COMMAND=/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.7_icc2021/bin/mpirun  --use-hwthread-cpus  --bind-to core:overload-allowed  -f /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/runs/GRID2025_data/FlSe_relcc/openmpi_v5.0.7_icc2021_mkl_i8/nodes.cascade.12784238 -np 6


 I am in the directory: /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/runs/GRID2025_data/FlSe_relcc/openmpi_v5.0.7_icc2021_mkl_i8


 Running parallel DIRAC job with  --mpi=6 :

 **** dirac-executable stderr console output : **** 
--------------------------------------------------------------------------
A request was made to bind that would require binding
processes to more cpus than are available in your allocation:

   Application:     /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_x2c-ach0.scf_relcc_m3.20-vir50.00_FlSe.dyall_acv3z_lsym_227694/dirac.x
   #processes:      6
   Mapping policy:  BYCORE
   Binding policy:  CORE

You can override this protection by adding the "overload-allowed"
option to your binding directive.
--------------------------------------------------------------------------
 
directory: /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/runs/GRID2025_data/FlSe_relcc/openmpi_v5.0.7_icc2021_mkl_i8
   inputs: ../FlSe.dyall_acv3z_lsym.mol  &  ../x2c-ach0.scf_relcc_m3.20-vir50.00.inp

  DIRAC pam script running:

  user           : milias
  machine        : n02p051
  date and time  : 2025-07-03 13:21:48.760293
  input dir      : /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/runs/GRID2025_data/FlSe_relcc/openmpi_v5.0.7_icc2021_mkl_i8
  pam command    : /lustre/home/user/m/milias/work/software/dirac/trunk_study/build_openmpi-v5.0.7_icc2021_mkl_i8/pam
  all pam args   : ['--mpi=6', '--put=../DFPCMO.FlSe.acv3z_x2c_scf_lsym=DFPCMO', '--noarch', '--gb=122.0', '--ag=123.0', '--inp=../x2c-ach0.scf_relcc_m3.20-vir50.00.inp', '--mol=../FlSe.dyall_acv3z_lsym.mol', '--suffix=out.gvr.cascade.N1.n6.jid12784238.out']
  executable     : /lustre/home/user/m/milias/work/software/dirac/trunk_study/build_openmpi-v5.0.7_icc2021_mkl_i8/dirac.x
  scratch dir    : /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_x2c-ach0.scf_relcc_m3.20-vir50.00_FlSe.dyall_acv3z_lsym_227694 (availspace=129874.760[GB])
  output file    : x2c-ach0.scf_relcc_m3.20-vir50.00_FlSe.dyall_acv3z_lsym.out.gvr.cascade.N1.n6.jid12784238.out
  DIRAC run      : parallel (launcher: /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.7_icc2021/bin/mpirun)
  local disks    : False
  rsh/rcp        : ssh / scp
  machine file   : None

  Setting MKL and OPENMP environment to default values (if not set already)
   Variabl.MKL_NUM_THREADS =  1
   Variabl.MKL_DYNAMIC =  FALSE
   OMP_NUM_THREADS = 1
   OMP_DYNAMIC="FALSE"
  Setting environment variables (as specified explicitly or in .diracrc)

  Creating the scratch directory.
  Copying /lustre/home/user/m/milias/work/software/dirac/trunk_study/build_openmpi-v5.0.7_icc2021_mkl_i8/dirac.x to /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_x2c-ach0.scf_relcc_m3.20-vir50.00_FlSe.dyall_acv3z_lsym_227694/dirac.x
  Copying ../FlSe.dyall_acv3z_lsym.mol to /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_x2c-ach0.scf_relcc_m3.20-vir50.00_FlSe.dyall_acv3z_lsym_227694/MOLECULE.MOL
  Copying ../x2c-ach0.scf_relcc_m3.20-vir50.00.inp to /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_x2c-ach0.scf_relcc_m3.20-vir50.00_FlSe.dyall_acv3z_lsym_227694/DIRAC.INP
  Copying ../DFPCMO.FlSe.acv3z_x2c_scf_lsym to /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_x2c-ach0.scf_relcc_m3.20-vir50.00_FlSe.dyall_acv3z_lsym_227694/DFPCMO
  Basis set libraries (default)    : /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/runs/GRID2025_data/FlSe_relcc/openmpi_v5.0.7_icc2021_mkl_i8
                        /lustre/home/user/m/milias/work/software/dirac/trunk_study/build_openmpi-v5.0.7_icc2021_mkl_i8/basis
                        /lustre/home/user/m/milias/work/software/dirac/trunk_study/build_openmpi-v5.0.7_icc2021_mkl_i8/basis_dalton
                        /lustre/home/user/m/milias/work/software/dirac/trunk_study/build_openmpi-v5.0.7_icc2021_mkl_i8/basis_ecp
  DIRAC command  : /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.7_icc2021/bin/mpirun -np 6 /lustre/home/user/m/milias/DIRAC_scratch_directory/milias/DIRAC_x2c-ach0.scf_relcc_m3.20-vir50.00_FlSe.dyall_acv3z_lsym_227694/dirac.x (PID=227697)
  pam, stdout info: process ended with nonzero stderr stream - check 
  content of the (master) scratch directory
  ------------------------------------------------------------------------------
                     name    size (MB)    last accessed
  ------------------------------------------------------------------------------
             MOLECULE.MOL      0.000   07/03/2025 01:21:49 PM
                DIRAC.INP      0.001   07/03/2025 01:21:49 PM
                   DFPCMO      6.685   07/03/2025 01:21:49 PM
        schema_labels.txt      0.017   07/03/2025 01:21:48 PM
                  dirac.x     78.635   07/03/2025 01:21:48 PM
  ------------------------------------------------------------------------------
      Total size of all files :      85.337  MB 
      Disk info:  used    available   capacity [GB]
             535725.261 129874.739   665600.000 

  Could not construct hdf5 checkpoint file
  going to delete the scratch directory ... done

  exit date      : 2025-07-03 13:21:51.312604
  elapsed time   : 00h00m02s
  exit           : ABNORMAL (CHECK DIRAC OUTPUT)
