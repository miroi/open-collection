#!/bin/bash

#SBATCH -J Se4CCx

##  partition (queue)
##SBATCH -p cascade
##SBATCH -p knl
##SBATCH -p flnr-ice
#SBATCH -p slo-ice

## max. execution time
#SBATCH -t 2-00:00:00

##SBATCH --exclusive

##SBATCH --mem=224GB
#SBATCH --mem=124GB
##SBATCH --mem=64GB

# do not restart in the case of nodefail!
#SBATCH --no-requeue
#SBATCH --no-kill

#SBATCH -N 1
#SBATCH --ntasks-per-node=12

##SBATCH  --sockets-per-node=1

## memory NO !!!
##SBATCH --mem-per-cpu=28G

## stdout/stderr output file
##SBATCH -o log_slurm_job.%j.%N.std_out_err
#SBATCH -o log_slurm_job.%j.%N.std_out

## Email
#SBATCH --mail-user=milias@theor.jinr.ru
#SBATCH --mail-type=ALL

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"
ulimit -s unlimited

echo -e "\n\n The total memory at the node (in GB)"
free -t -g
 
echo -e "\n\n Job user is $SLURM_JOB_USER and his job $SLURM_JOB_NAME has assigned ID $SLURM_JOBID"
echo -e "This job was submitted from the computer SLURM_SUBMIT_HOST=$SLURM_SUBMIT_HOST"
echo -e "\nThis job was submitted from the home directory:"
echo -e "SLURM_SUBMIT_DIR=$SLURM_SUBMIT_DIR"
echo -e "\nThis job is running on the cluster compute node:"
echo -e "SLURM_CLUSTER_NAME=$SLURM_CLUSTER_NAME"
echo -e "\nThis job is employing SLURM_JOB_NUM_NODES=$SLURM_JOB_NUM_NODES node/nodes:"
echo -e "SLURM_JOB_NODELIST = $SLURM_JOB_NODELIST"
echo -e "\nJob's partition is  SLURM_JOB_PARTITION=$SLURM_JOB_PARTITION"

echo -e "\nThis job reserved SLURM_CPUS_ON_NODE=$SLURM_CPUS_ON_NODE threads per node."
echo -e "This job wants, in total, SLURM_NTASKS=$SLURM_NTASKS threads . "
#
# CPU model
echo -e "\n\nThe master's node CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo "This node has total NPROCS=$NPROCS CPUs available for an master EXCLUSIVE job."

MACHINEFILE="nodes.$SLURM_JOB_PARTITION.$SLURM_JOB_ID"
# Generate Machinefile for mpi such that hosts are in the same
# order as if run via srun
srun -l /bin/hostname | sort -n | awk '{print $2}' > $MACHINEFILE
echo -e "\n\n Generated machinefile for MPI, MACHINEFILE=$MACHINEFILE"
ls -lt $PWD/$MACHINEFILE
echo -e "MACHINEFILE contains these nodes:"
#cat $MACHINEFILE
# print nicely in the line !
paste -s $MACHINEFILE

echo -e "\n\n Loaded modules:"
module purge
module add GVR/v1.0-1
module add Python/v3.10.2
module add CMake/v3.29.2
module add intel/v2021.1
#module add openmpi/v5.0.3_gcc1230
#module add LAPACK/v3.12.0_gcc1230
module add  gcc/v12.3.0
module list

# Add your own OpenMPI-i8
export PATH=$PATH:/lustre/home/user/m/milias/work/software/openmpi/openmpi-5.0.7-gnu12.3.0-i8/openmpi-5.0.7/bin
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/lustre/home/user/m/milias/work/software/openmpi/openmpi-5.0.7-gnu12.3.0-i8/openmpi-5.0.7/lib

echo -e "\n\n The environmental variable PATH=$PATH "
echo -e "\n The environmental variable LD_LIBRARY_PATH=$LD_LIBRARY_PATH "

echo -e "\n python ? :\c"; which python; python -V
echo -e " cmake ? :\c";  which cmake; cmake --version
echo -e " mpif90 ? :\c"; which mpif90; mpif90 --version
echo -e " mpicc ? :\c"; which mpicc; mpicc --version
echo -e " mpicxx ? :\c"; which mpicxx; mpicxx  --version
echo -e " mpirun ? :\c"; which mpirun; mpirun  --version
echo -e " ompi_info -a | grep 'Fort integer size'"; ompi_info -a | grep 'Fort integer size'
#echo -e "\n LAPACK and  BLAS libs in /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/LAPACK/v3.12.0_gcc1230/lib64 ?";
#ls -lt /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/LAPACK/v3.12.0_gcc1230/lib64
echo -e "\n Intel MKL library ?  MKLROOT=$MKLROOT ?";

#DIRAC=/lustre/home/user/m/milias/work/software/dirac/trunk
DIRAC=/lustre/home/user/m/milias/work/software/dirac/trunk_study
#BUILD=build_intelmpi_mkl_i8
BUILD=build_openmpi5-i8_mkl_i8

echo -e "\n\n checking the workhorse executable, ldd $DIRAC/$BUILD/dirac.x:"
ldd $DIRAC/$BUILD/dirac.x
echo -e "\n checking pam script, $DIRAC/$BUILD/pam --help  :"
$DIRAC/$BUILD/pam --help

export OMPI_MCA_oob=^usock
export MKL_NUM_THREADS=1
export MKL_DYNAMIC=FALSE

export DIRAC_MPI_COMMAND="mpirun -f $SLURM_SUBMIT_DIR/$MACHINEFILE -np $SLURM_NTASKS"
echo -e "\n DIRAC_MPI_COMMAND=$DIRAC_MPI_COMMAND"

out_suffix=out.$SLURM_CLUSTER_NAME.$SLURM_JOB_PARTITION.N$SLURM_JOB_NUM_NODES.n$SLURM_NTASKS.jid$SLURM_JOBID.out

echo -e "\n\n I am in the directory: \c "; pwd

#
# DIRAC test directory, input, mol files and some pam parameters
#
THISMPI=$SLURM_NTASKS
echo -e "\n\n Running parallel DIRAC job: \n"
$DIRAC/$BUILD/pam --mpi=$THISMPI  --put="../DFPCMO.Se4.scf_v2z_x2c_Cs=DFPCMO" --noarch --gb=122.0 --ag=123.0 --inp=x2c.scf_relcc_adjust.inp   --mol=Se4.v2z_b3lyp_Cs.mol  --suffix=$out_suffix

exit
