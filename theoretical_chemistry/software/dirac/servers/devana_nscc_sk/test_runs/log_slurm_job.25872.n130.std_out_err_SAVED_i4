Job user is milias and his job Dctest has assigned ID 25872
This job was submitted from the computer login01.devana.local
and from the home directory:
/home/milias/work/projects/open-collection/theoretical_chemistry/software/dirac/servers/devana_nscc_sk/test_runs

It is running on the cluster compute node:
devana
and is employing 1 node/nodes:
n130

Job partition is ncpu 

The job requests 8 CPU per task.

 loading modules...
Loading mkl version 2023.0.0
Loading tbb version 2021.8.0
Loading compiler-rt version 2023.0.0

The following have been reloaded with a version change:
  1) GCCcore/11.2.0 => GCCcore/10.3.0
  2) XZ/5.2.5-GCCcore-11.2.0 => XZ/5.2.5-GCCcore-10.3.0
  3) zlib/1.2.11-GCCcore-11.2.0 => zlib/1.2.11-GCCcore-10.3.0


The following have been reloaded with a version change:
  1) GCCcore/10.3.0 => GCCcore/11.3.0
  2) UCX/1.11.2-GCCcore-11.2.0 => UCX/1.12.1-GCCcore-11.3.0
  3) binutils/2.37-GCCcore-11.2.0 => binutils/2.38-GCCcore-11.3.0
  4) intel-compilers/2021.4.0 => intel-compilers/2022.1.0
  5) numactl/2.0.14-GCCcore-11.2.0 => numactl/2.0.14-GCCcore-11.3.0
  6) zlib/1.2.11-GCCcore-10.3.0 => zlib/1.2.12-GCCcore-11.3.0



 loaded modules:

Currently Loaded Modules:
  1) libxml2/2.9.10-GCCcore-11.2.0
  2) libpciaccess/0.16-GCCcore-11.2.0
  3) hwloc/2.5.0-GCCcore-11.2.0
  4) OpenSSL/1.1
  5) libevent/2.1.12-GCCcore-11.2.0
  6) libfabric/1.13.2-GCCcore-11.2.0
  7) PMIx/4.1.0-GCCcore-11.2.0
  8) OpenMPI/4.1.1-intel-compilers-2021.4.0
  9) tbb/2021.8.0
 10) compiler-rt/2023.0.0
 11) mkl/latest
 12) ncurses/6.2-GCCcore-10.3.0
 13) bzip2/1.0.8-GCCcore-10.3.0
 14) cURL/7.76.0-GCCcore-10.3.0
 15) XZ/5.2.5-GCCcore-10.3.0
 16) libarchive/3.5.1-GCCcore-10.3.0
 17) CMake/3.20.1-GCCcore-10.3.0
 18) GCCcore/11.3.0
 19) zlib/1.2.12-GCCcore-11.3.0
 20) binutils/2.38-GCCcore-11.3.0
 21) intel-compilers/2022.1.0
 22) numactl/2.0.14-GCCcore-11.3.0
 23) UCX/1.12.1-GCCcore-11.3.0
 24) impi/2021.6.0-intel-compilers-2022.1.0
 25) iimpi/2022a
 26) Szip/2.1.1-GCCcore-11.3.0
 27) HDF5/1.12.2-iimpi-2022a

 


Running on host n130
Time is Mon Jul 24 18:35:44 CEST 2023 

The node's CPU model name	: Intel(R) Xeon(R) Gold 6338 CPU @ 2.00GHz
This node has total 64 CPUs available for EXCLUSIVE job.
This node has 8 CPUs allocated for SLURM calculations.

 The total memory at the node (in GB)
              total        used        free      shared  buff/cache   available
Mem:            251           4         238           7           8         238
Swap:             0           0           0
Total:          251           4         238



ldd /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x:
	linux-vdso.so.1 =>  (0x00007fff24593000)
	libmkl_intel_lp64.so.2 => /storage-apps/intel-v.2023/oneapi/mkl/2023.0.0/lib/intel64/libmkl_intel_lp64.so.2 (0x00002af411701000)
	libmkl_intel_thread.so.2 => /storage-apps/intel-v.2023/oneapi/mkl/2023.0.0/lib/intel64/libmkl_intel_thread.so.2 (0x00002af41292d000)
	libmkl_core.so.2 => /storage-apps/intel-v.2023/oneapi/mkl/2023.0.0/lib/intel64/libmkl_core.so.2 (0x00002af4160c6000)
	libiomp5.so => /storage-apps/easybuild/software/intel-compilers/2022.1.0/compiler/2022.1.0/linux/compiler/lib/intel64_lin/libiomp5.so (0x00002af41a4a8000)
	libmpi.so.40 => /storage-apps/easybuild/software/OpenMPI/4.1.1-intel-compilers-2021.4.0/lib/libmpi.so.40 (0x00002af411504000)
	libimf.so => /storage-apps/easybuild/software/intel-compilers/2022.1.0/compiler/2022.1.0/linux/compiler/lib/intel64_lin/libimf.so (0x00002af41a8e1000)
	libsvml.so => /storage-apps/easybuild/software/intel-compilers/2022.1.0/compiler/2022.1.0/linux/compiler/lib/intel64_lin/libsvml.so (0x00002af41af6f000)
	libirng.so => /storage-apps/easybuild/software/intel-compilers/2022.1.0/compiler/2022.1.0/linux/compiler/lib/intel64_lin/libirng.so (0x00002af41cf2d000)
	libstdc++.so.6 => /storage-apps/easybuild/software/GCCcore/11.3.0/lib64/libstdc++.so.6 (0x00002af41d297000)
	libm.so.6 => /lib64/libm.so.6 (0x00002af41d4ab000)
	libgcc_s.so.1 => /storage-apps/easybuild/software/GCCcore/11.3.0/lib64/libgcc_s.so.1 (0x00002af41d7ad000)
	libirc.so => /storage-apps/easybuild/software/intel-compilers/2022.1.0/compiler/2022.1.0/linux/compiler/lib/intel64_lin/libirc.so (0x00002af41d7c7000)
	libc.so.6 => /lib64/libc.so.6 (0x00002af41da3f000)
	libdl.so.2 => /lib64/libdl.so.2 (0x00002af41de0d000)
	libz.so.1 => /storage-apps/easybuild/software/zlib/1.2.12-GCCcore-11.3.0/lib/libz.so.1 (0x00002af41e011000)
	libhdf5.so.200 => /storage-apps/easybuild/software/HDF5/1.12.2-iimpi-2022a/lib/libhdf5.so.200 (0x00002af41e02b000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x00002af41e5a9000)
	libsz.so.2 => /storage-apps/easybuild/software/Szip/2.1.1-GCCcore-11.3.0/lib/libsz.so.2 (0x00002af41e7c5000)
	libmpi_usempif08.so.40 => /storage-apps/easybuild/software/OpenMPI/4.1.1-intel-compilers-2021.4.0/lib/libmpi_usempif08.so.40 (0x00002af41e7de000)
	libmpi_usempi_ignore_tkr.so.40 => /storage-apps/easybuild/software/OpenMPI/4.1.1-intel-compilers-2021.4.0/lib/libmpi_usempi_ignore_tkr.so.40 (0x00002af41e819000)
	libmpi_mpifh.so.40 => /storage-apps/easybuild/software/OpenMPI/4.1.1-intel-compilers-2021.4.0/lib/libmpi_mpifh.so.40 (0x00002af41e826000)
	/lib64/ld-linux-x86-64.so.2 (0x00002af4114dd000)
	librt.so.1 => /lib64/librt.so.1 (0x00002af41e892000)
	libopen-rte.so.40 => /storage-apps/easybuild/software/OpenMPI/4.1.1-intel-compilers-2021.4.0/lib/libopen-rte.so.40 (0x00002af41ea9a000)
	libopen-orted-mpir.so => /storage-apps/easybuild/software/OpenMPI/4.1.1-intel-compilers-2021.4.0/lib/libopen-orted-mpir.so (0x00002af41eb63000)
	libopen-pal.so.40 => /storage-apps/easybuild/software/OpenMPI/4.1.1-intel-compilers-2021.4.0/lib/libopen-pal.so.40 (0x00002af41eb68000)
	libutil.so.1 => /lib64/libutil.so.1 (0x00002af41ec33000)
	libhwloc.so.15 => /storage-apps/easybuild/software/hwloc/2.5.0-GCCcore-11.2.0/lib/libhwloc.so.15 (0x00002af41ee36000)
	libpciaccess.so.0 => /storage-apps/easybuild/software/libpciaccess/0.16-GCCcore-11.2.0/lib/libpciaccess.so.0 (0x00002af41ee93000)
	libxml2.so.2 => /storage-apps/easybuild/software/libxml2/2.9.10-GCCcore-11.2.0/lib/libxml2.so.2 (0x00002af41ee9e000)
	liblzma.so.5 => /storage-apps/easybuild/software/XZ/5.2.5-GCCcore-11.2.0/lib/liblzma.so.5 (0x00002af41f00c000)
	libevent_core-2.1.so.7 => /storage-apps/easybuild/software/libevent/2.1.12-GCCcore-11.2.0/lib/libevent_core-2.1.so.7 (0x00002af41f034000)
	libevent_pthreads-2.1.so.7 => /storage-apps/easybuild/software/libevent/2.1.12-GCCcore-11.2.0/lib/libevent_pthreads-2.1.so.7 (0x00002af41f06b000)
	libintlc.so.5 => /storage-apps/easybuild/software/intel-compilers/2022.1.0/compiler/2022.1.0/linux/compiler/lib/intel64_lin/libintlc.so.5 (0x00002af41f06f000)
	libmpifort.so.12 => /storage-apps/easybuild/software/impi/2021.6.0-intel-compilers-2022.1.0/mpi/2021.6.0/lib/libmpifort.so.12 (0x00002af41f2e7000)
	libmpi.so.12 => /storage-apps/easybuild/software/impi/2021.6.0-intel-compilers-2022.1.0/mpi/2021.6.0/lib/release/libmpi.so.12 (0x00002af41f69b000)
	libifport.so.5 => /storage-apps/easybuild/software/intel-compilers/2022.1.0/compiler/2022.1.0/linux/compiler/lib/intel64_lin/libifport.so.5 (0x00002af420ee3000)
	libifcoremt.so.5 => /storage-apps/easybuild/software/intel-compilers/2022.1.0/compiler/2022.1.0/linux/compiler/lib/intel64_lin/libifcoremt.so.5 (0x00002af421111000)

 ifort -V: Intel(R) Fortran Intel(R) 64 Compiler Classic for applications running on Intel(R) 64, Version 2021.6.0 Build 20220226_000000
Copyright (C) 1985-2022 Intel Corporation.  All rights reserved.


My PATH=/storage-apps/easybuild/software/HDF5/1.12.2-iimpi-2022a/bin:/storage-apps/easybuild/software/impi/2021.6.0-intel-compilers-2022.1.0/mpi/2021.6.0/libfabric/bin:/storage-apps/easybuild/software/impi/2021.6.0-intel-compilers-2022.1.0/mpi/2021.6.0/bin:/storage-apps/easybuild/software/UCX/1.12.1-GCCcore-11.3.0/bin:/storage-apps/easybuild/software/numactl/2.0.14-GCCcore-11.3.0/bin:/storage-apps/easybuild/software/intel-compilers/2022.1.0/compiler/2022.1.0/linux/bin/intel64:/storage-apps/easybuild/software/intel-compilers/2022.1.0/compiler/2022.1.0/linux/bin:/storage-apps/easybuild/software/binutils/2.38-GCCcore-11.3.0/bin:/storage-apps/easybuild/software/GCCcore/11.3.0/bin:/storage-apps/easybuild/software/CMake/3.20.1-GCCcore-10.3.0/bin:/storage-apps/easybuild/software/libarchive/3.5.1-GCCcore-10.3.0/bin:/storage-apps/easybuild/software/XZ/5.2.5-GCCcore-10.3.0/bin:/storage-apps/easybuild/software/cURL/7.76.0-GCCcore-10.3.0/bin:/storage-apps/easybuild/software/bzip2/1.0.8-GCCcore-10.3.0/bin:/storage-apps/easybuild/software/ncurses/6.2-GCCcore-10.3.0/bin:/storage-apps/easybuild/software/OpenMPI/4.1.1-intel-compilers-2021.4.0/bin:/storage-apps/easybuild/software/PMIx/4.1.0-GCCcore-11.2.0/bin:/storage-apps/easybuild/software/libfabric/1.13.2-GCCcore-11.2.0/bin:/storage-apps/easybuild/software/libevent/2.1.12-GCCcore-11.2.0/bin:/storage-apps/easybuild/software/OpenSSL/1.1/bin:/storage-apps/easybuild/software/hwloc/2.5.0-GCCcore-11.2.0/sbin:/storage-apps/easybuild/software/hwloc/2.5.0-GCCcore-11.2.0/bin:/storage-apps/easybuild/software/libxml2/2.9.10-GCCcore-11.2.0/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:.

Python -v :Python 2.7.5
cmake ? which cmake = /storage-apps/easybuild/software/CMake/3.20.1-GCCcore-10.3.0/bin/cmake
ctest ? which ctest = /storage-apps/easybuild/software/CMake/3.20.1-GCCcore-10.3.0/bin/ctest
ctest --version ctest version 3.20.1

CMake suite maintained and supported by Kitware (kitware.com/cmake).

 mpirun ? /storage-apps/easybuild/software/impi/2021.6.0-intel-compilers-2022.1.0/mpi/2021.6.0/bin/mpirun
Intel(R) MPI Library for Linux* OS, Version 2021.6 Build 20220227 (id: 28877f3f32)
Copyright 2003-2022, Intel Corporation.


DIRAC scratch directory space, /scratch/p175-23-t
Filesystem      Size  Used Avail Use% Mounted on
beegfs_nodev    282T  5.8T  276T   3% /scratch

 For comparison, /tmp local disk;  df -h /tmp/.
Filesystem      Size  Used Avail Use% Mounted on
rootfs          126G  2.7G  124G   3% /



 Running short test suite on the hostname=n130
DIRAC_MPI_COMMAND=mpirun -np 2
UpdateCTestConfiguration  from :/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/DartConfiguration.tcl
Parse Config file:/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/DartConfiguration.tcl
Parse Config file:/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/DartConfiguration.tcl
Test project /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 86
      Start  86: reladc_dip

86: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/reladc_dip/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/reladc_dip" "--verbose"
86: Test timeout computed to be: 1500
test 87
      Start  87: x2c-SCF_to_4c-SCF

87: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/x2c-SCF_to_4c-SCF/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/x2c-SCF_to_4c-SCF" "--verbose"
87: Test timeout computed to be: 1500
test 88
      Start  88: xyz_symmetry_recognition

88: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/xyz_symmetry_recognition/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition" "--verbose"
88: Test timeout computed to be: 1500
test 89
      Start  89: nmqm_operator

89: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/nmqm_operator/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/nmqm_operator" "--verbose"
89: Test timeout computed to be: 1500
86: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=real_ne_dip.inp', '--mol=ne_d2h.mol']
86: 
86:  **** dirac-executable stderr console output : **** 
86: [n130:43048] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
86: [n130:43049] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
86: --------------------------------------------------------------------------
86: The application appears to have been direct launched using "srun",
86: but OMPI was not built with SLURM's PMI support and therefore cannot
86: execute. There are several options for building PMI support under
86: SLURM, depending upon the SLURM version you are using:
86: 
86:   version 16.05 or later: you can use SLURM's PMIx support. This
86:   requires that you configure and build SLURM --with-pmix.
86: 
86:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
86:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
86:   install PMI-2. You must then build Open MPI using --with-pmi pointing
86:   to the SLURM PMI library location.
86: 
86: Please configure as appropriate and try again.
86: --------------------------------------------------------------------------
86: *** An error occurred in MPI_Init_thread
86: *** on a NULL communicator
86: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
86: ***    and potentially your MPI job)
86: [n130:43049] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
86: --------------------------------------------------------------------------
86: The application appears to have been direct launched using "srun",
86: but OMPI was not built with SLURM's PMI support and therefore cannot
86: execute. There are several options for building PMI support under
86: SLURM, depending upon the SLURM version you are using:
86: 
86:   version 16.05 or later: you can use SLURM's PMIx support. This
86:   requires that you configure and build SLURM --with-pmix.
86: 
86:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
86:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
86:   install PMI-2. You must then build Open MPI using --with-pmi pointing
86:   to the SLURM PMI library location.
86: 
86: Please configure as appropriate and try again.
86: --------------------------------------------------------------------------
86: *** An error occurred in MPI_Init_thread
86: *** on a NULL communicator
86: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
86: ***    and potentially your MPI job)
86: [n130:43048] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
86: 
86: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/reladc_dip
86:    inputs: ne_d2h.mol  &  real_ne_dip.inp
86: 
86: running test: real_ne_dip ne_d2h
 1/92 Test  #86: reladc_dip .......................***Failed    4.00 sec
test 90
      Start  90: visual_div_rot

90: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/visual_div_rot/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_div_rot" "--verbose"
90: Test timeout computed to be: 1500
89: 
89: running test with input files ['MgF.dc_rkb.scf_prptra_nmqm.inp', 'MgF.sto-2g.C2v.mol'] and args None
89: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=MgF.dc_rkb.scf_prptra_nmqm.inp', '--mol=MgF.sto-2g.C2v.mol']
89: 
89:  **** dirac-executable stderr console output : **** 
89: [n130:43062] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
89: --------------------------------------------------------------------------
89: The application appears to have been direct launched using "srun",
89: but OMPI was not built with SLURM's PMI support and therefore cannot
89: execute. There are several options for building PMI support under
89: SLURM, depending upon the SLURM version you are using:
89: 
89:   version 16.05 or later: you can use SLURM's PMIx support. This
89:   requires that you configure and build SLURM --with-pmix.
89: 
89:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
89:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
89:   install PMI-2. You must then build Open MPI using --with-pmi pointing
89:   to the SLURM PMI library location.
89: 
89: Please configure as appropriate and try again.
89: --------------------------------------------------------------------------
89: *** An error occurred in MPI_Init_thread
89: *** on a NULL communicator
89: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
89: ***    and potentially your MPI job)
89: [n130:43062] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
89: [n130:43063] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
89: --------------------------------------------------------------------------
89: The application appears to have been direct launched using "srun",
89: but OMPI was not built with SLURM's PMI support and therefore cannot
89: execute. There are several options for building PMI support under
89: SLURM, depending upon the SLURM version you are using:
89: 
89:   version 16.05 or later: you can use SLURM's PMIx support. This
89:   requires that you configure and build SLURM --with-pmix.
89: 
89:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
89:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
89:   install PMI-2. You must then build Open MPI using --with-pmi pointing
89:   to the SLURM PMI library location.
89: 
89: Please configure as appropriate and try again.
89: --------------------------------------------------------------------------
89: *** An error occurred in MPI_Init_thread
89: *** on a NULL communicator
89: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
89: ***    and potentially your MPI job)
89: [n130:43063] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
89: 
89: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/nmqm_operator
89:    inputs: MgF.sto-2g.C2v.mol  &  MgF.dc_rkb.scf_prptra_nmqm.inp
89: 
89: running test with input files ['MgF.dc_rkb.scf_prptra_nmqm_def.inp', 'MgF.sto-2g.C2v.mol'] and args None
89: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=MgF.dc_rkb.scf_prptra_nmqm_def.inp', '--mol=MgF.sto-2g.C2v.mol']
89: 
89:  **** dirac-executable stderr console output : **** 
89: [n130:43166] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
89: [n130:43167] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
89: --------------------------------------------------------------------------
89: The application appears to have been direct launched using "srun",
89: but OMPI was not built with SLURM's PMI support and therefore cannot
89: execute. There are several options for building PMI support under
89: SLURM, depending upon the SLURM version you are using:
88: 
88: running test with input files ['test_xyz.inp', 'molecule01.xyz'] and args None
88: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=test_xyz.inp', '--mol=molecule01.xyz']
88: 
88:  **** dirac-executable stderr console output : **** 
88: [n130:43064] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:43064] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: [n130:43065] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:43065] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: 
88: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition
88:    inputs: molecule01.xyz  &  test_xyz.inp
88: 
88: running test with input files ['test_xyz.inp', 'molecule02.xyz'] and args None
88: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=test_xyz.inp', '--mol=molecule02.xyz']
88: 
88:  **** dirac-executable stderr console output : **** 
88: [n130:43184] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
87: 
87: running test: x2c H2O
87: found error which is expected/accepted: 2
87: 
87: running test: 4c H2O
87: found error which is expected/accepted: 2
87: 
87: running test: x2c-SCF_to_4c-SCF H2O
87: found error which is expected/accepted: 2
89: 
89:   version 16.05 or later: you can use SLURM's PMIx support. This
89:   requires that you configure and build SLURM --with-pmix.
89: 
89:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
89:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
89:   install PMI-2. You must then build Open MPI using --with-pmi pointing
89:   to the SLURM PMI library location.
89: 
89: Please configure as appropriate and try again.
89: --------------------------------------------------------------------------
89: *** An error occurred in MPI_Init_thread
89: *** on a NULL communicator
89: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
89: ***    and potentially your MPI job)
89: [n130:43166] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
89: --------------------------------------------------------------------------
89: The application appears to have been direct launched using "srun",
89: but OMPI was not built with SLURM's PMI support and therefore cannot
89: execute. There are several options for building PMI support under
89: SLURM, depending upon the SLURM version you are using:
89: 
89:   version 16.05 or later: you can use SLURM's PMIx support. This
89:   requires that you configure and build SLURM --with-pmix.
89: 
89:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
89:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
89:   install PMI-2. You must then build Open MPI using --with-pmi pointing
89:   to the SLURM PMI library location.
89: 
89: Please configure as appropriate and try again.
89: --------------------------------------------------------------------------
89: *** An error occurred in MPI_Init_thread
89: *** on a NULL communicator
89: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
89: ***    and potentially your MPI job)
89: [n130:43167] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
89: 
89: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/nmqm_operator
89:    inputs: MgF.sto-2g.C2v.mol  &  MgF.dc_rkb.scf_prptra_nmqm_def.inp
89: 
89: running test with input files ['MgF.dc_rkb.scf_relcc_nmqm.1e-4.inp', 'MgF.sto-2g.C2v.mol'] and args None
89: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=MgF.dc_rkb.scf_relcc_nmqm.1e-4.inp', '--mol=MgF.sto-2g.C2v.mol']
89: 
89:  **** dirac-executable stderr console output : **** 
89: [n130:43307] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
89: --------------------------------------------------------------------------
89: The application appears to have been direct launched using "srun",
89: but OMPI was not built with SLURM's PMI support and therefore cannot
89: execute. There are several options for building PMI support under
89: SLURM, depending upon the SLURM version you are using:
89: 
89:   version 16.05 or later: you can use SLURM's PMIx support. This
89:   requires that you configure and build SLURM --with-pmix.
89: 
89:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
89:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
89:   install PMI-2. You must then build Open MPI using --with-pmi pointing
89:   to the SLURM PMI library location.
89: 
89: Please configure as appropriate and try again.
89: --------------------------------------------------------------------------
89: *** An error occurred in MPI_Init_thread
89: *** on a NULL communicator
89: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
89: ***    and potentially your MPI job)
89: [n130:43307] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
89: [n130:43308] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
89: --------------------------------------------------------------------------
89: The application appears to have been direct launched using "srun",
89: but OMPI was not built with SLURM's PMI support and therefore cannot
89: execute. There are several options for building PMI support under
89: SLURM, depending upon the SLURM version you are using:
89: 
89:   version 16.05 or later: you can use SLURM's PMIx support. This
89:   requires that you configure and build SLURM --with-pmix.
89: 
89:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
89:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
89:   install PMI-2. You must then build Open MPI using --with-pmi pointing
89:   to the SLURM PMI library location.
89: 
89: Please configure as appropriate and try again.
89: --------------------------------------------------------------------------
89: *** An error occurred in MPI_Init_thread
89: *** on a NULL communicator
89: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
89: ***    and potentially your MPI job)
89: [n130:43308] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
89: 
89: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/nmqm_operator
89:    inputs: MgF.sto-2g.C2v.mol  &  MgF.dc_rkb.scf_relcc_nmqm.1e-4.inp
 2/92 Test  #87: x2c-SCF_to_4c-SCF ................   Passed   11.96 sec
test 91
      Start  91: bed_isotropic

91: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/bed_isotropic/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/bed_isotropic" "--verbose"
91: Test timeout computed to be: 1500
 3/92 Test  #89: nmqm_operator ....................***Failed   11.95 sec
test 92
      Start  92: localization

92: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/localization/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/localization" "--verbose"
92: Test timeout computed to be: 1500
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:43184] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: [n130:43185] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:43185] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: 
88: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition
88:    inputs: molecule02.xyz  &  test_xyz.inp
88: 
88: running test with input files ['test_xyz.inp', 'molecule03.xyz'] and args None
88: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=test_xyz.inp', '--mol=molecule03.xyz']
88: 
88:  **** dirac-executable stderr console output : **** 
88: [n130:43329] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:43329] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: [n130:43328] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
90: 
90: running test with input files ['response_1z_bz.inp', 'co.mol'] and args --outcmo --get="PAMXVC"
90: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=response_1z_bz.inp', '--mol=co.mol', '--outcmo', '--get=PAMXVC']
90: 
90:  **** dirac-executable stderr console output : **** 
90: [n130:43209] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:43209] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: [n130:43210] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:43210] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: 
90: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_div_rot
90:    inputs: co.mol  &  response_1z_bz.inp
90: 
90: running test with input files ['visual_1z_bz.inp', 'co.mol'] and args --incmo --copy="PAMXVC"
90: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=visual_1z_bz.inp', '--mol=co.mol', '--incmo', '--copy=PAMXVC']
90: 
90:  **** dirac-executable stderr console output : **** 
90: [n130:43354] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:43328] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: 
88: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition
88:    inputs: molecule03.xyz  &  test_xyz.inp
88: 
88: running test with input files ['test_xyz.inp', 'molecule04.xyz'] and args None
88: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=test_xyz.inp', '--mol=molecule04.xyz']
88: 
88:  **** dirac-executable stderr console output : **** 
88: [n130:43469] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:43469] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: [n130:43468] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:43468] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: 
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:43354] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: [n130:43353] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:43353] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: 
90: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_div_rot
90:    inputs: co.mol  &  visual_1z_bz.inp
90: 
90: running test with input files ['visual_1z_bz_only_para.inp', 'co.mol'] and args --incmo --copy="PAMXVC"
90: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=visual_1z_bz_only_para.inp', '--mol=co.mol', '--incmo', '--copy=PAMXVC']
90: 
90:  **** dirac-executable stderr console output : **** 
90: [n130:43493] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:43493] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: [n130:43492] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: --------------------------------------------------------------------------
91: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=bed.inp', '--mol=Mg.xyz']
91: 
91:  **** dirac-executable stderr console output : **** 
91: [n130:43491] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
91: --------------------------------------------------------------------------
91: The application appears to have been direct launched using "srun",
91: but OMPI was not built with SLURM's PMI support and therefore cannot
91: execute. There are several options for building PMI support under
91: SLURM, depending upon the SLURM version you are using:
91: 
91:   version 16.05 or later: you can use SLURM's PMIx support. This
91:   requires that you configure and build SLURM --with-pmix.
91: 
91:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
91:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
91:   install PMI-2. You must then build Open MPI using --with-pmi pointing
91:   to the SLURM PMI library location.
91: 
91: Please configure as appropriate and try again.
91: --------------------------------------------------------------------------
91: *** An error occurred in MPI_Init_thread
91: *** on a NULL communicator
91: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
91: ***    and potentially your MPI job)
91: [n130:43491] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
91: [n130:43490] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
91: --------------------------------------------------------------------------
91: The application appears to have been direct launched using "srun",
91: but OMPI was not built with SLURM's PMI support and therefore cannot
91: execute. There are several options for building PMI support under
91: SLURM, depending upon the SLURM version you are using:
91: 
91:   version 16.05 or later: you can use SLURM's PMIx support. This
91:   requires that you configure and build SLURM --with-pmix.
91: 
91:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
91:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
91:   install PMI-2. You must then build Open MPI using --with-pmi pointing
91:   to the SLURM PMI library location.
91: 
91: Please configure as appropriate and try again.
91: --------------------------------------------------------------------------
91: *** An error occurred in MPI_Init_thread
91: *** on a NULL communicator
91: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
91: ***    and potentially your MPI job)
91: [n130:43490] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
91: 
91: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/bed_isotropic
91:    inputs: Mg.xyz  &  bed.inp
91: 
91: running test: bed Mg
 4/92 Test  #91: bed_isotropic ....................***Failed    4.52 sec
test 93
      Start  93: krci_properties_perm_dipmom

93: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/krci_properties_perm_dipmom/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/krci_properties_perm_dipmom" "--verbose"
93: Test timeout computed to be: 1500
92: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--outcmo', '--inp=H.inp', '--mol=H.xyz']
92: 
92:  **** dirac-executable stderr console output : **** 
92: [n130:43496] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
92: [n130:43497] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
92: --------------------------------------------------------------------------
92: The application appears to have been direct launched using "srun",
92: but OMPI was not built with SLURM's PMI support and therefore cannot
92: execute. There are several options for building PMI support under
92: SLURM, depending upon the SLURM version you are using:
92: 
92:   version 16.05 or later: you can use SLURM's PMIx support. This
92:   requires that you configure and build SLURM --with-pmix.
92: 
92:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
92:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
92:   install PMI-2. You must then build Open MPI using --with-pmi pointing
92:   to the SLURM PMI library location.
92: 
92: Please configure as appropriate and try again.
92: --------------------------------------------------------------------------
92: *** An error occurred in MPI_Init_thread
92: *** on a NULL communicator
92: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
92: ***    and potentially your MPI job)
92: [n130:43496] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
92: --------------------------------------------------------------------------
92: The application appears to have been direct launched using "srun",
92: but OMPI was not built with SLURM's PMI support and therefore cannot
92: execute. There are several options for building PMI support under
92: SLURM, depending upon the SLURM version you are using:
92: 
92:   version 16.05 or later: you can use SLURM's PMIx support. This
92:   requires that you configure and build SLURM --with-pmix.
92: 
92:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
92:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
92:   install PMI-2. You must then build Open MPI using --with-pmi pointing
92:   to the SLURM PMI library location.
92: 
92: Please configure as appropriate and try again.
92: --------------------------------------------------------------------------
92: *** An error occurred in MPI_Init_thread
92: *** on a NULL communicator
92: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
92: ***    and potentially your MPI job)
92: [n130:43497] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
92: 
92: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/localization
92:    inputs: H.xyz  &  H.inp
92: 
92: running test: H H
 5/92 Test  #92: localization .....................***Failed    4.54 sec
test 94
      Start  94: huckel_start

94: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/huckel_start/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/huckel_start" "--verbose"
94: Test timeout computed to be: 1500
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:43492] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: 
90: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_div_rot
90:    inputs: co.mol  &  visual_1z_bz_only_para.inp
90: 
90: running test with input files ['visual_divj.inp', 'co.mol'] and args --incmo --copy="PAMXVC"
90: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=visual_divj.inp', '--mol=co.mol', '--incmo', '--copy=PAMXVC']
90: 
90:  **** dirac-executable stderr console output : **** 
90: [n130:43636] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:43636] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: [n130:43635] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
93: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=h3.inp', '--mol=H3.mol']
93: 
93:  **** dirac-executable stderr console output : **** 
93: [n130:43638] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
93: --------------------------------------------------------------------------
93: The application appears to have been direct launched using "srun",
93: but OMPI was not built with SLURM's PMI support and therefore cannot
93: execute. There are several options for building PMI support under
93: SLURM, depending upon the SLURM version you are using:
93: 
93:   version 16.05 or later: you can use SLURM's PMIx support. This
93:   requires that you configure and build SLURM --with-pmix.
93: 
93:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
93:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
93:   install PMI-2. You must then build Open MPI using --with-pmi pointing
93:   to the SLURM PMI library location.
93: 
93: Please configure as appropriate and try again.
93: --------------------------------------------------------------------------
93: *** An error occurred in MPI_Init_thread
93: *** on a NULL communicator
93: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
93: ***    and potentially your MPI job)
93: [n130:43638] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
93: [n130:43637] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
93: --------------------------------------------------------------------------
93: The application appears to have been direct launched using "srun",
93: but OMPI was not built with SLURM's PMI support and therefore cannot
93: execute. There are several options for building PMI support under
93: SLURM, depending upon the SLURM version you are using:
93: 
93:   version 16.05 or later: you can use SLURM's PMIx support. This
93:   requires that you configure and build SLURM --with-pmix.
93: 
93:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
93:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
93:   install PMI-2. You must then build Open MPI using --with-pmi pointing
93:   to the SLURM PMI library location.
93: 
93: Please configure as appropriate and try again.
93: --------------------------------------------------------------------------
93: *** An error occurred in MPI_Init_thread
93: *** on a NULL communicator
93: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
93: ***    and potentially your MPI job)
93: [n130:43637] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
93: 
93: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/krci_properties_perm_dipmom
93:    inputs: H3.mol  &  h3.inp
93: 
93: running test: h3 H3
 6/92 Test  #93: krci_properties_perm_dipmom ......***Failed    4.47 sec
test 95
      Start  95: cosci_tmom

95: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/cosci_tmom/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/cosci_tmom" "--verbose"
95: Test timeout computed to be: 1500
94: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--outcmo', '--inp=H.inp', '--mol=H.xyz']
94: 
94:  **** dirac-executable stderr console output : **** 
94: [n130:43640] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
94: --------------------------------------------------------------------------
94: The application appears to have been direct launched using "srun",
94: but OMPI was not built with SLURM's PMI support and therefore cannot
94: execute. There are several options for building PMI support under
94: SLURM, depending upon the SLURM version you are using:
94: 
94:   version 16.05 or later: you can use SLURM's PMIx support. This
94:   requires that you configure and build SLURM --with-pmix.
94: 
94:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
94:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
94:   install PMI-2. You must then build Open MPI using --with-pmi pointing
94:   to the SLURM PMI library location.
94: 
94: Please configure as appropriate and try again.
94: --------------------------------------------------------------------------
94: *** An error occurred in MPI_Init_thread
94: *** on a NULL communicator
94: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
94: ***    and potentially your MPI job)
94: [n130:43640] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
94: [n130:43639] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
94: --------------------------------------------------------------------------
94: The application appears to have been direct launched using "srun",
94: but OMPI was not built with SLURM's PMI support and therefore cannot
94: execute. There are several options for building PMI support under
94: SLURM, depending upon the SLURM version you are using:
94: 
94:   version 16.05 or later: you can use SLURM's PMIx support. This
94:   requires that you configure and build SLURM --with-pmix.
94: 
94:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
94:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
94:   install PMI-2. You must then build Open MPI using --with-pmi pointing
94:   to the SLURM PMI library location.
94: 
94: Please configure as appropriate and try again.
94: --------------------------------------------------------------------------
94: *** An error occurred in MPI_Init_thread
94: *** on a NULL communicator
94: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
94: ***    and potentially your MPI job)
94: [n130:43639] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
94: 
94: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/huckel_start
94:    inputs: H.xyz  &  H.inp
94: 
94: running test: H H
 7/92 Test  #94: huckel_start .....................***Failed    4.51 sec
test 96
      Start  96: reladc_sipeigv

96: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/reladc_sipeigv/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/reladc_sipeigv" "--verbose"
96: Test timeout computed to be: 1500
88: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition
88:    inputs: molecule04.xyz  &  test_xyz.inp
88: 
88: running test with input files ['test_xyz.inp', 'molecule05.xyz'] and args None
88: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=test_xyz.inp', '--mol=molecule05.xyz']
88: 
88:  **** dirac-executable stderr console output : **** 
88: [n130:43621] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: [n130:43623] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:43623] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:43621] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: 
88: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition
88:    inputs: molecule05.xyz  &  test_xyz.inp
88: 
88: running test with input files ['test_xyz.inp', 'molecule06.xyz'] and args None
88: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=test_xyz.inp', '--mol=molecule06.xyz']
88: 
88:  **** dirac-executable stderr console output : **** 
88: [n130:43757] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
95: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--mw=120', '--inp=cosci.Li2.inp', '--mol=Li2.mol']
95: 
95:  **** dirac-executable stderr console output : **** 
95: [n130:43781] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
95: --------------------------------------------------------------------------
95: The application appears to have been direct launched using "srun",
95: but OMPI was not built with SLURM's PMI support and therefore cannot
95: execute. There are several options for building PMI support under
95: SLURM, depending upon the SLURM version you are using:
95: 
95:   version 16.05 or later: you can use SLURM's PMIx support. This
95:   requires that you configure and build SLURM --with-pmix.
95: 
95:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
95:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
95:   install PMI-2. You must then build Open MPI using --with-pmi pointing
95:   to the SLURM PMI library location.
95: 
95: Please configure as appropriate and try again.
95: --------------------------------------------------------------------------
95: *** An error occurred in MPI_Init_thread
95: *** on a NULL communicator
95: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
95: ***    and potentially your MPI job)
95: [n130:43781] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
95: [n130:43780] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
95: --------------------------------------------------------------------------
95: The application appears to have been direct launched using "srun",
95: but OMPI was not built with SLURM's PMI support and therefore cannot
95: execute. There are several options for building PMI support under
95: SLURM, depending upon the SLURM version you are using:
95: 
95:   version 16.05 or later: you can use SLURM's PMIx support. This
95:   requires that you configure and build SLURM --with-pmix.
95: 
95:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
95:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
95:   install PMI-2. You must then build Open MPI using --with-pmi pointing
95:   to the SLURM PMI library location.
95: 
95: Please configure as appropriate and try again.
95: --------------------------------------------------------------------------
95: *** An error occurred in MPI_Init_thread
95: *** on a NULL communicator
95: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
95: ***    and potentially your MPI job)
95: [n130:43780] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
95: 
95: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/cosci_tmom
95:    inputs: Li2.mol  &  cosci.Li2.inp
95: 
95: running test: cosci.Li2 Li2
 8/92 Test  #95: cosci_tmom .......................***Failed    4.26 sec
test 97
      Start  97: bed_anisotropic

97: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/bed_anisotropic/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/bed_anisotropic" "--verbose"
97: Test timeout computed to be: 1500
96: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=real_ne_eigv.inp', '--mol=ne_d2h.mol']
96: 
96:  **** dirac-executable stderr console output : **** 
96: [n130:43783] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
96: --------------------------------------------------------------------------
96: The application appears to have been direct launched using "srun",
96: but OMPI was not built with SLURM's PMI support and therefore cannot
96: execute. There are several options for building PMI support under
96: SLURM, depending upon the SLURM version you are using:
96: 
96:   version 16.05 or later: you can use SLURM's PMIx support. This
96:   requires that you configure and build SLURM --with-pmix.
96: 
96:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
96:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
96:   install PMI-2. You must then build Open MPI using --with-pmi pointing
96:   to the SLURM PMI library location.
96: 
96: Please configure as appropriate and try again.
96: --------------------------------------------------------------------------
96: *** An error occurred in MPI_Init_thread
96: *** on a NULL communicator
96: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
96: ***    and potentially your MPI job)
96: [n130:43783] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
96: [n130:43784] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
96: --------------------------------------------------------------------------
96: The application appears to have been direct launched using "srun",
96: but OMPI was not built with SLURM's PMI support and therefore cannot
96: execute. There are several options for building PMI support under
96: SLURM, depending upon the SLURM version you are using:
96: 
96:   version 16.05 or later: you can use SLURM's PMIx support. This
96:   requires that you configure and build SLURM --with-pmix.
96: 
96:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
96:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
96:   install PMI-2. You must then build Open MPI using --with-pmi pointing
96:   to the SLURM PMI library location.
96: 
96: Please configure as appropriate and try again.
96: --------------------------------------------------------------------------
96: *** An error occurred in MPI_Init_thread
96: *** on a NULL communicator
96: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
96: ***    and potentially your MPI job)
96: [n130:43784] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
96: 
96: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/reladc_sipeigv
96:    inputs: ne_d2h.mol  &  real_ne_eigv.inp
96: 
96: running test: real_ne_eigv ne_d2h
 9/92 Test  #96: reladc_sipeigv ...................***Failed    4.31 sec
test 98
      Start  98: response_lao_magnetiz_dft

98: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/response_lao_magnetiz_dft/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/response_lao_magnetiz_dft" "--verbose"
98: Test timeout computed to be: 1500
90: [n130:43635] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: 
90: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_div_rot
90:    inputs: co.mol  &  visual_divj.inp
90: 
90: running test with input files ['visual_divs.inp', 'co.mol'] and args --incmo --copy="PAMXVC"
90: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=visual_divs.inp', '--mol=co.mol', '--incmo', '--copy=PAMXVC']
90: 
90:  **** dirac-executable stderr console output : **** 
90: [n130:43759] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: [n130:43758] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:43759] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: [n130:43758] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: 
90: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_div_rot
90:    inputs: co.mol  &  visual_divs.inp
90: 
90: running test with input files ['visual_j.inp', 'co.mol'] and args --incmo --copy="PAMXVC"
90: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=visual_j.inp', '--mol=co.mol', '--incmo', '--copy=PAMXVC']
90: 
90:  **** dirac-executable stderr console output : **** 
90: [n130:43874] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:43757] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: [n130:43756] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:43756] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: 
88: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition
88:    inputs: molecule06.xyz  &  test_xyz.inp
88: 
88: running test with input files ['test_xyz.inp', 'molecule07.xyz'] and args None
88: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=test_xyz.inp', '--mol=molecule07.xyz']
88: 
88:  **** dirac-executable stderr console output : **** 
88: [n130:43897] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:43897] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: [n130:43896] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
97: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=AnisotropicKedge.inp', '--mol=H2O.xyz']
97: 
97:  **** dirac-executable stderr console output : **** 
97: [n130:43900] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
97: [n130:43901] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
97: --------------------------------------------------------------------------
97: The application appears to have been direct launched using "srun",
97: but OMPI was not built with SLURM's PMI support and therefore cannot
97: execute. There are several options for building PMI support under
97: SLURM, depending upon the SLURM version you are using:
97: 
97:   version 16.05 or later: you can use SLURM's PMIx support. This
97:   requires that you configure and build SLURM --with-pmix.
97: 
97:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
97:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
97:   install PMI-2. You must then build Open MPI using --with-pmi pointing
97:   to the SLURM PMI library location.
97: 
97: Please configure as appropriate and try again.
97: --------------------------------------------------------------------------
97: *** An error occurred in MPI_Init_thread
97: *** on a NULL communicator
97: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
97: ***    and potentially your MPI job)
97: [n130:43900] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
97: --------------------------------------------------------------------------
97: The application appears to have been direct launched using "srun",
97: but OMPI was not built with SLURM's PMI support and therefore cannot
97: execute. There are several options for building PMI support under
97: SLURM, depending upon the SLURM version you are using:
97: 
97:   version 16.05 or later: you can use SLURM's PMIx support. This
97:   requires that you configure and build SLURM --with-pmix.
97: 
97:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
97:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
97:   install PMI-2. You must then build Open MPI using --with-pmi pointing
97:   to the SLURM PMI library location.
97: 
97: Please configure as appropriate and try again.
97: --------------------------------------------------------------------------
97: *** An error occurred in MPI_Init_thread
97: *** on a NULL communicator
97: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
97: ***    and potentially your MPI job)
97: [n130:43901] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
97: 
97: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/bed_anisotropic
97:    inputs: H2O.xyz  &  AnisotropicKedge.inp
97: 
97: running test: AnisotropicKedge H2O
10/92 Test  #97: bed_anisotropic ..................***Failed    4.37 sec
test 99
      Start  99: dft_pp86

99: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/dft_pp86/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_pp86" "--verbose"
99: Test timeout computed to be: 1500
98: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--outcmo', '--inp=dc_pbe.inp', '--mol=hf.xyz']
98: 
98:  **** dirac-executable stderr console output : **** 
98: [n130:43925] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
98: --------------------------------------------------------------------------
98: The application appears to have been direct launched using "srun",
98: but OMPI was not built with SLURM's PMI support and therefore cannot
98: execute. There are several options for building PMI support under
98: SLURM, depending upon the SLURM version you are using:
98: 
98:   version 16.05 or later: you can use SLURM's PMIx support. This
98:   requires that you configure and build SLURM --with-pmix.
98: 
98:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
98:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
98:   install PMI-2. You must then build Open MPI using --with-pmi pointing
98:   to the SLURM PMI library location.
98: 
98: Please configure as appropriate and try again.
98: --------------------------------------------------------------------------
98: *** An error occurred in MPI_Init_thread
98: *** on a NULL communicator
98: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
98: ***    and potentially your MPI job)
98: [n130:43925] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
98: [n130:43926] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
98: --------------------------------------------------------------------------
98: The application appears to have been direct launched using "srun",
98: but OMPI was not built with SLURM's PMI support and therefore cannot
98: execute. There are several options for building PMI support under
98: SLURM, depending upon the SLURM version you are using:
98: 
98:   version 16.05 or later: you can use SLURM's PMIx support. This
98:   requires that you configure and build SLURM --with-pmix.
98: 
98:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
98:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
98:   install PMI-2. You must then build Open MPI using --with-pmi pointing
98:   to the SLURM PMI library location.
98: 
98: Please configure as appropriate and try again.
98: --------------------------------------------------------------------------
98: *** An error occurred in MPI_Init_thread
98: *** on a NULL communicator
98: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
98: ***    and potentially your MPI job)
98: [n130:43926] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
98: 
98: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/response_lao_magnetiz_dft
98:    inputs: hf.xyz  &  dc_pbe.inp
98: 
98: running test: dc_pbe hf
11/92 Test  #98: response_lao_magnetiz_dft ........***Failed    4.37 sec
test 100
      Start 100: bss_energy

100: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/bss_energy/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/bss_energy" "--verbose"
100: Test timeout computed to be: 1500
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:43874] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: [n130:43875] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:43875] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: 
90: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_div_rot
90:    inputs: co.mol  &  visual_j.inp
90: 
90: running test with input files ['visual_jdia.inp', 'co.mol'] and args --incmo --copy="PAMXVC"
90: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=visual_jdia.inp', '--mol=co.mol', '--incmo', '--copy=PAMXVC']
90: 
90:  **** dirac-executable stderr console output : **** 
90: [n130:43998] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: [n130:43997] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:43896] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: 
88: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition
88:    inputs: molecule07.xyz  &  test_xyz.inp
88: 
88: running test with input files ['test_xyz.inp', 'molecule08.xyz'] and args None
88: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=test_xyz.inp', '--mol=molecule08.xyz']
88: 
88:  **** dirac-executable stderr console output : **** 
88: [n130:44017] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:44017] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: [n130:44018] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
100: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--mw=80', '--inp=HF.bss.cc.inp', '--mol=Z80H.lsym.dir.mol']
100: 
100:  **** dirac-executable stderr console output : **** 
100: [n130:44066] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
100: --------------------------------------------------------------------------
100: The application appears to have been direct launched using "srun",
100: but OMPI was not built with SLURM's PMI support and therefore cannot
100: execute. There are several options for building PMI support under
100: SLURM, depending upon the SLURM version you are using:
100: 
100:   version 16.05 or later: you can use SLURM's PMIx support. This
100:   requires that you configure and build SLURM --with-pmix.
100: 
100:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
100:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
100:   install PMI-2. You must then build Open MPI using --with-pmi pointing
100:   to the SLURM PMI library location.
100: 
100: Please configure as appropriate and try again.
100: --------------------------------------------------------------------------
100: *** An error occurred in MPI_Init_thread
100: *** on a NULL communicator
100: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
100: ***    and potentially your MPI job)
100: [n130:44066] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
100: [n130:44065] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
100: --------------------------------------------------------------------------
100: The application appears to have been direct launched using "srun",
100: but OMPI was not built with SLURM's PMI support and therefore cannot
100: execute. There are several options for building PMI support under
100: SLURM, depending upon the SLURM version you are using:
100: 
100:   version 16.05 or later: you can use SLURM's PMIx support. This
100:   requires that you configure and build SLURM --with-pmix.
100: 
100:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
100:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
100:   install PMI-2. You must then build Open MPI using --with-pmi pointing
100:   to the SLURM PMI library location.
100: 
100: Please configure as appropriate and try again.
100: --------------------------------------------------------------------------
100: *** An error occurred in MPI_Init_thread
100: *** on a NULL communicator
100: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
100: ***    and potentially your MPI job)
100: [n130:44065] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
100: 
100: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/bss_energy
100:    inputs: Z80H.lsym.dir.mol  &  HF.bss.cc.inp
100: 
100: running test: HF.bss.cc Z80H.lsym.dir
12/92 Test #100: bss_energy .......................***Failed    4.71 sec
test 101
      Start 101: acmoin

101: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/acmoin/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/acmoin" "--verbose"
101: Test timeout computed to be: 1500
90: [n130:43998] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:43997] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: 
90: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_div_rot
90:    inputs: co.mol  &  visual_jdia.inp
90: 
90: running test with input files ['visual_rotj.inp', 'co.mol'] and args --incmo --copy="PAMXVC"
90: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=visual_rotj.inp', '--mol=co.mol', '--incmo', '--copy=PAMXVC']
90: 
90:  **** dirac-executable stderr console output : **** 
90: [n130:44128] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:44128] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: [n130:44127] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
99: 
99: running test with input files ['pp86.inp', 'ne.mol'] and args None
99: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=pp86.inp', '--mol=ne.mol']
99: 
99:  **** dirac-executable stderr console output : **** 
99: [n130:44068] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
99: --------------------------------------------------------------------------
99: The application appears to have been direct launched using "srun",
99: but OMPI was not built with SLURM's PMI support and therefore cannot
99: execute. There are several options for building PMI support under
99: SLURM, depending upon the SLURM version you are using:
99: 
99:   version 16.05 or later: you can use SLURM's PMIx support. This
99:   requires that you configure and build SLURM --with-pmix.
99: 
99:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
99:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
99:   install PMI-2. You must then build Open MPI using --with-pmi pointing
99:   to the SLURM PMI library location.
99: 
99: Please configure as appropriate and try again.
99: --------------------------------------------------------------------------
99: *** An error occurred in MPI_Init_thread
99: *** on a NULL communicator
99: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
99: ***    and potentially your MPI job)
99: [n130:44068] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
99: [n130:44067] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
99: --------------------------------------------------------------------------
99: The application appears to have been direct launched using "srun",
99: but OMPI was not built with SLURM's PMI support and therefore cannot
99: execute. There are several options for building PMI support under
99: SLURM, depending upon the SLURM version you are using:
99: 
99:   version 16.05 or later: you can use SLURM's PMIx support. This
99:   requires that you configure and build SLURM --with-pmix.
99: 
99:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
99:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
99:   install PMI-2. You must then build Open MPI using --with-pmi pointing
99:   to the SLURM PMI library location.
99: 
99: Please configure as appropriate and try again.
99: --------------------------------------------------------------------------
99: *** An error occurred in MPI_Init_thread
99: *** on a NULL communicator
99: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
99: ***    and potentially your MPI job)
99: [n130:44067] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
99: 
99: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_pp86
99:    inputs: ne.mol  &  pp86.inp
99: 
99: running test with input files ['pp86_ggakey.inp', 'ne.mol'] and args None
99: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=pp86_ggakey.inp', '--mol=ne.mol']
99: 
99:  **** dirac-executable stderr console output : **** 
99: [n130:44183] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
99: --------------------------------------------------------------------------
99: The application appears to have been direct launched using "srun",
99: but OMPI was not built with SLURM's PMI support and therefore cannot
99: execute. There are several options for building PMI support under
99: SLURM, depending upon the SLURM version you are using:
99: 
99:   version 16.05 or later: you can use SLURM's PMIx support. This
99:   requires that you configure and build SLURM --with-pmix.
99: 
99:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
101: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--outcmo', '--inp=H.inp', '--mol=H.mol']
101: 
101:  **** dirac-executable stderr console output : **** 
101: [n130:44215] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
101: --------------------------------------------------------------------------
101: The application appears to have been direct launched using "srun",
101: but OMPI was not built with SLURM's PMI support and therefore cannot
101: execute. There are several options for building PMI support under
101: SLURM, depending upon the SLURM version you are using:
101: 
101:   version 16.05 or later: you can use SLURM's PMIx support. This
101:   requires that you configure and build SLURM --with-pmix.
101: 
101:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
101:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
101:   install PMI-2. You must then build Open MPI using --with-pmi pointing
101:   to the SLURM PMI library location.
101: 
101: Please configure as appropriate and try again.
101: --------------------------------------------------------------------------
101: *** An error occurred in MPI_Init_thread
101: *** on a NULL communicator
101: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
101: ***    and potentially your MPI job)
101: [n130:44215] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
101: [n130:44216] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
101: --------------------------------------------------------------------------
101: The application appears to have been direct launched using "srun",
101: but OMPI was not built with SLURM's PMI support and therefore cannot
101: execute. There are several options for building PMI support under
101: SLURM, depending upon the SLURM version you are using:
101: 
101:   version 16.05 or later: you can use SLURM's PMIx support. This
101:   requires that you configure and build SLURM --with-pmix.
101: 
101:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
101:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
101:   install PMI-2. You must then build Open MPI using --with-pmi pointing
101:   to the SLURM PMI library location.
101: 
101: Please configure as appropriate and try again.
101: --------------------------------------------------------------------------
101: *** An error occurred in MPI_Init_thread
101: *** on a NULL communicator
101: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
101: ***    and potentially your MPI job)
101: [n130:44216] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
101: 
101: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/acmoin
101:    inputs: H.mol  &  H.inp
101: 
101: running test: H H
13/92 Test #101: acmoin ...........................***Failed    4.47 sec
test 102
      Start 102: dft_cam

102: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/dft_cam/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_cam" "--verbose"
102: Test timeout computed to be: 1500
88: [n130:44018] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: 
88: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition
88:    inputs: molecule08.xyz  &  test_xyz.inp
88: 
88: running test with input files ['test_xyz.inp', 'molecule09.xyz'] and args None
88: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=test_xyz.inp', '--mol=molecule09.xyz']
88: 
88:  **** dirac-executable stderr console output : **** 
88: [n130:44156] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:44156] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: [n130:44157] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:44157] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: 
88: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition
88:    inputs: molecule09.xyz  &  test_xyz.inp
88: 
88: running test with input files ['test_xyz.inp', 'molecule10.xyz'] and args None
88: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=test_xyz.inp', '--mol=molecule10.xyz']
88: 
88:  **** dirac-executable stderr console output : **** 
88: [n130:44297] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
99:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
99:   install PMI-2. You must then build Open MPI using --with-pmi pointing
99:   to the SLURM PMI library location.
99: 
99: Please configure as appropriate and try again.
99: --------------------------------------------------------------------------
99: *** An error occurred in MPI_Init_thread
99: *** on a NULL communicator
99: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
99: ***    and potentially your MPI job)
99: [n130:44183] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
99: [n130:44184] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
99: --------------------------------------------------------------------------
99: The application appears to have been direct launched using "srun",
99: but OMPI was not built with SLURM's PMI support and therefore cannot
99: execute. There are several options for building PMI support under
99: SLURM, depending upon the SLURM version you are using:
99: 
99:   version 16.05 or later: you can use SLURM's PMIx support. This
99:   requires that you configure and build SLURM --with-pmix.
99: 
99:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
99:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
99:   install PMI-2. You must then build Open MPI using --with-pmi pointing
99:   to the SLURM PMI library location.
99: 
99: Please configure as appropriate and try again.
99: --------------------------------------------------------------------------
99: *** An error occurred in MPI_Init_thread
99: *** on a NULL communicator
99: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
99: ***    and potentially your MPI job)
99: [n130:44184] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
99: 
99: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_pp86
99:    inputs: ne.mol  &  pp86_ggakey.inp
99: 
99: running test with input files ['pp86_xcfun.inp', 'ne.mol'] and args None
99: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=pp86_xcfun.inp', '--mol=ne.mol']
99: 
99:  **** dirac-executable stderr console output : **** 
99: [n130:44334] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
99: [n130:44335] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
99: --------------------------------------------------------------------------
99: The application appears to have been direct launched using "srun",
99: but OMPI was not built with SLURM's PMI support and therefore cannot
99: execute. There are several options for building PMI support under
99: SLURM, depending upon the SLURM version you are using:
99: 
99:   version 16.05 or later: you can use SLURM's PMIx support. This
99:   requires that you configure and build SLURM --with-pmix.
99: 
99:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
99:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
99:   install PMI-2. You must then build Open MPI using --with-pmi pointing
99:   to the SLURM PMI library location.
99: 
99: Please configure as appropriate and try again.
99: --------------------------------------------------------------------------
99: *** An error occurred in MPI_Init_thread
99: *** on a NULL communicator
99: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
99: ***    and potentially your MPI job)
99: [n130:44334] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
99: --------------------------------------------------------------------------
99: The application appears to have been direct launched using "srun",
99: but OMPI was not built with SLURM's PMI support and therefore cannot
99: execute. There are several options for building PMI support under
99: SLURM, depending upon the SLURM version you are using:
99: 
102: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=camb3lyp.inp', '--mol=ne.mol']
102: 
102:  **** dirac-executable stderr console output : **** 
102: [n130:44366] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
102: --------------------------------------------------------------------------
102: The application appears to have been direct launched using "srun",
102: but OMPI was not built with SLURM's PMI support and therefore cannot
102: execute. There are several options for building PMI support under
102: SLURM, depending upon the SLURM version you are using:
102: 
102:   version 16.05 or later: you can use SLURM's PMIx support. This
102:   requires that you configure and build SLURM --with-pmix.
102: 
102:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
102:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
102:   install PMI-2. You must then build Open MPI using --with-pmi pointing
102:   to the SLURM PMI library location.
102: 
102: Please configure as appropriate and try again.
102: --------------------------------------------------------------------------
102: *** An error occurred in MPI_Init_thread
102: *** on a NULL communicator
102: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
102: ***    and potentially your MPI job)
102: [n130:44366] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
102: [n130:44365] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
102: --------------------------------------------------------------------------
102: The application appears to have been direct launched using "srun",
102: but OMPI was not built with SLURM's PMI support and therefore cannot
102: execute. There are several options for building PMI support under
102: SLURM, depending upon the SLURM version you are using:
102: 
102:   version 16.05 or later: you can use SLURM's PMIx support. This
102:   requires that you configure and build SLURM --with-pmix.
102: 
102:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
102:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
102:   install PMI-2. You must then build Open MPI using --with-pmi pointing
102:   to the SLURM PMI library location.
102: 
102: Please configure as appropriate and try again.
102: --------------------------------------------------------------------------
102: *** An error occurred in MPI_Init_thread
102: *** on a NULL communicator
102: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
102: ***    and potentially your MPI job)
102: [n130:44365] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
102: 
102: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_cam
102:    inputs: ne.mol  &  camb3lyp.inp
102: 
102: running test: camb3lyp ne
14/92 Test #102: dft_cam ..........................***Failed    4.36 sec
test 103
      Start 103: visual_frac_occupation

103: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/visual_frac_occupation/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_frac_occupation" "--verbose"
103: Test timeout computed to be: 1500
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:44127] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: 
90: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_div_rot
90:    inputs: co.mol  &  visual_rotj.inp
90: 
90: running test with input files ['visual_jpara.inp', 'co.mol'] and args --incmo --copy="PAMXVC"
90: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=visual_jpara.inp', '--mol=co.mol', '--incmo', '--copy=PAMXVC']
90: 
90:  **** dirac-executable stderr console output : **** 
90: [n130:44260] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:44260] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: [n130:44259] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:44259] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: 
90: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_div_rot
90:    inputs: co.mol  &  visual_jpara.inp
90: 
90: running test with input files ['visual_s.inp', 'co.mol'] and args --incmo --copy="PAMXVC"
90: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=visual_s.inp', '--mol=co.mol', '--incmo', '--copy=PAMXVC']
90: 
90:  **** dirac-executable stderr console output : **** 
90: [n130:44393] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:44297] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: [n130:44296] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:44296] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: 
88: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition
88:    inputs: molecule10.xyz  &  test_xyz.inp
88: 
88: running test with input files ['test_xyz.inp', 'molecule11.xyz'] and args None
88: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=test_xyz.inp', '--mol=molecule11.xyz']
88: 
88:  **** dirac-executable stderr console output : **** 
88: [n130:44436] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:44436] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: [n130:44435] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
99:   version 16.05 or later: you can use SLURM's PMIx support. This
99:   requires that you configure and build SLURM --with-pmix.
99: 
99:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
99:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
99:   install PMI-2. You must then build Open MPI using --with-pmi pointing
99:   to the SLURM PMI library location.
99: 
99: Please configure as appropriate and try again.
99: --------------------------------------------------------------------------
99: *** An error occurred in MPI_Init_thread
99: *** on a NULL communicator
99: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
99: ***    and potentially your MPI job)
99: [n130:44335] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
99: 
99: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_pp86
99:    inputs: ne.mol  &  pp86_xcfun.inp
99: 
99: running test with input files ['pw86x.inp', 'ne.mol'] and args None
99: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=pw86x.inp', '--mol=ne.mol']
99: 
99:  **** dirac-executable stderr console output : **** 
99: [n130:44475] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
99: --------------------------------------------------------------------------
99: The application appears to have been direct launched using "srun",
99: but OMPI was not built with SLURM's PMI support and therefore cannot
99: execute. There are several options for building PMI support under
99: SLURM, depending upon the SLURM version you are using:
99: 
99:   version 16.05 or later: you can use SLURM's PMIx support. This
99:   requires that you configure and build SLURM --with-pmix.
99: 
99:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
99:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
99:   install PMI-2. You must then build Open MPI using --with-pmi pointing
99:   to the SLURM PMI library location.
99: 
99: Please configure as appropriate and try again.
99: --------------------------------------------------------------------------
99: *** An error occurred in MPI_Init_thread
99: *** on a NULL communicator
99: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
99: ***    and potentially your MPI job)
99: [n130:44475] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
99: [n130:44476] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
99: --------------------------------------------------------------------------
99: The application appears to have been direct launched using "srun",
99: but OMPI was not built with SLURM's PMI support and therefore cannot
99: execute. There are several options for building PMI support under
99: SLURM, depending upon the SLURM version you are using:
99: 
99:   version 16.05 or later: you can use SLURM's PMIx support. This
99:   requires that you configure and build SLURM --with-pmix.
99: 
99:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
99:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
99:   install PMI-2. You must then build Open MPI using --with-pmi pointing
99:   to the SLURM PMI library location.
99: 
99: Please configure as appropriate and try again.
99: --------------------------------------------------------------------------
99: *** An error occurred in MPI_Init_thread
99: *** on a NULL communicator
99: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
99: ***    and potentially your MPI job)
99: [n130:44476] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
99: 
99: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_pp86
99:    inputs: ne.mol  &  pw86x.inp
99: 
103: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=density_1s.inp', '--mol=he.mol']
103: 
103:  **** dirac-executable stderr console output : **** 
103: [n130:44535] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
103: --------------------------------------------------------------------------
103: The application appears to have been direct launched using "srun",
103: but OMPI was not built with SLURM's PMI support and therefore cannot
103: execute. There are several options for building PMI support under
103: SLURM, depending upon the SLURM version you are using:
103: 
103:   version 16.05 or later: you can use SLURM's PMIx support. This
103:   requires that you configure and build SLURM --with-pmix.
103: 
103:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
103:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
103:   install PMI-2. You must then build Open MPI using --with-pmi pointing
103:   to the SLURM PMI library location.
103: 
103: Please configure as appropriate and try again.
103: --------------------------------------------------------------------------
103: *** An error occurred in MPI_Init_thread
103: *** on a NULL communicator
103: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
103: ***    and potentially your MPI job)
103: [n130:44535] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
103: [n130:44536] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
103: --------------------------------------------------------------------------
103: The application appears to have been direct launched using "srun",
103: but OMPI was not built with SLURM's PMI support and therefore cannot
103: execute. There are several options for building PMI support under
103: SLURM, depending upon the SLURM version you are using:
103: 
103:   version 16.05 or later: you can use SLURM's PMIx support. This
103:   requires that you configure and build SLURM --with-pmix.
103: 
103:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
103:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
103:   install PMI-2. You must then build Open MPI using --with-pmi pointing
103:   to the SLURM PMI library location.
103: 
103: Please configure as appropriate and try again.
103: --------------------------------------------------------------------------
103: *** An error occurred in MPI_Init_thread
103: *** on a NULL communicator
103: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
103: ***    and potentially your MPI job)
103: [n130:44536] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
103: 
103: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_frac_occupation
103:    inputs: he.mol  &  density_1s.inp
103: 
103: running test: density_1s he
15/92 Test #103: visual_frac_occupation ...........***Failed    4.76 sec
test 104
      Start 104: eomee_fc_cvs

104: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/eomee_fc_cvs/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/eomee_fc_cvs" "--verbose"
104: Test timeout computed to be: 1500
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:44393] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: [n130:44392] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:44392] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: 
90: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_div_rot
90:    inputs: co.mol  &  visual_s.inp
90: 
90: running test with input files ['visual_rots.inp', 'co.mol'] and args --incmo --copy="PAMXVC"
90: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=visual_rots.inp', '--mol=co.mol', '--incmo', '--copy=PAMXVC']
90: 
90:  **** dirac-executable stderr console output : **** 
90: [n130:44533] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: [n130:44532] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:44533] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: --------------------------------------------------------------------------
90: The application appears to have been direct launched using "srun",
90: but OMPI was not built with SLURM's PMI support and therefore cannot
90: execute. There are several options for building PMI support under
90: SLURM, depending upon the SLURM version you are using:
90: 
90:   version 16.05 or later: you can use SLURM's PMIx support. This
90:   requires that you configure and build SLURM --with-pmix.
90: 
90:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
90:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
90:   install PMI-2. You must then build Open MPI using --with-pmi pointing
90:   to the SLURM PMI library location.
90: 
90: Please configure as appropriate and try again.
90: --------------------------------------------------------------------------
90: *** An error occurred in MPI_Init_thread
90: *** on a NULL communicator
90: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
90: ***    and potentially your MPI job)
90: [n130:44532] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
90: 
90: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_div_rot
90:    inputs: co.mol  &  visual_rots.inp
16/92 Test  #90: visual_div_rot ...................***Failed   44.08 sec
test 105
      Start 105: response_lao_shield

105: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/response_lao_shield/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/response_lao_shield" "--verbose"
105: Test timeout computed to be: 1500
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:44435] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: 
88: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition
88:    inputs: molecule11.xyz  &  test_xyz.inp
88: 
88: running test with input files ['test_xyz.inp', 'molecule12.xyz'] and args None
88: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=test_xyz.inp', '--mol=molecule12.xyz']
88: 
88:  **** dirac-executable stderr console output : **** 
88: [n130:44570] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:44570] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: [n130:44571] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
104: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--mw=110', '--inp=fc_cvs_eom_ee_sym.inp', '--mol=h2o.xyz']
104: 
104:  **** dirac-executable stderr console output : **** 
104: [n130:44662] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
104: --------------------------------------------------------------------------
104: The application appears to have been direct launched using "srun",
104: but OMPI was not built with SLURM's PMI support and therefore cannot
104: execute. There are several options for building PMI support under
104: SLURM, depending upon the SLURM version you are using:
104: 
104:   version 16.05 or later: you can use SLURM's PMIx support. This
104:   requires that you configure and build SLURM --with-pmix.
104: 
104:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
104:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
104:   install PMI-2. You must then build Open MPI using --with-pmi pointing
104:   to the SLURM PMI library location.
104: 
104: Please configure as appropriate and try again.
104: --------------------------------------------------------------------------
104: *** An error occurred in MPI_Init_thread
104: *** on a NULL communicator
104: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
104: ***    and potentially your MPI job)
104: [n130:44662] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
104: [n130:44661] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
104: --------------------------------------------------------------------------
104: The application appears to have been direct launched using "srun",
104: but OMPI was not built with SLURM's PMI support and therefore cannot
104: execute. There are several options for building PMI support under
104: SLURM, depending upon the SLURM version you are using:
104: 
104:   version 16.05 or later: you can use SLURM's PMIx support. This
104:   requires that you configure and build SLURM --with-pmix.
104: 
104:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
104:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
104:   install PMI-2. You must then build Open MPI using --with-pmi pointing
104:   to the SLURM PMI library location.
104: 
104: Please configure as appropriate and try again.
104: --------------------------------------------------------------------------
104: *** An error occurred in MPI_Init_thread
104: *** on a NULL communicator
104: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
104: ***    and potentially your MPI job)
104: [n130:44661] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
104: 
104: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/eomee_fc_cvs
104:    inputs: h2o.xyz  &  fc_cvs_eom_ee_sym.inp
104: 
104: running test: fc_cvs_eom_ee_sym h2o
17/92 Test #104: eomee_fc_cvs .....................***Failed    3.88 sec
test 106
      Start 106: response_lao_magnetiz

106: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/response_lao_magnetiz/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/response_lao_magnetiz" "--verbose"
106: Test timeout computed to be: 1500
105: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=LiH.levyle.shield_lao_go=cm.inp', '--mol=LiH.lsym.dir.mol']
105: 
105:  **** dirac-executable stderr console output : **** 
105: [n130:44689] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
105: [n130:44690] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
105: --------------------------------------------------------------------------
105: The application appears to have been direct launched using "srun",
105: but OMPI was not built with SLURM's PMI support and therefore cannot
105: execute. There are several options for building PMI support under
105: SLURM, depending upon the SLURM version you are using:
105: 
105:   version 16.05 or later: you can use SLURM's PMIx support. This
105:   requires that you configure and build SLURM --with-pmix.
105: 
105:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
105:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
105:   install PMI-2. You must then build Open MPI using --with-pmi pointing
105:   to the SLURM PMI library location.
105: 
105: Please configure as appropriate and try again.
105: --------------------------------------------------------------------------
105: --------------------------------------------------------------------------
105: The application appears to have been direct launched using "srun",
105: but OMPI was not built with SLURM's PMI support and therefore cannot
105: execute. There are several options for building PMI support under
105: SLURM, depending upon the SLURM version you are using:
105: 
105:   version 16.05 or later: you can use SLURM's PMIx support. This
105:   requires that you configure and build SLURM --with-pmix.
105: 
105:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
105:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
105:   install PMI-2. You must then build Open MPI using --with-pmi pointing
105:   to the SLURM PMI library location.
105: 
105: Please configure as appropriate and try again.
105: --------------------------------------------------------------------------
105: *** An error occurred in MPI_Init_thread
105: *** on a NULL communicator
105: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
105: ***    and potentially your MPI job)
105: [n130:44690] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
105: *** An error occurred in MPI_Init_thread
105: *** on a NULL communicator
105: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
105: ***    and potentially your MPI job)
105: [n130:44689] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
105: 
105: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/response_lao_shield
105:    inputs: LiH.lsym.dir.mol  &  LiH.levyle.shield_lao_go=cm.inp
105: 
105: running test: LiH.levyle.shield_lao_go=cm LiH.lsym.dir
18/92 Test #105: response_lao_shield ..............***Failed    4.05 sec
test 107
      Start 107: krci_properties_omega_tdm

107: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/krci_properties_omega_tdm/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/krci_properties_omega_tdm" "--verbose"
107: Test timeout computed to be: 1500
99: running test with input files ['pw86x_xcfun.inp', 'ne.mol'] and args None
99: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=pw86x_xcfun.inp', '--mol=ne.mol']
99: 
99:  **** dirac-executable stderr console output : **** 
99: [n130:44613] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
99: --------------------------------------------------------------------------
99: The application appears to have been direct launched using "srun",
99: but OMPI was not built with SLURM's PMI support and therefore cannot
99: execute. There are several options for building PMI support under
99: SLURM, depending upon the SLURM version you are using:
99: 
99:   version 16.05 or later: you can use SLURM's PMIx support. This
99:   requires that you configure and build SLURM --with-pmix.
99: 
99:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
99:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
99:   install PMI-2. You must then build Open MPI using --with-pmi pointing
99:   to the SLURM PMI library location.
99: 
99: Please configure as appropriate and try again.
99: --------------------------------------------------------------------------
99: *** An error occurred in MPI_Init_thread
99: *** on a NULL communicator
99: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
99: ***    and potentially your MPI job)
99: [n130:44613] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
99: [n130:44612] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
99: --------------------------------------------------------------------------
99: The application appears to have been direct launched using "srun",
99: but OMPI was not built with SLURM's PMI support and therefore cannot
99: execute. There are several options for building PMI support under
99: SLURM, depending upon the SLURM version you are using:
99: 
99:   version 16.05 or later: you can use SLURM's PMIx support. This
99:   requires that you configure and build SLURM --with-pmix.
99: 
99:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
99:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
99:   install PMI-2. You must then build Open MPI using --with-pmi pointing
99:   to the SLURM PMI library location.
99: 
99: Please configure as appropriate and try again.
99: --------------------------------------------------------------------------
99: *** An error occurred in MPI_Init_thread
99: *** on a NULL communicator
99: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
99: ***    and potentially your MPI job)
99: [n130:44612] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
99: 
99: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_pp86
99:    inputs: ne.mol  &  pw86x_xcfun.inp
99: 
99: running test with input files ['p86c.inp', 'ne.mol'] and args None
99: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=p86c.inp', '--mol=ne.mol']
99: 
99:  **** dirac-executable stderr console output : **** 
99: [n130:44750] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
99: --------------------------------------------------------------------------
99: The application appears to have been direct launched using "srun",
99: but OMPI was not built with SLURM's PMI support and therefore cannot
99: execute. There are several options for building PMI support under
99: SLURM, depending upon the SLURM version you are using:
99: 
99:   version 16.05 or later: you can use SLURM's PMIx support. This
99:   requires that you configure and build SLURM --with-pmix.
99: 
99:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
99:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
106: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=LiH.dc_ukb.linresp_ep_magnetiz_lao-go_cm.inp', '--mol=LiH.lsym.mol']
106: 
106:  **** dirac-executable stderr console output : **** 
106: [n130:44812] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
106: [n130:44810] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
106: --------------------------------------------------------------------------
106: The application appears to have been direct launched using "srun",
106: but OMPI was not built with SLURM's PMI support and therefore cannot
106: execute. There are several options for building PMI support under
106: SLURM, depending upon the SLURM version you are using:
106: 
106:   version 16.05 or later: you can use SLURM's PMIx support. This
106:   requires that you configure and build SLURM --with-pmix.
106: 
106:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
106:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
106:   install PMI-2. You must then build Open MPI using --with-pmi pointing
106:   to the SLURM PMI library location.
106: 
106: Please configure as appropriate and try again.
106: --------------------------------------------------------------------------
106: *** An error occurred in MPI_Init_thread
106: *** on a NULL communicator
106: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
106: ***    and potentially your MPI job)
106: [n130:44812] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
106: --------------------------------------------------------------------------
106: The application appears to have been direct launched using "srun",
106: but OMPI was not built with SLURM's PMI support and therefore cannot
106: execute. There are several options for building PMI support under
106: SLURM, depending upon the SLURM version you are using:
106: 
106:   version 16.05 or later: you can use SLURM's PMIx support. This
106:   requires that you configure and build SLURM --with-pmix.
106: 
106:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
106:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
106:   install PMI-2. You must then build Open MPI using --with-pmi pointing
106:   to the SLURM PMI library location.
106: 
106: Please configure as appropriate and try again.
106: --------------------------------------------------------------------------
106: *** An error occurred in MPI_Init_thread
106: *** on a NULL communicator
106: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
106: ***    and potentially your MPI job)
106: [n130:44810] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
106: 
106: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/response_lao_magnetiz
106:    inputs: LiH.lsym.mol  &  LiH.dc_ukb.linresp_ep_magnetiz_lao-go_cm.inp
106: 
106: running test: LiH.dc_ukb.linresp_ep_magnetiz_lao-go_cm LiH.lsym
19/92 Test #106: response_lao_magnetiz ............***Failed    4.51 sec
test 108
      Start 108: response_hf_polarizability

108: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/response_hf_polarizability/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/response_hf_polarizability" "--verbose"
108: Test timeout computed to be: 1500
107: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=h2.inp', '--mol=H2.mol']
107: 
107:  **** dirac-executable stderr console output : **** 
107: [n130:44832] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
107: --------------------------------------------------------------------------
107: The application appears to have been direct launched using "srun",
107: but OMPI was not built with SLURM's PMI support and therefore cannot
107: execute. There are several options for building PMI support under
107: SLURM, depending upon the SLURM version you are using:
107: 
107:   version 16.05 or later: you can use SLURM's PMIx support. This
107:   requires that you configure and build SLURM --with-pmix.
107: 
107:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
107:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
107:   install PMI-2. You must then build Open MPI using --with-pmi pointing
107:   to the SLURM PMI library location.
107: 
107: Please configure as appropriate and try again.
107: --------------------------------------------------------------------------
107: *** An error occurred in MPI_Init_thread
107: *** on a NULL communicator
107: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
107: ***    and potentially your MPI job)
107: [n130:44832] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
107: [n130:44831] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
107: --------------------------------------------------------------------------
107: The application appears to have been direct launched using "srun",
107: but OMPI was not built with SLURM's PMI support and therefore cannot
107: execute. There are several options for building PMI support under
107: SLURM, depending upon the SLURM version you are using:
107: 
107:   version 16.05 or later: you can use SLURM's PMIx support. This
107:   requires that you configure and build SLURM --with-pmix.
107: 
107:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
107:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
107:   install PMI-2. You must then build Open MPI using --with-pmi pointing
107:   to the SLURM PMI library location.
107: 
107: Please configure as appropriate and try again.
107: --------------------------------------------------------------------------
107: *** An error occurred in MPI_Init_thread
107: *** on a NULL communicator
107: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
107: ***    and potentially your MPI job)
107: [n130:44831] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
107: 
107: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/krci_properties_omega_tdm
107:    inputs: H2.mol  &  h2.inp
107: 
107: running test: h2 H2
20/92 Test #107: krci_properties_omega_tdm ........***Failed    4.90 sec
test 109
      Start 109: count_cc_memory

109: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/count_cc_memory/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/count_cc_memory" "--verbose"
109: Test timeout computed to be: 1500
88: [n130:44571] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: 
88: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition
88:    inputs: molecule12.xyz  &  test_xyz.inp
88: 
88: running test with input files ['test_xyz.inp', 'molecule13.xyz'] and args None
88: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=test_xyz.inp', '--mol=molecule13.xyz']
88: 
88:  **** dirac-executable stderr console output : **** 
88: [n130:44718] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: [n130:44717] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:44718] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:44717] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: 
88: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition
88:    inputs: molecule13.xyz  &  test_xyz.inp
88: 
88: running test with input files ['test_xyz.inp', 'molecule14.xyz'] and args None
88: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=test_xyz.inp', '--mol=molecule14.xyz']
88: 
88:  **** dirac-executable stderr console output : **** 
88: [n130:44860] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
99:   install PMI-2. You must then build Open MPI using --with-pmi pointing
99:   to the SLURM PMI library location.
99: 
99: Please configure as appropriate and try again.
99: --------------------------------------------------------------------------
99: *** An error occurred in MPI_Init_thread
99: *** on a NULL communicator
99: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
99: ***    and potentially your MPI job)
99: [n130:44750] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
99: [n130:44749] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
99: --------------------------------------------------------------------------
99: The application appears to have been direct launched using "srun",
99: but OMPI was not built with SLURM's PMI support and therefore cannot
99: execute. There are several options for building PMI support under
99: SLURM, depending upon the SLURM version you are using:
99: 
99:   version 16.05 or later: you can use SLURM's PMIx support. This
99:   requires that you configure and build SLURM --with-pmix.
99: 
99:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
99:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
99:   install PMI-2. You must then build Open MPI using --with-pmi pointing
99:   to the SLURM PMI library location.
99: 
99: Please configure as appropriate and try again.
99: --------------------------------------------------------------------------
99: *** An error occurred in MPI_Init_thread
99: *** on a NULL communicator
99: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
99: ***    and potentially your MPI job)
99: [n130:44749] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
99: 
99: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_pp86
99:    inputs: ne.mol  &  p86c.inp
99: 
99: running test with input files ['p86c_xcfun.inp', 'ne.mol'] and args None
99: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=p86c_xcfun.inp', '--mol=ne.mol']
99: 
99:  **** dirac-executable stderr console output : **** 
99: [n130:44884] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
99: --------------------------------------------------------------------------
99: The application appears to have been direct launched using "srun",
99: but OMPI was not built with SLURM's PMI support and therefore cannot
99: execute. There are several options for building PMI support under
99: SLURM, depending upon the SLURM version you are using:
99: 
99:   version 16.05 or later: you can use SLURM's PMIx support. This
99:   requires that you configure and build SLURM --with-pmix.
99: 
99:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
99:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
99:   install PMI-2. You must then build Open MPI using --with-pmi pointing
99:   to the SLURM PMI library location.
99: 
99: Please configure as appropriate and try again.
99: --------------------------------------------------------------------------
99: *** An error occurred in MPI_Init_thread
99: *** on a NULL communicator
99: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
99: ***    and potentially your MPI job)
99: [n130:44884] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
99: [n130:44885] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
99: --------------------------------------------------------------------------
99: The application appears to have been direct launched using "srun",
99: but OMPI was not built with SLURM's PMI support and therefore cannot
99: execute. There are several options for building PMI support under
99: SLURM, depending upon the SLURM version you are using:
99: 
99:   version 16.05 or later: you can use SLURM's PMIx support. This
99:   requires that you configure and build SLURM --with-pmix.
99: 
99:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
99:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
99:   install PMI-2. You must then build Open MPI using --with-pmi pointing
99:   to the SLURM PMI library location.
99: 
99: Please configure as appropriate and try again.
99: --------------------------------------------------------------------------
99: *** An error occurred in MPI_Init_thread
99: *** on a NULL communicator
99: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
99: ***    and potentially your MPI job)
99: [n130:44885] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
99: 
99: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_pp86
99:    inputs: ne.mol  &  p86c_xcfun.inp
21/92 Test  #99: dft_pp86 .........................***Failed   28.63 sec
test 110
      Start 110: reladc_fano

110: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/reladc_fano/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/reladc_fano" "--verbose"
110: Test timeout computed to be: 1500
108: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=dc.inp', '--mol=H2O.mol']
108: 
108:  **** dirac-executable stderr console output : **** 
108: [n130:44967] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
108: --------------------------------------------------------------------------
108: The application appears to have been direct launched using "srun",
108: but OMPI was not built with SLURM's PMI support and therefore cannot
108: execute. There are several options for building PMI support under
108: SLURM, depending upon the SLURM version you are using:
108: 
108:   version 16.05 or later: you can use SLURM's PMIx support. This
108:   requires that you configure and build SLURM --with-pmix.
108: 
108:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
108:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
108:   install PMI-2. You must then build Open MPI using --with-pmi pointing
108:   to the SLURM PMI library location.
108: 
108: Please configure as appropriate and try again.
108: --------------------------------------------------------------------------
108: *** An error occurred in MPI_Init_thread
108: *** on a NULL communicator
108: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
108: ***    and potentially your MPI job)
108: [n130:44967] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
108: [n130:44966] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
108: --------------------------------------------------------------------------
108: The application appears to have been direct launched using "srun",
108: but OMPI was not built with SLURM's PMI support and therefore cannot
108: execute. There are several options for building PMI support under
108: SLURM, depending upon the SLURM version you are using:
108: 
108:   version 16.05 or later: you can use SLURM's PMIx support. This
108:   requires that you configure and build SLURM --with-pmix.
108: 
108:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
108:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
108:   install PMI-2. You must then build Open MPI using --with-pmi pointing
108:   to the SLURM PMI library location.
108: 
108: Please configure as appropriate and try again.
108: --------------------------------------------------------------------------
108: *** An error occurred in MPI_Init_thread
108: *** on a NULL communicator
108: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
108: ***    and potentially your MPI job)
108: [n130:44966] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
108: 
108: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/response_hf_polarizability
108:    inputs: H2O.mol  &  dc.inp
108: 
108: running test: dc H2O
22/92 Test #108: response_hf_polarizability .......***Failed    4.21 sec
test 111
      Start 111: cosci_methods

111: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/cosci_methods/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/cosci_methods" "--verbose"
111: Test timeout computed to be: 1500
109: 
109: running test with input files ['H2O.ae4z.x2c.scf_countmem-relcc.inp', 'H2O.xyz'] and args  --mw=32 --nw=32 --aw=64 --put "DFPCMO.H2O.x2c.ae4z=DFPCMO" 
109: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=H2O.ae4z.x2c.scf_countmem-relcc.inp', '--mol=H2O.xyz', '--mw=32', '--nw=32', '--aw=64', '--put', 'DFPCMO.H2O.x2c.ae4z=DFPCMO']
109: 
109:  **** dirac-executable stderr console output : **** 
109: [n130:44974] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
109: --------------------------------------------------------------------------
109: The application appears to have been direct launched using "srun",
109: but OMPI was not built with SLURM's PMI support and therefore cannot
109: execute. There are several options for building PMI support under
109: SLURM, depending upon the SLURM version you are using:
109: 
109:   version 16.05 or later: you can use SLURM's PMIx support. This
109:   requires that you configure and build SLURM --with-pmix.
109: 
109:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
109:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
109:   install PMI-2. You must then build Open MPI using --with-pmi pointing
109:   to the SLURM PMI library location.
109: 
109: Please configure as appropriate and try again.
109: --------------------------------------------------------------------------
109: *** An error occurred in MPI_Init_thread
109: *** on a NULL communicator
109: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
109: ***    and potentially your MPI job)
109: [n130:44974] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
109: [n130:44975] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
109: --------------------------------------------------------------------------
109: The application appears to have been direct launched using "srun",
109: but OMPI was not built with SLURM's PMI support and therefore cannot
109: execute. There are several options for building PMI support under
109: SLURM, depending upon the SLURM version you are using:
109: 
109:   version 16.05 or later: you can use SLURM's PMIx support. This
109:   requires that you configure and build SLURM --with-pmix.
109: 
109:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
109:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
109:   install PMI-2. You must then build Open MPI using --with-pmi pointing
109:   to the SLURM PMI library location.
109: 
109: Please configure as appropriate and try again.
109: --------------------------------------------------------------------------
109: *** An error occurred in MPI_Init_thread
109: *** on a NULL communicator
109: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
109: ***    and potentially your MPI job)
109: [n130:44975] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
109: 
109: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/count_cc_memory
109:    inputs: H2O.xyz  &  H2O.ae4z.x2c.scf_countmem-relcc.inp
23/92 Test #109: count_cc_memory ..................***Failed    4.26 sec
test 112
      Start 112: response_nonrel

112: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/response_nonrel/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/response_nonrel" "--verbose"
112: Test timeout computed to be: 1500
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:44860] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: [n130:44859] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:44859] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: 
88: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition
88:    inputs: molecule14.xyz  &  test_xyz.inp
88: 
88: running test with input files ['test_xyz.inp', 'molecule15.xyz'] and args None
88: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=test_xyz.inp', '--mol=molecule15.xyz']
88: 
88:  **** dirac-executable stderr console output : **** 
88: [n130:45002] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:45002] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
110: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--mw=64', '--aw=130', '--inp=fano_real.inp', '--mol=ne_d2h.mol']
110: 
110:  **** dirac-executable stderr console output : **** 
110: [n130:45033] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
110: --------------------------------------------------------------------------
110: The application appears to have been direct launched using "srun",
110: but OMPI was not built with SLURM's PMI support and therefore cannot
110: execute. There are several options for building PMI support under
110: SLURM, depending upon the SLURM version you are using:
110: 
110:   version 16.05 or later: you can use SLURM's PMIx support. This
110:   requires that you configure and build SLURM --with-pmix.
110: 
110:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
110:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
110:   install PMI-2. You must then build Open MPI using --with-pmi pointing
110:   to the SLURM PMI library location.
110: 
110: Please configure as appropriate and try again.
110: --------------------------------------------------------------------------
110: *** An error occurred in MPI_Init_thread
110: *** on a NULL communicator
110: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
110: ***    and potentially your MPI job)
110: [n130:45033] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
110: [n130:45034] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
110: --------------------------------------------------------------------------
110: The application appears to have been direct launched using "srun",
110: but OMPI was not built with SLURM's PMI support and therefore cannot
110: execute. There are several options for building PMI support under
110: SLURM, depending upon the SLURM version you are using:
110: 
110:   version 16.05 or later: you can use SLURM's PMIx support. This
110:   requires that you configure and build SLURM --with-pmix.
110: 
110:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
110:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
110:   install PMI-2. You must then build Open MPI using --with-pmi pointing
110:   to the SLURM PMI library location.
110: 
110: Please configure as appropriate and try again.
110: --------------------------------------------------------------------------
110: *** An error occurred in MPI_Init_thread
110: *** on a NULL communicator
110: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
110: ***    and potentially your MPI job)
110: [n130:45034] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
110: 
110: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/reladc_fano
110:    inputs: ne_d2h.mol  &  fano_real.inp
110: 
110: running test: fano_real ne_d2h
24/92 Test #110: reladc_fano ......................***Failed    4.49 sec
test 113
      Start 113: eomip_fc_cvs

113: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/eomip_fc_cvs/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/eomip_fc_cvs" "--verbose"
113: Test timeout computed to be: 1500
111: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=ci.1.inp', '--mol=O.mol']
111: 
111:  **** dirac-executable stderr console output : **** 
111: [n130:45099] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
111: --------------------------------------------------------------------------
111: The application appears to have been direct launched using "srun",
111: but OMPI was not built with SLURM's PMI support and therefore cannot
111: execute. There are several options for building PMI support under
111: SLURM, depending upon the SLURM version you are using:
111: 
111:   version 16.05 or later: you can use SLURM's PMIx support. This
111:   requires that you configure and build SLURM --with-pmix.
111: 
111:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
111:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
111:   install PMI-2. You must then build Open MPI using --with-pmi pointing
111:   to the SLURM PMI library location.
111: 
111: Please configure as appropriate and try again.
111: --------------------------------------------------------------------------
111: *** An error occurred in MPI_Init_thread
111: *** on a NULL communicator
111: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
111: ***    and potentially your MPI job)
111: [n130:45099] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
111: [n130:45100] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
111: --------------------------------------------------------------------------
111: The application appears to have been direct launched using "srun",
111: but OMPI was not built with SLURM's PMI support and therefore cannot
111: execute. There are several options for building PMI support under
111: SLURM, depending upon the SLURM version you are using:
111: 
111:   version 16.05 or later: you can use SLURM's PMIx support. This
111:   requires that you configure and build SLURM --with-pmix.
111: 
111:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
111:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
111:   install PMI-2. You must then build Open MPI using --with-pmi pointing
111:   to the SLURM PMI library location.
111: 
111: Please configure as appropriate and try again.
111: --------------------------------------------------------------------------
111: *** An error occurred in MPI_Init_thread
111: *** on a NULL communicator
111: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
111: ***    and potentially your MPI job)
111: [n130:45100] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
111: 
111: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/cosci_methods
111:    inputs: O.mol  &  ci.1.inp
111: 
111: running test: ci.1 O
25/92 Test #111: cosci_methods ....................***Failed    3.81 sec
test 114
      Start 114: symmetry_recognition

114: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/symmetry_recognition/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/symmetry_recognition" "--verbose"
114: Test timeout computed to be: 1500
112: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=levy-leblond.inp', '--mol=hf.mol']
112: 
112:  **** dirac-executable stderr console output : **** 
112: [n130:45117] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
112: [n130:45118] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
112: --------------------------------------------------------------------------
112: The application appears to have been direct launched using "srun",
112: but OMPI was not built with SLURM's PMI support and therefore cannot
112: execute. There are several options for building PMI support under
112: SLURM, depending upon the SLURM version you are using:
112: 
112:   version 16.05 or later: you can use SLURM's PMIx support. This
112:   requires that you configure and build SLURM --with-pmix.
112: 
112:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
112:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
112:   install PMI-2. You must then build Open MPI using --with-pmi pointing
112:   to the SLURM PMI library location.
112: 
112: Please configure as appropriate and try again.
112: --------------------------------------------------------------------------
112: *** An error occurred in MPI_Init_thread
112: *** on a NULL communicator
112: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
112: ***    and potentially your MPI job)
112: [n130:45118] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
112: --------------------------------------------------------------------------
112: The application appears to have been direct launched using "srun",
112: but OMPI was not built with SLURM's PMI support and therefore cannot
112: execute. There are several options for building PMI support under
112: SLURM, depending upon the SLURM version you are using:
112: 
112:   version 16.05 or later: you can use SLURM's PMIx support. This
112:   requires that you configure and build SLURM --with-pmix.
112: 
112:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
112:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
112:   install PMI-2. You must then build Open MPI using --with-pmi pointing
112:   to the SLURM PMI library location.
112: 
112: Please configure as appropriate and try again.
112: --------------------------------------------------------------------------
112: *** An error occurred in MPI_Init_thread
112: *** on a NULL communicator
112: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
112: ***    and potentially your MPI job)
112: [n130:45117] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
112: 
112: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/response_nonrel
112:    inputs: hf.mol  &  levy-leblond.inp
112: 
112: running test: levy-leblond hf
26/92 Test #112: response_nonrel ..................***Failed    3.39 sec
test 115
      Start 115: reladc_sip

115: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/reladc_sip/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/reladc_sip" "--verbose"
115: Test timeout computed to be: 1500
88: [n130:45003] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:45003] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: 
88: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition
88:    inputs: molecule15.xyz  &  test_xyz.inp
88: 
88: running test with input files ['test_xyz.inp', 'molecule16.xyz'] and args None
88: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=test_xyz.inp', '--mol=molecule16.xyz']
88: 
88:  **** dirac-executable stderr console output : **** 
88: [n130:45147] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: [n130:45146] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:45147] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
113: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--mw=110', '--inp=fc_cvs_eom_ip_sym.inp', '--mol=h2o.xyz']
113: 
113:  **** dirac-executable stderr console output : **** 
113: [n130:45186] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
113: --------------------------------------------------------------------------
113: The application appears to have been direct launched using "srun",
113: but OMPI was not built with SLURM's PMI support and therefore cannot
113: execute. There are several options for building PMI support under
113: SLURM, depending upon the SLURM version you are using:
113: 
113:   version 16.05 or later: you can use SLURM's PMIx support. This
113:   requires that you configure and build SLURM --with-pmix.
113: 
113:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
113:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
113:   install PMI-2. You must then build Open MPI using --with-pmi pointing
113:   to the SLURM PMI library location.
113: 
113: Please configure as appropriate and try again.
113: --------------------------------------------------------------------------
113: *** An error occurred in MPI_Init_thread
113: *** on a NULL communicator
113: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
113: ***    and potentially your MPI job)
113: [n130:45186] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
113: [n130:45185] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
113: --------------------------------------------------------------------------
113: The application appears to have been direct launched using "srun",
113: but OMPI was not built with SLURM's PMI support and therefore cannot
113: execute. There are several options for building PMI support under
113: SLURM, depending upon the SLURM version you are using:
113: 
113:   version 16.05 or later: you can use SLURM's PMIx support. This
113:   requires that you configure and build SLURM --with-pmix.
113: 
113:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
113:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
113:   install PMI-2. You must then build Open MPI using --with-pmi pointing
113:   to the SLURM PMI library location.
113: 
113: Please configure as appropriate and try again.
113: --------------------------------------------------------------------------
113: *** An error occurred in MPI_Init_thread
113: *** on a NULL communicator
113: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
113: ***    and potentially your MPI job)
113: [n130:45185] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
113: 
113: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/eomip_fc_cvs
113:    inputs: h2o.xyz  &  fc_cvs_eom_ip_sym.inp
113: 
113: running test: fc_cvs_eom_ip_sym h2o
27/92 Test #113: eomip_fc_cvs .....................***Failed    4.13 sec
test 116
      Start 116: ecp

116: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/ecp/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/ecp" "--verbose"
116: Test timeout computed to be: 1500
114: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=lda.inp', '--mol=H2O.mol']
114: 
114:  **** dirac-executable stderr console output : **** 
114: [n130:45246] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
114: --------------------------------------------------------------------------
114: The application appears to have been direct launched using "srun",
114: but OMPI was not built with SLURM's PMI support and therefore cannot
114: execute. There are several options for building PMI support under
114: SLURM, depending upon the SLURM version you are using:
114: 
114:   version 16.05 or later: you can use SLURM's PMIx support. This
114:   requires that you configure and build SLURM --with-pmix.
114: 
114:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
114:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
114:   install PMI-2. You must then build Open MPI using --with-pmi pointing
114:   to the SLURM PMI library location.
114: 
114: Please configure as appropriate and try again.
114: --------------------------------------------------------------------------
114: *** An error occurred in MPI_Init_thread
114: *** on a NULL communicator
114: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
114: ***    and potentially your MPI job)
114: [n130:45246] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
114: [n130:45248] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
114: --------------------------------------------------------------------------
114: The application appears to have been direct launched using "srun",
114: but OMPI was not built with SLURM's PMI support and therefore cannot
114: execute. There are several options for building PMI support under
114: SLURM, depending upon the SLURM version you are using:
114: 
114:   version 16.05 or later: you can use SLURM's PMIx support. This
114:   requires that you configure and build SLURM --with-pmix.
114: 
114:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
114:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
114:   install PMI-2. You must then build Open MPI using --with-pmi pointing
114:   to the SLURM PMI library location.
114: 
114: Please configure as appropriate and try again.
114: --------------------------------------------------------------------------
114: *** An error occurred in MPI_Init_thread
114: *** on a NULL communicator
114: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
114: ***    and potentially your MPI job)
114: [n130:45248] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
114: 
114: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/symmetry_recognition
114:    inputs: H2O.mol  &  lda.inp
114: 
114: running test: lda H2O
28/92 Test #114: symmetry_recognition .............***Failed    4.11 sec
test 117
      Start 117: scf_levelshift

117: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/scf_levelshift/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/scf_levelshift" "--verbose"
117: Test timeout computed to be: 1500
115: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=hcn_complex_1.inp', '--mol=hcn_cs.mol']
115: 
115:  **** dirac-executable stderr console output : **** 
115: [n130:45251] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
115: --------------------------------------------------------------------------
115: The application appears to have been direct launched using "srun",
115: but OMPI was not built with SLURM's PMI support and therefore cannot
115: execute. There are several options for building PMI support under
115: SLURM, depending upon the SLURM version you are using:
115: 
115:   version 16.05 or later: you can use SLURM's PMIx support. This
115:   requires that you configure and build SLURM --with-pmix.
115: 
115:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
115:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
115:   install PMI-2. You must then build Open MPI using --with-pmi pointing
115:   to the SLURM PMI library location.
115: 
115: Please configure as appropriate and try again.
115: --------------------------------------------------------------------------
115: *** An error occurred in MPI_Init_thread
115: *** on a NULL communicator
115: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
115: ***    and potentially your MPI job)
115: [n130:45251] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
115: [n130:45252] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
115: --------------------------------------------------------------------------
115: The application appears to have been direct launched using "srun",
115: but OMPI was not built with SLURM's PMI support and therefore cannot
115: execute. There are several options for building PMI support under
115: SLURM, depending upon the SLURM version you are using:
115: 
115:   version 16.05 or later: you can use SLURM's PMIx support. This
115:   requires that you configure and build SLURM --with-pmix.
115: 
115:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
115:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
115:   install PMI-2. You must then build Open MPI using --with-pmi pointing
115:   to the SLURM PMI library location.
115: 
115: Please configure as appropriate and try again.
115: --------------------------------------------------------------------------
115: *** An error occurred in MPI_Init_thread
115: *** on a NULL communicator
115: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
115: ***    and potentially your MPI job)
115: [n130:45252] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
115: 
115: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/reladc_sip
115:    inputs: hcn_cs.mol  &  hcn_complex_1.inp
115: 
115: running test: hcn_complex_1 hcn_cs
29/92 Test #115: reladc_sip .......................***Failed    3.92 sec
test 118
      Start 118: basis_input

118: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/basis_input/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/basis_input" "--verbose"
118: Test timeout computed to be: 1500
116: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=HF.inp', '--mol=HI_arep.mol']
116: 
116:  **** dirac-executable stderr console output : **** 
116: [n130:45330] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
116: --------------------------------------------------------------------------
116: The application appears to have been direct launched using "srun",
116: but OMPI was not built with SLURM's PMI support and therefore cannot
116: execute. There are several options for building PMI support under
116: SLURM, depending upon the SLURM version you are using:
116: 
116:   version 16.05 or later: you can use SLURM's PMIx support. This
116:   requires that you configure and build SLURM --with-pmix.
116: 
116:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
116:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
116:   install PMI-2. You must then build Open MPI using --with-pmi pointing
116:   to the SLURM PMI library location.
116: 
116: Please configure as appropriate and try again.
116: --------------------------------------------------------------------------
116: *** An error occurred in MPI_Init_thread
116: *** on a NULL communicator
116: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
116: ***    and potentially your MPI job)
116: [n130:45330] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
116: [n130:45329] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
116: --------------------------------------------------------------------------
116: The application appears to have been direct launched using "srun",
116: but OMPI was not built with SLURM's PMI support and therefore cannot
116: execute. There are several options for building PMI support under
116: SLURM, depending upon the SLURM version you are using:
116: 
116:   version 16.05 or later: you can use SLURM's PMIx support. This
116:   requires that you configure and build SLURM --with-pmix.
116: 
116:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
116:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
116:   install PMI-2. You must then build Open MPI using --with-pmi pointing
116:   to the SLURM PMI library location.
116: 
116: Please configure as appropriate and try again.
116: --------------------------------------------------------------------------
116: *** An error occurred in MPI_Init_thread
116: *** on a NULL communicator
116: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
116: ***    and potentially your MPI job)
116: [n130:45329] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
116: 
116: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/ecp
116:    inputs: HI_arep.mol  &  HF.inp
116: 
116: running test: HF HI_arep
30/92 Test #116: ecp ..............................***Failed    4.20 sec
test 119
      Start 119: visual_gamma5

119: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/visual_gamma5/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_gamma5" "--verbose"
119: Test timeout computed to be: 1500
117: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--mw=110', '--inp=levelshift_virtuals.inp', '--mol=Mg.mol']
117: 
117:  **** dirac-executable stderr console output : **** 
117: [n130:45375] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
117: --------------------------------------------------------------------------
117: The application appears to have been direct launched using "srun",
117: but OMPI was not built with SLURM's PMI support and therefore cannot
117: execute. There are several options for building PMI support under
117: SLURM, depending upon the SLURM version you are using:
117: 
117:   version 16.05 or later: you can use SLURM's PMIx support. This
117:   requires that you configure and build SLURM --with-pmix.
117: 
117:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
117:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
117:   install PMI-2. You must then build Open MPI using --with-pmi pointing
117:   to the SLURM PMI library location.
117: 
117: Please configure as appropriate and try again.
117: --------------------------------------------------------------------------
117: *** An error occurred in MPI_Init_thread
117: *** on a NULL communicator
117: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
117: ***    and potentially your MPI job)
117: [n130:45375] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
117: [n130:45374] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
117: --------------------------------------------------------------------------
117: The application appears to have been direct launched using "srun",
117: but OMPI was not built with SLURM's PMI support and therefore cannot
117: execute. There are several options for building PMI support under
117: SLURM, depending upon the SLURM version you are using:
117: 
117:   version 16.05 or later: you can use SLURM's PMIx support. This
117:   requires that you configure and build SLURM --with-pmix.
117: 
117:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
117:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
117:   install PMI-2. You must then build Open MPI using --with-pmi pointing
117:   to the SLURM PMI library location.
117: 
117: Please configure as appropriate and try again.
117: --------------------------------------------------------------------------
117: *** An error occurred in MPI_Init_thread
117: *** on a NULL communicator
117: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
117: ***    and potentially your MPI job)
117: [n130:45374] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
117: 
117: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/scf_levelshift
117:    inputs: Mg.mol  &  levelshift_virtuals.inp
117: 
117: running test: levelshift_virtuals Mg
31/92 Test #117: scf_levelshift ...................***Failed    3.16 sec
test 120
      Start 120: pvc_scf

120: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/pvc_scf/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/pvc_scf" "--verbose"
120: Test timeout computed to be: 1500
118: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=ch4.inp', '--mol=ch4_temp.mol']
118: 
118:  **** dirac-executable stderr console output : **** 
118: [n130:45395] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
118: --------------------------------------------------------------------------
118: The application appears to have been direct launched using "srun",
118: but OMPI was not built with SLURM's PMI support and therefore cannot
118: execute. There are several options for building PMI support under
118: SLURM, depending upon the SLURM version you are using:
118: 
118:   version 16.05 or later: you can use SLURM's PMIx support. This
118:   requires that you configure and build SLURM --with-pmix.
118: 
118:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
118:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
118:   install PMI-2. You must then build Open MPI using --with-pmi pointing
118:   to the SLURM PMI library location.
118: 
118: Please configure as appropriate and try again.
118: --------------------------------------------------------------------------
118: *** An error occurred in MPI_Init_thread
118: *** on a NULL communicator
118: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
118: ***    and potentially your MPI job)
118: [n130:45395] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
118: [n130:45394] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
118: --------------------------------------------------------------------------
118: The application appears to have been direct launched using "srun",
118: but OMPI was not built with SLURM's PMI support and therefore cannot
118: execute. There are several options for building PMI support under
118: SLURM, depending upon the SLURM version you are using:
118: 
118:   version 16.05 or later: you can use SLURM's PMIx support. This
118:   requires that you configure and build SLURM --with-pmix.
118: 
118:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
118:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
118:   install PMI-2. You must then build Open MPI using --with-pmi pointing
118:   to the SLURM PMI library location.
118: 
118: Please configure as appropriate and try again.
118: --------------------------------------------------------------------------
118: *** An error occurred in MPI_Init_thread
118: *** on a NULL communicator
118: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
118: ***    and potentially your MPI job)
118: [n130:45394] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
118: 
118: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/basis_input
118:    inputs: ch4_temp.mol  &  ch4.inp
118: 
118: running test: ch4 ch4_temp
32/92 Test #118: basis_input ......................***Failed    3.18 sec
test 121
      Start 121: cosci_energy_spinfree

121: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/cosci_energy_spinfree/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/cosci_energy_spinfree" "--verbose"
121: Test timeout computed to be: 1500
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:45146] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: 
88: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition
88:    inputs: molecule16.xyz  &  test_xyz.inp
88: 
88: running test with input files ['test_xyz.inp', 'molecule17.xyz'] and args None
88: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=test_xyz.inp', '--mol=molecule17.xyz']
88: 
88:  **** dirac-executable stderr console output : **** 
88: [n130:45290] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:45290] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: [n130:45289] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:45289] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: 
88: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition
88:    inputs: molecule17.xyz  &  test_xyz.inp
88: 
88: running test with input files ['test_xyz.inp', 'molecule18.xyz'] and args None
88: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=test_xyz.inp', '--mol=molecule18.xyz']
88: 
88:  **** dirac-executable stderr console output : **** 
88: [n130:45444] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:45444] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: [n130:45443] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
88: --------------------------------------------------------------------------
88: The application appears to have been direct launched using "srun",
88: but OMPI was not built with SLURM's PMI support and therefore cannot
88: execute. There are several options for building PMI support under
88: SLURM, depending upon the SLURM version you are using:
88: 
88:   version 16.05 or later: you can use SLURM's PMIx support. This
88:   requires that you configure and build SLURM --with-pmix.
88: 
88:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
88:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
88:   install PMI-2. You must then build Open MPI using --with-pmi pointing
88:   to the SLURM PMI library location.
88: 
88: Please configure as appropriate and try again.
88: --------------------------------------------------------------------------
88: *** An error occurred in MPI_Init_thread
88: *** on a NULL communicator
88: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
88: ***    and potentially your MPI job)
88: [n130:45443] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
88: 
88: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/xyz_symmetry_recognition
88:    inputs: molecule18.xyz  &  test_xyz.inp
33/92 Test  #88: xyz_symmetry_recognition .........***Failed   73.53 sec
test 122
      Start 122: density_at_nuclei

122: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/density_at_nuclei/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/density_at_nuclei" "--verbose"
122: Test timeout computed to be: 1500
119: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=gamma5.inp', '--mol=h2o2.mol']
119: 
119:  **** dirac-executable stderr console output : **** 
119: [n130:45479] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
119: [n130:45480] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
119: --------------------------------------------------------------------------
119: The application appears to have been direct launched using "srun",
119: but OMPI was not built with SLURM's PMI support and therefore cannot
119: execute. There are several options for building PMI support under
119: SLURM, depending upon the SLURM version you are using:
119: 
119:   version 16.05 or later: you can use SLURM's PMIx support. This
119:   requires that you configure and build SLURM --with-pmix.
119: 
119:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
119:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
119:   install PMI-2. You must then build Open MPI using --with-pmi pointing
119:   to the SLURM PMI library location.
119: 
119: Please configure as appropriate and try again.
119: --------------------------------------------------------------------------
119: *** An error occurred in MPI_Init_thread
119: *** on a NULL communicator
119: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
119: ***    and potentially your MPI job)
119: [n130:45479] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
119: --------------------------------------------------------------------------
119: The application appears to have been direct launched using "srun",
119: but OMPI was not built with SLURM's PMI support and therefore cannot
119: execute. There are several options for building PMI support under
119: SLURM, depending upon the SLURM version you are using:
119: 
119:   version 16.05 or later: you can use SLURM's PMIx support. This
119:   requires that you configure and build SLURM --with-pmix.
119: 
119:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
119:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
119:   install PMI-2. You must then build Open MPI using --with-pmi pointing
119:   to the SLURM PMI library location.
119: 
119: Please configure as appropriate and try again.
119: --------------------------------------------------------------------------
119: *** An error occurred in MPI_Init_thread
119: *** on a NULL communicator
119: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
119: ***    and potentially your MPI job)
119: [n130:45480] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
119: 
119: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_gamma5
119:    inputs: h2o2.mol  &  gamma5.inp
119: 
119: running test: gamma5 h2o2
34/92 Test #119: visual_gamma5 ....................***Failed    3.84 sec
test 123
      Start 123: dft_erf_xcfun

123: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/dft_erf_xcfun/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_erf_xcfun" "--verbose"
123: Test timeout computed to be: 1500
120: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=point_charge.inp', '--mol=h2o2.mol']
120: 
120:  **** dirac-executable stderr console output : **** 
120: [n130:45533] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
120: --------------------------------------------------------------------------
120: The application appears to have been direct launched using "srun",
120: but OMPI was not built with SLURM's PMI support and therefore cannot
120: execute. There are several options for building PMI support under
120: SLURM, depending upon the SLURM version you are using:
120: 
120:   version 16.05 or later: you can use SLURM's PMIx support. This
120:   requires that you configure and build SLURM --with-pmix.
120: 
120:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
120:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
120:   install PMI-2. You must then build Open MPI using --with-pmi pointing
120:   to the SLURM PMI library location.
120: 
120: Please configure as appropriate and try again.
120: --------------------------------------------------------------------------
120: [n130:45532] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
120: *** An error occurred in MPI_Init_thread
120: *** on a NULL communicator
120: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
120: ***    and potentially your MPI job)
120: [n130:45533] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
120: --------------------------------------------------------------------------
120: The application appears to have been direct launched using "srun",
120: but OMPI was not built with SLURM's PMI support and therefore cannot
120: execute. There are several options for building PMI support under
120: SLURM, depending upon the SLURM version you are using:
120: 
120:   version 16.05 or later: you can use SLURM's PMIx support. This
120:   requires that you configure and build SLURM --with-pmix.
120: 
120:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
120:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
120:   install PMI-2. You must then build Open MPI using --with-pmi pointing
120:   to the SLURM PMI library location.
120: 
120: Please configure as appropriate and try again.
120: --------------------------------------------------------------------------
120: *** An error occurred in MPI_Init_thread
120: *** on a NULL communicator
120: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
120: ***    and potentially your MPI job)
120: [n130:45532] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
120: 
120: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/pvc_scf
120:    inputs: h2o2.mol  &  point_charge.inp
120: 
120: running test: point_charge h2o2
35/92 Test #120: pvc_scf ..........................***Failed    4.31 sec
test 124
      Start 124: mp2_natural_orbitals

124: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/mp2_natural_orbitals/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/mp2_natural_orbitals" "--verbose"
124: Test timeout computed to be: 1500
121: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=ci.inp', '--mol=F.mol']
121: 
121:  **** dirac-executable stderr console output : **** 
121: [n130:45536] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
121: --------------------------------------------------------------------------
121: The application appears to have been direct launched using "srun",
121: but OMPI was not built with SLURM's PMI support and therefore cannot
121: execute. There are several options for building PMI support under
121: SLURM, depending upon the SLURM version you are using:
121: 
121:   version 16.05 or later: you can use SLURM's PMIx support. This
121:   requires that you configure and build SLURM --with-pmix.
121: 
121:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
121:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
121:   install PMI-2. You must then build Open MPI using --with-pmi pointing
121:   to the SLURM PMI library location.
121: 
121: Please configure as appropriate and try again.
121: --------------------------------------------------------------------------
121: *** An error occurred in MPI_Init_thread
121: *** on a NULL communicator
121: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
121: ***    and potentially your MPI job)
121: [n130:45536] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
121: [n130:45537] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
121: --------------------------------------------------------------------------
121: The application appears to have been direct launched using "srun",
121: but OMPI was not built with SLURM's PMI support and therefore cannot
121: execute. There are several options for building PMI support under
121: SLURM, depending upon the SLURM version you are using:
121: 
121:   version 16.05 or later: you can use SLURM's PMIx support. This
121:   requires that you configure and build SLURM --with-pmix.
121: 
121:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
121:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
121:   install PMI-2. You must then build Open MPI using --with-pmi pointing
121:   to the SLURM PMI library location.
121: 
121: Please configure as appropriate and try again.
121: --------------------------------------------------------------------------
121: *** An error occurred in MPI_Init_thread
121: *** on a NULL communicator
121: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
121: ***    and potentially your MPI job)
121: [n130:45537] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
121: 
121: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/cosci_energy_spinfree
121:    inputs: F.mol  &  ci.inp
121: 
121: running test: ci F
36/92 Test #121: cosci_energy_spinfree ............***Failed    4.42 sec
test 125
      Start 125: ffpt_dipmom_polariz_relcc

125: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/ffpt_dipmom_polariz_relcc/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/ffpt_dipmom_polariz_relcc" "--verbose"
125: Test timeout computed to be: 1500
122: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=Rhonuc.inp', '--mol=CO.mol']
122: 
122:  **** dirac-executable stderr console output : **** 
122: [n130:45576] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
122: --------------------------------------------------------------------------
122: The application appears to have been direct launched using "srun",
122: but OMPI was not built with SLURM's PMI support and therefore cannot
122: execute. There are several options for building PMI support under
122: SLURM, depending upon the SLURM version you are using:
122: 
122:   version 16.05 or later: you can use SLURM's PMIx support. This
122:   requires that you configure and build SLURM --with-pmix.
122: 
122:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
122:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
122:   install PMI-2. You must then build Open MPI using --with-pmi pointing
122:   to the SLURM PMI library location.
122: 
122: Please configure as appropriate and try again.
122: --------------------------------------------------------------------------
122: *** An error occurred in MPI_Init_thread
122: *** on a NULL communicator
122: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
122: ***    and potentially your MPI job)
122: [n130:45576] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
122: [n130:45577] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
122: --------------------------------------------------------------------------
122: The application appears to have been direct launched using "srun",
122: but OMPI was not built with SLURM's PMI support and therefore cannot
122: execute. There are several options for building PMI support under
122: SLURM, depending upon the SLURM version you are using:
122: 
122:   version 16.05 or later: you can use SLURM's PMIx support. This
122:   requires that you configure and build SLURM --with-pmix.
122: 
122:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
122:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
122:   install PMI-2. You must then build Open MPI using --with-pmi pointing
122:   to the SLURM PMI library location.
122: 
122: Please configure as appropriate and try again.
122: --------------------------------------------------------------------------
122: *** An error occurred in MPI_Init_thread
122: *** on a NULL communicator
122: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
122: ***    and potentially your MPI job)
122: [n130:45577] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
122: 
122: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/density_at_nuclei
122:    inputs: CO.mol  &  Rhonuc.inp
122: 
122: running test: Rhonuc CO
37/92 Test #122: density_at_nuclei ................***Failed    4.11 sec
test 126
      Start 126: mp2_srdft_energies

126: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/mp2_srdft_energies/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/mp2_srdft_energies" "--verbose"
126: Test timeout computed to be: 1500
123: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=ldaerf.inp', '--mol=he.mol']
123: 
123:  **** dirac-executable stderr console output : **** 
123: [n130:45620] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
123: --------------------------------------------------------------------------
123: The application appears to have been direct launched using "srun",
123: but OMPI was not built with SLURM's PMI support and therefore cannot
123: execute. There are several options for building PMI support under
123: SLURM, depending upon the SLURM version you are using:
123: 
123:   version 16.05 or later: you can use SLURM's PMIx support. This
123:   requires that you configure and build SLURM --with-pmix.
123: 
123:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
123:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
123:   install PMI-2. You must then build Open MPI using --with-pmi pointing
123:   to the SLURM PMI library location.
123: 
123: Please configure as appropriate and try again.
123: --------------------------------------------------------------------------
123: *** An error occurred in MPI_Init_thread
123: *** on a NULL communicator
123: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
123: ***    and potentially your MPI job)
123: [n130:45620] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
123: [n130:45621] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
123: --------------------------------------------------------------------------
123: The application appears to have been direct launched using "srun",
123: but OMPI was not built with SLURM's PMI support and therefore cannot
123: execute. There are several options for building PMI support under
123: SLURM, depending upon the SLURM version you are using:
123: 
123:   version 16.05 or later: you can use SLURM's PMIx support. This
123:   requires that you configure and build SLURM --with-pmix.
123: 
123:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
123:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
123:   install PMI-2. You must then build Open MPI using --with-pmi pointing
123:   to the SLURM PMI library location.
123: 
123: Please configure as appropriate and try again.
123: --------------------------------------------------------------------------
123: *** An error occurred in MPI_Init_thread
123: *** on a NULL communicator
123: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
123: ***    and potentially your MPI job)
123: [n130:45621] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
123: 
123: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_erf_xcfun
123:    inputs: he.mol  &  ldaerf.inp
123: 
123: running test: ldaerf he
38/92 Test #123: dft_erf_xcfun ....................***Failed    4.22 sec
test 127
      Start 127: operators_mo_mtx_elements

127: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/operators_mo_mtx_elements/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/operators_mo_mtx_elements" "--verbose"
127: Test timeout computed to be: 1500
124: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--mw=80', '--aw=120', '--inp=hf.inp', '--mol=cinfv.mol']
124: 
124:  **** dirac-executable stderr console output : **** 
124: [n130:45655] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
124: --------------------------------------------------------------------------
124: The application appears to have been direct launched using "srun",
124: but OMPI was not built with SLURM's PMI support and therefore cannot
124: execute. There are several options for building PMI support under
124: SLURM, depending upon the SLURM version you are using:
124: 
124:   version 16.05 or later: you can use SLURM's PMIx support. This
124:   requires that you configure and build SLURM --with-pmix.
124: 
124:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
124:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
124:   install PMI-2. You must then build Open MPI using --with-pmi pointing
124:   to the SLURM PMI library location.
124: 
124: Please configure as appropriate and try again.
124: --------------------------------------------------------------------------
124: *** An error occurred in MPI_Init_thread
124: *** on a NULL communicator
124: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
124: ***    and potentially your MPI job)
124: [n130:45655] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
124: [n130:45656] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
124: --------------------------------------------------------------------------
124: The application appears to have been direct launched using "srun",
124: but OMPI was not built with SLURM's PMI support and therefore cannot
124: execute. There are several options for building PMI support under
124: SLURM, depending upon the SLURM version you are using:
124: 
124:   version 16.05 or later: you can use SLURM's PMIx support. This
124:   requires that you configure and build SLURM --with-pmix.
124: 
124:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
124:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
124:   install PMI-2. You must then build Open MPI using --with-pmi pointing
124:   to the SLURM PMI library location.
124: 
124: Please configure as appropriate and try again.
124: --------------------------------------------------------------------------
124: *** An error occurred in MPI_Init_thread
124: *** on a NULL communicator
124: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
124: ***    and potentially your MPI job)
124: [n130:45656] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
124: 
124: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/mp2_natural_orbitals
124:    inputs: cinfv.mol  &  hf.inp
124: 
124: running test: hf cinfv
39/92 Test #124: mp2_natural_orbitals .............***Failed    4.60 sec
test 128
      Start 128: response_C6

128: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/response_C6/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/response_C6" "--verbose"
128: Test timeout computed to be: 1500
126: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=srLDA.inp', '--mol=He2.mol']
126: 
126:  **** dirac-executable stderr console output : **** 
126: [n130:45730] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
126: --------------------------------------------------------------------------
126: The application appears to have been direct launched using "srun",
126: but OMPI was not built with SLURM's PMI support and therefore cannot
126: execute. There are several options for building PMI support under
126: SLURM, depending upon the SLURM version you are using:
126: 
126:   version 16.05 or later: you can use SLURM's PMIx support. This
126:   requires that you configure and build SLURM --with-pmix.
126: 
126:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
126:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
126:   install PMI-2. You must then build Open MPI using --with-pmi pointing
126:   to the SLURM PMI library location.
126: 
126: Please configure as appropriate and try again.
126: --------------------------------------------------------------------------
126: *** An error occurred in MPI_Init_thread
126: *** on a NULL communicator
126: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
126: ***    and potentially your MPI job)
126: [n130:45730] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
126: [n130:45729] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
126: --------------------------------------------------------------------------
126: The application appears to have been direct launched using "srun",
126: but OMPI was not built with SLURM's PMI support and therefore cannot
126: execute. There are several options for building PMI support under
126: SLURM, depending upon the SLURM version you are using:
126: 
126:   version 16.05 or later: you can use SLURM's PMIx support. This
126:   requires that you configure and build SLURM --with-pmix.
126: 
126:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
126:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
126:   install PMI-2. You must then build Open MPI using --with-pmi pointing
126:   to the SLURM PMI library location.
126: 
126: Please configure as appropriate and try again.
126: --------------------------------------------------------------------------
126: *** An error occurred in MPI_Init_thread
126: *** on a NULL communicator
126: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
126: ***    and potentially your MPI job)
126: [n130:45729] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
126: 
126: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/mp2_srdft_energies
126:    inputs: He2.mol  &  srLDA.inp
126: 
126: running test: srLDA He2
40/92 Test #126: mp2_srdft_energies ...............***Failed    4.03 sec
test 129
      Start 129: bsse

129: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/bsse/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/bsse" "--verbose"
129: Test timeout computed to be: 1500
128: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=C6.inp', '--mol=Ne.mol']
128: 
128:  **** dirac-executable stderr console output : **** 
128: [n130:45812] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
128: --------------------------------------------------------------------------
128: The application appears to have been direct launched using "srun",
128: but OMPI was not built with SLURM's PMI support and therefore cannot
128: execute. There are several options for building PMI support under
128: SLURM, depending upon the SLURM version you are using:
128: 
128:   version 16.05 or later: you can use SLURM's PMIx support. This
128:   requires that you configure and build SLURM --with-pmix.
128: 
128:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
128:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
128:   install PMI-2. You must then build Open MPI using --with-pmi pointing
128:   to the SLURM PMI library location.
128: 
128: Please configure as appropriate and try again.
128: --------------------------------------------------------------------------
128: *** An error occurred in MPI_Init_thread
128: *** on a NULL communicator
128: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
128: ***    and potentially your MPI job)
128: [n130:45812] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
128: [n130:45813] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
128: --------------------------------------------------------------------------
128: The application appears to have been direct launched using "srun",
128: but OMPI was not built with SLURM's PMI support and therefore cannot
128: execute. There are several options for building PMI support under
128: SLURM, depending upon the SLURM version you are using:
128: 
128:   version 16.05 or later: you can use SLURM's PMIx support. This
128:   requires that you configure and build SLURM --with-pmix.
128: 
128:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
128:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
128:   install PMI-2. You must then build Open MPI using --with-pmi pointing
128:   to the SLURM PMI library location.
128: 
128: Please configure as appropriate and try again.
128: --------------------------------------------------------------------------
128: *** An error occurred in MPI_Init_thread
128: *** on a NULL communicator
128: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
128: ***    and potentially your MPI job)
128: [n130:45813] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
128: 
128: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/response_C6
128:    inputs: Ne.mol  &  C6.inp
128: 
128: running test: C6 Ne
41/92 Test #128: response_C6 ......................***Failed    4.43 sec
test 130
      Start 130: import_mos

130: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/import_mos/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/import_mos" "--verbose"
130: Test timeout computed to be: 1500
125: 
125: running test with input files ['BeH.x2c_scf_relcc_+0.0005.inp', 'BeH.sto-2g.C2v.mol'] and args --noarch --put "DFPCMO.BeH.x2c_scf_sto-2g.C2v=DFPCMO"
125: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=BeH.x2c_scf_relcc_+0.0005.inp', '--mol=BeH.sto-2g.C2v.mol', '--noarch', '--put', 'DFPCMO.BeH.x2c_scf_sto-2g.C2v=DFPCMO']
125: 
125:  **** dirac-executable stderr console output : **** 
125: [n130:45684] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: [n130:45685] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
125: [n130:45684] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
125: [n130:45685] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: 
125: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/ffpt_dipmom_polariz_relcc
125:    inputs: BeH.sto-2g.C2v.mol  &  BeH.x2c_scf_relcc_+0.0005.inp
125: 
125: running test with input files ['BeH.x2c_scf_relcc_-0.0005.inp', 'BeH.sto-2g.C2v.mol'] and args --noarch --put "DFPCMO.BeH.x2c_scf_sto-2g.C2v=DFPCMO"
125: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=BeH.x2c_scf_relcc_-0.0005.inp', '--mol=BeH.sto-2g.C2v.mol', '--noarch', '--put', 'DFPCMO.BeH.x2c_scf_sto-2g.C2v=DFPCMO']
125: 
125:  **** dirac-executable stderr console output : **** 
125: [n130:45844] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
129: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=x2c.scf_autoocc.inp', '--mol=HF.sto-2g.lsym.mol']
129: 
129:  **** dirac-executable stderr console output : **** 
129: [n130:45872] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
129: --------------------------------------------------------------------------
129: The application appears to have been direct launched using "srun",
129: but OMPI was not built with SLURM's PMI support and therefore cannot
129: execute. There are several options for building PMI support under
129: SLURM, depending upon the SLURM version you are using:
129: 
129:   version 16.05 or later: you can use SLURM's PMIx support. This
129:   requires that you configure and build SLURM --with-pmix.
129: 
129:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
129:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
129:   install PMI-2. You must then build Open MPI using --with-pmi pointing
129:   to the SLURM PMI library location.
129: 
129: Please configure as appropriate and try again.
129: --------------------------------------------------------------------------
129: *** An error occurred in MPI_Init_thread
129: *** on a NULL communicator
129: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
129: ***    and potentially your MPI job)
129: [n130:45872] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
129: [n130:45873] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
129: --------------------------------------------------------------------------
129: The application appears to have been direct launched using "srun",
129: but OMPI was not built with SLURM's PMI support and therefore cannot
129: execute. There are several options for building PMI support under
129: SLURM, depending upon the SLURM version you are using:
129: 
129:   version 16.05 or later: you can use SLURM's PMIx support. This
129:   requires that you configure and build SLURM --with-pmix.
129: 
129:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
129:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
129:   install PMI-2. You must then build Open MPI using --with-pmi pointing
129:   to the SLURM PMI library location.
129: 
129: Please configure as appropriate and try again.
129: --------------------------------------------------------------------------
129: *** An error occurred in MPI_Init_thread
129: *** on a NULL communicator
129: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
129: ***    and potentially your MPI job)
129: [n130:45873] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
129: 
129: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/bsse
129:    inputs: HF.sto-2g.lsym.mol  &  x2c.scf_autoocc.inp
129: 
129: running test: x2c.scf_autoocc HF.sto-2g.lsym
42/92 Test #129: bsse .............................***Failed    4.43 sec
test 131
      Start 131: stex

131: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/stex/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/stex" "--verbose"
131: Test timeout computed to be: 1500
127: 
127: running test with input files ['Ne.dc_sf.2fs.scf_prptra_x-y-z-dipole.inp', 'Rn_Ne-like.mol'] and args None
127: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=Ne.dc_sf.2fs.scf_prptra_x-y-z-dipole.inp', '--mol=Rn_Ne-like.mol']
127: 
127:  **** dirac-executable stderr console output : **** 
127: [n130:45774] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
127: [n130:45773] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
127: --------------------------------------------------------------------------
127: The application appears to have been direct launched using "srun",
127: but OMPI was not built with SLURM's PMI support and therefore cannot
127: execute. There are several options for building PMI support under
127: SLURM, depending upon the SLURM version you are using:
127: 
127:   version 16.05 or later: you can use SLURM's PMIx support. This
127:   requires that you configure and build SLURM --with-pmix.
127: 
127:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
127:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
127:   install PMI-2. You must then build Open MPI using --with-pmi pointing
127:   to the SLURM PMI library location.
127: 
127: Please configure as appropriate and try again.
127: --------------------------------------------------------------------------
127: *** An error occurred in MPI_Init_thread
127: *** on a NULL communicator
127: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
127: ***    and potentially your MPI job)
127: [n130:45774] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
127: --------------------------------------------------------------------------
127: The application appears to have been direct launched using "srun",
127: but OMPI was not built with SLURM's PMI support and therefore cannot
127: execute. There are several options for building PMI support under
127: SLURM, depending upon the SLURM version you are using:
127: 
127:   version 16.05 or later: you can use SLURM's PMIx support. This
127:   requires that you configure and build SLURM --with-pmix.
127: 
127:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
127:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
127:   install PMI-2. You must then build Open MPI using --with-pmi pointing
127:   to the SLURM PMI library location.
127: 
127: Please configure as appropriate and try again.
127: --------------------------------------------------------------------------
127: *** An error occurred in MPI_Init_thread
127: *** on a NULL communicator
127: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
127: ***    and potentially your MPI job)
127: [n130:45773] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
127: 
127: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/operators_mo_mtx_elements
127:    inputs: Rn_Ne-like.mol  &  Ne.dc_sf.2fs.scf_prptra_x-y-z-dipole.inp
127: 
127: running test with input files ['Ne.levyle.2fs.scf_prptra_xy2z3.inp', 'Rn_Ne-like.mol'] and args None
127: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=Ne.levyle.2fs.scf_prptra_xy2z3.inp', '--mol=Rn_Ne-like.mol']
127: 
127:  **** dirac-executable stderr console output : **** 
127: [n130:45905] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
127: --------------------------------------------------------------------------
127: The application appears to have been direct launched using "srun",
127: but OMPI was not built with SLURM's PMI support and therefore cannot
127: execute. There are several options for building PMI support under
127: SLURM, depending upon the SLURM version you are using:
127: 
130: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--put=ScfOrb', '--inp=scf-nr-molcas.inp', '--mol=h2o-cc-pV5Z-nosym.mol']
130: 
130:  **** dirac-executable stderr console output : **** 
130: [n130:45986] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
130: [n130:45987] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
130: --------------------------------------------------------------------------
130: The application appears to have been direct launched using "srun",
130: but OMPI was not built with SLURM's PMI support and therefore cannot
130: execute. There are several options for building PMI support under
130: SLURM, depending upon the SLURM version you are using:
130: 
130:   version 16.05 or later: you can use SLURM's PMIx support. This
130:   requires that you configure and build SLURM --with-pmix.
130: 
130:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
130:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
130:   install PMI-2. You must then build Open MPI using --with-pmi pointing
130:   to the SLURM PMI library location.
130: 
130: Please configure as appropriate and try again.
130: --------------------------------------------------------------------------
130: *** An error occurred in MPI_Init_thread
130: *** on a NULL communicator
130: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
130: ***    and potentially your MPI job)
130: [n130:45986] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
130: --------------------------------------------------------------------------
130: The application appears to have been direct launched using "srun",
130: but OMPI was not built with SLURM's PMI support and therefore cannot
130: execute. There are several options for building PMI support under
130: SLURM, depending upon the SLURM version you are using:
130: 
130:   version 16.05 or later: you can use SLURM's PMIx support. This
130:   requires that you configure and build SLURM --with-pmix.
130: 
130:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
130:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
130:   install PMI-2. You must then build Open MPI using --with-pmi pointing
130:   to the SLURM PMI library location.
130: 
130: Please configure as appropriate and try again.
130: --------------------------------------------------------------------------
130: *** An error occurred in MPI_Init_thread
130: *** on a NULL communicator
130: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
130: ***    and potentially your MPI job)
130: [n130:45987] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
130: 
130: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/import_mos
130:    inputs: h2o-cc-pV5Z-nosym.mol  &  scf-nr-molcas.inp
130: 
130: running test: scf-nr-molcas h2o-cc-pV5Z-nosym
43/92 Test #130: import_mos .......................***Failed    5.03 sec
test 132
      Start 132: zora

132: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/zora/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/zora" "--verbose"
132: Test timeout computed to be: 1500
131: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--outcmo', '--inp=gs.inp', '--mol=LiH-c1.mol']
131: 
131:  **** dirac-executable stderr console output : **** 
131: [n130:46014] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
131: --------------------------------------------------------------------------
131: The application appears to have been direct launched using "srun",
131: but OMPI was not built with SLURM's PMI support and therefore cannot
131: execute. There are several options for building PMI support under
131: SLURM, depending upon the SLURM version you are using:
131: 
131:   version 16.05 or later: you can use SLURM's PMIx support. This
131:   requires that you configure and build SLURM --with-pmix.
131: 
131:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
131:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
131:   install PMI-2. You must then build Open MPI using --with-pmi pointing
131:   to the SLURM PMI library location.
131: 
131: Please configure as appropriate and try again.
131: --------------------------------------------------------------------------
131: *** An error occurred in MPI_Init_thread
131: *** on a NULL communicator
131: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
131: ***    and potentially your MPI job)
131: [n130:46014] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
131: [n130:46015] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
131: --------------------------------------------------------------------------
131: The application appears to have been direct launched using "srun",
131: but OMPI was not built with SLURM's PMI support and therefore cannot
131: execute. There are several options for building PMI support under
131: SLURM, depending upon the SLURM version you are using:
131: 
131:   version 16.05 or later: you can use SLURM's PMIx support. This
131:   requires that you configure and build SLURM --with-pmix.
131: 
131:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
131:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
131:   install PMI-2. You must then build Open MPI using --with-pmi pointing
131:   to the SLURM PMI library location.
131: 
131: Please configure as appropriate and try again.
131: --------------------------------------------------------------------------
131: *** An error occurred in MPI_Init_thread
131: *** on a NULL communicator
131: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
131: ***    and potentially your MPI job)
131: [n130:46015] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
131: 
131: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/stex
131:    inputs: LiH-c1.mol  &  gs.inp
131: 
131: running test: gs LiH-c1
44/92 Test #131: stex .............................***Failed    4.42 sec
test 133
      Start 133: dft_grid_export_import

133: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/dft_grid_export_import/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_grid_export_import" "--verbose"
133: Test timeout computed to be: 1500
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
125: [n130:45844] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: [n130:45845] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
125: [n130:45845] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: 
125: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/ffpt_dipmom_polariz_relcc
125:    inputs: BeH.sto-2g.C2v.mol  &  BeH.x2c_scf_relcc_-0.0005.inp
125: 
125: running test with input files ['BeH.x2c_scf_dipmom.inp', 'BeH.sto-2g.C2v.mol'] and args --noarch --put "DFPCMO.BeH.x2c_scf_sto-2g.C2v=DFPCMO"
125: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=BeH.x2c_scf_dipmom.inp', '--mol=BeH.sto-2g.C2v.mol', '--noarch', '--put', 'DFPCMO.BeH.x2c_scf_sto-2g.C2v=DFPCMO']
125: 
125:  **** dirac-executable stderr console output : **** 
125: [n130:46010] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
127:   version 16.05 or later: you can use SLURM's PMIx support. This
127:   requires that you configure and build SLURM --with-pmix.
127: 
127:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
127:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
127:   install PMI-2. You must then build Open MPI using --with-pmi pointing
127:   to the SLURM PMI library location.
127: 
127: Please configure as appropriate and try again.
127: --------------------------------------------------------------------------
127: *** An error occurred in MPI_Init_thread
127: *** on a NULL communicator
127: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
127: ***    and potentially your MPI job)
127: [n130:45905] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
127: [n130:45904] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
127: --------------------------------------------------------------------------
127: The application appears to have been direct launched using "srun",
127: but OMPI was not built with SLURM's PMI support and therefore cannot
127: execute. There are several options for building PMI support under
127: SLURM, depending upon the SLURM version you are using:
127: 
127:   version 16.05 or later: you can use SLURM's PMIx support. This
127:   requires that you configure and build SLURM --with-pmix.
127: 
127:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
127:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
127:   install PMI-2. You must then build Open MPI using --with-pmi pointing
127:   to the SLURM PMI library location.
127: 
127: Please configure as appropriate and try again.
127: --------------------------------------------------------------------------
127: *** An error occurred in MPI_Init_thread
127: *** on a NULL communicator
127: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
127: ***    and potentially your MPI job)
127: [n130:45904] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
127: 
127: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/operators_mo_mtx_elements
127:    inputs: Rn_Ne-like.mol  &  Ne.levyle.2fs.scf_prptra_xy2z3.inp
127: 
127: running test with input files ['Ne.dc_rkb.2fs.scf_prptra_dsigmadot-nucfield.inp', 'Rn_Ne-like.mol'] and args None
127: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=Ne.dc_rkb.2fs.scf_prptra_dsigmadot-nucfield.inp', '--mol=Rn_Ne-like.mol']
127: 
127:  **** dirac-executable stderr console output : **** 
127: [n130:46040] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
127: --------------------------------------------------------------------------
127: The application appears to have been direct launched using "srun",
127: but OMPI was not built with SLURM's PMI support and therefore cannot
127: execute. There are several options for building PMI support under
127: SLURM, depending upon the SLURM version you are using:
127: 
127:   version 16.05 or later: you can use SLURM's PMIx support. This
127:   requires that you configure and build SLURM --with-pmix.
127: 
127:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
127:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
127:   install PMI-2. You must then build Open MPI using --with-pmi pointing
127:   to the SLURM PMI library location.
127: 
127: Please configure as appropriate and try again.
127: --------------------------------------------------------------------------
127: *** An error occurred in MPI_Init_thread
127: *** on a NULL communicator
127: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
127: ***    and potentially your MPI job)
127: [n130:46040] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
127: [n130:46039] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
132: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=1el.dirac.inp', '--mol=U.dyall.2p.mol']
132: 
132:  **** dirac-executable stderr console output : **** 
132: [n130:46116] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
132: --------------------------------------------------------------------------
132: The application appears to have been direct launched using "srun",
132: but OMPI was not built with SLURM's PMI support and therefore cannot
132: execute. There are several options for building PMI support under
132: SLURM, depending upon the SLURM version you are using:
132: 
132:   version 16.05 or later: you can use SLURM's PMIx support. This
132:   requires that you configure and build SLURM --with-pmix.
132: 
132:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
132:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
132:   install PMI-2. You must then build Open MPI using --with-pmi pointing
132:   to the SLURM PMI library location.
132: 
132: Please configure as appropriate and try again.
132: --------------------------------------------------------------------------
132: *** An error occurred in MPI_Init_thread
132: *** on a NULL communicator
132: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
132: ***    and potentially your MPI job)
132: [n130:46116] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
132: [n130:46118] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
132: --------------------------------------------------------------------------
132: The application appears to have been direct launched using "srun",
132: but OMPI was not built with SLURM's PMI support and therefore cannot
132: execute. There are several options for building PMI support under
132: SLURM, depending upon the SLURM version you are using:
132: 
132:   version 16.05 or later: you can use SLURM's PMIx support. This
132:   requires that you configure and build SLURM --with-pmix.
132: 
132:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
132:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
132:   install PMI-2. You must then build Open MPI using --with-pmi pointing
132:   to the SLURM PMI library location.
132: 
132: Please configure as appropriate and try again.
132: --------------------------------------------------------------------------
132: *** An error occurred in MPI_Init_thread
132: *** on a NULL communicator
132: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
132: ***    and potentially your MPI job)
132: [n130:46118] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
132: 
132: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/zora
132:    inputs: U.dyall.2p.mol  &  1el.dirac.inp
132: 
132: running test: 1el.dirac U.dyall.2p
45/92 Test #132: zora .............................***Failed    4.38 sec
test 134
      Start 134: cosci_average

134: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/cosci_average/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/cosci_average" "--verbose"
134: Test timeout computed to be: 1500
133: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--get', 'numerical_grid', '--inp=export.inp', '--mol=hehe.mol']
133: 
133:  **** dirac-executable stderr console output : **** 
133: [n130:46154] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
133: --------------------------------------------------------------------------
133: The application appears to have been direct launched using "srun",
133: but OMPI was not built with SLURM's PMI support and therefore cannot
133: execute. There are several options for building PMI support under
133: SLURM, depending upon the SLURM version you are using:
133: 
133:   version 16.05 or later: you can use SLURM's PMIx support. This
133:   requires that you configure and build SLURM --with-pmix.
133: 
133:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
133:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
133:   install PMI-2. You must then build Open MPI using --with-pmi pointing
133:   to the SLURM PMI library location.
133: 
133: Please configure as appropriate and try again.
133: --------------------------------------------------------------------------
133: *** An error occurred in MPI_Init_thread
133: *** on a NULL communicator
133: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
133: ***    and potentially your MPI job)
133: [n130:46154] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
133: [n130:46155] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
133: --------------------------------------------------------------------------
133: The application appears to have been direct launched using "srun",
133: but OMPI was not built with SLURM's PMI support and therefore cannot
133: execute. There are several options for building PMI support under
133: SLURM, depending upon the SLURM version you are using:
133: 
133:   version 16.05 or later: you can use SLURM's PMIx support. This
133:   requires that you configure and build SLURM --with-pmix.
133: 
133:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
133:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
133:   install PMI-2. You must then build Open MPI using --with-pmi pointing
133:   to the SLURM PMI library location.
133: 
133: Please configure as appropriate and try again.
133: --------------------------------------------------------------------------
133: *** An error occurred in MPI_Init_thread
133: *** on a NULL communicator
133: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
133: ***    and potentially your MPI job)
133: [n130:46155] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
133: 
133: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_grid_export_import
133:    inputs: hehe.mol  &  export.inp
133: 
133: running test: export hehe
46/92 Test #133: dft_grid_export_import ...........***Failed    4.62 sec
test 135
      Start 135: linear_structures

135: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/linear_structures/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/linear_structures" "--verbose"
135: Test timeout computed to be: 1500
125: [n130:46010] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: [n130:46009] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
125: [n130:46009] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: 
125: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/ffpt_dipmom_polariz_relcc
125:    inputs: BeH.sto-2g.C2v.mol  &  BeH.x2c_scf_dipmom.inp
125: 
125: running test with input files ['LiH.x2c_scf_relcc_analytic-gradient.inp', 'LiH.sto-6g.C2v.mol'] and args None
125: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=LiH.x2c_scf_relcc_analytic-gradient.inp', '--mol=LiH.sto-6g.C2v.mol']
125: 
125:  **** dirac-executable stderr console output : **** 
125: [n130:46156] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
125: [n130:46156] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: [n130:46157] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
127: --------------------------------------------------------------------------
127: The application appears to have been direct launched using "srun",
127: but OMPI was not built with SLURM's PMI support and therefore cannot
127: execute. There are several options for building PMI support under
127: SLURM, depending upon the SLURM version you are using:
127: 
127:   version 16.05 or later: you can use SLURM's PMIx support. This
127:   requires that you configure and build SLURM --with-pmix.
127: 
127:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
127:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
127:   install PMI-2. You must then build Open MPI using --with-pmi pointing
127:   to the SLURM PMI library location.
127: 
127: Please configure as appropriate and try again.
127: --------------------------------------------------------------------------
127: *** An error occurred in MPI_Init_thread
127: *** on a NULL communicator
127: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
127: ***    and potentially your MPI job)
127: [n130:46039] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
127: 
127: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/operators_mo_mtx_elements
127:    inputs: Rn_Ne-like.mol  &  Ne.dc_rkb.2fs.scf_prptra_dsigmadot-nucfield.inp
127: 
127: running test with input files ['Ne.dc_rkb.2fs.scf_prptra_ibetagama_edm.inp', 'Rn_Ne-like.mol'] and args None
127: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=Ne.dc_rkb.2fs.scf_prptra_ibetagama_edm.inp', '--mol=Rn_Ne-like.mol']
127: 
127:  **** dirac-executable stderr console output : **** 
127: [n130:46181] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
127: [n130:46182] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
127: --------------------------------------------------------------------------
127: The application appears to have been direct launched using "srun",
127: but OMPI was not built with SLURM's PMI support and therefore cannot
127: execute. There are several options for building PMI support under
127: SLURM, depending upon the SLURM version you are using:
127: 
127:   version 16.05 or later: you can use SLURM's PMIx support. This
127:   requires that you configure and build SLURM --with-pmix.
127: 
127:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
127:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
127:   install PMI-2. You must then build Open MPI using --with-pmi pointing
127:   to the SLURM PMI library location.
127: 
127: Please configure as appropriate and try again.
127: --------------------------------------------------------------------------
127: *** An error occurred in MPI_Init_thread
127: *** on a NULL communicator
127: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
127: ***    and potentially your MPI job)
127: [n130:46181] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
127: --------------------------------------------------------------------------
127: The application appears to have been direct launched using "srun",
127: but OMPI was not built with SLURM's PMI support and therefore cannot
127: execute. There are several options for building PMI support under
127: SLURM, depending upon the SLURM version you are using:
127: 
127:   version 16.05 or later: you can use SLURM's PMIx support. This
127:   requires that you configure and build SLURM --with-pmix.
127: 
127:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
127:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
127:   install PMI-2. You must then build Open MPI using --with-pmi pointing
127:   to the SLURM PMI library location.
127: 
127: Please configure as appropriate and try again.
127: --------------------------------------------------------------------------
127: *** An error occurred in MPI_Init_thread
127: *** on a NULL communicator
134: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--outcmo', '--inp=V_5+.x2c.scf.inp', '--mol=V.v2z.Dinfh.mol']
134: 
134:  **** dirac-executable stderr console output : **** 
134: [n130:46247] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
134: --------------------------------------------------------------------------
134: The application appears to have been direct launched using "srun",
134: but OMPI was not built with SLURM's PMI support and therefore cannot
134: execute. There are several options for building PMI support under
134: SLURM, depending upon the SLURM version you are using:
134: 
134:   version 16.05 or later: you can use SLURM's PMIx support. This
134:   requires that you configure and build SLURM --with-pmix.
134: 
134:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
134:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
134:   install PMI-2. You must then build Open MPI using --with-pmi pointing
134:   to the SLURM PMI library location.
134: 
134: Please configure as appropriate and try again.
134: --------------------------------------------------------------------------
134: *** An error occurred in MPI_Init_thread
134: *** on a NULL communicator
134: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
134: ***    and potentially your MPI job)
134: [n130:46247] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
134: [n130:46246] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
134: --------------------------------------------------------------------------
134: The application appears to have been direct launched using "srun",
134: but OMPI was not built with SLURM's PMI support and therefore cannot
134: execute. There are several options for building PMI support under
134: SLURM, depending upon the SLURM version you are using:
134: 
134:   version 16.05 or later: you can use SLURM's PMIx support. This
134:   requires that you configure and build SLURM --with-pmix.
134: 
134:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
134:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
134:   install PMI-2. You must then build Open MPI using --with-pmi pointing
134:   to the SLURM PMI library location.
134: 
134: Please configure as appropriate and try again.
134: --------------------------------------------------------------------------
134: *** An error occurred in MPI_Init_thread
134: *** on a NULL communicator
134: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
134: ***    and potentially your MPI job)
134: [n130:46246] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
134: 
134: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/cosci_average
134:    inputs: V.v2z.Dinfh.mol  &  V_5+.x2c.scf.inp
134: 
134: running test: V_5+.x2c.scf V.v2z.Dinfh
47/92 Test #134: cosci_average ....................***Failed    3.68 sec
test 136
      Start 136: gaunt

136: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/gaunt/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/gaunt" "--verbose"
136: Test timeout computed to be: 1500
135: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=x2c.scf.inp', '--mol=HeNe2.lsym1.mol']
135: 
135:  **** dirac-executable stderr console output : **** 
135: [n130:46299] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
135: --------------------------------------------------------------------------
135: The application appears to have been direct launched using "srun",
135: but OMPI was not built with SLURM's PMI support and therefore cannot
135: execute. There are several options for building PMI support under
135: SLURM, depending upon the SLURM version you are using:
135: 
135:   version 16.05 or later: you can use SLURM's PMIx support. This
135:   requires that you configure and build SLURM --with-pmix.
135: 
135:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
135:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
135:   install PMI-2. You must then build Open MPI using --with-pmi pointing
135:   to the SLURM PMI library location.
135: 
135: Please configure as appropriate and try again.
135: --------------------------------------------------------------------------
135: *** An error occurred in MPI_Init_thread
135: *** on a NULL communicator
135: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
135: ***    and potentially your MPI job)
135: [n130:46299] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
135: [n130:46300] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
135: --------------------------------------------------------------------------
135: The application appears to have been direct launched using "srun",
135: but OMPI was not built with SLURM's PMI support and therefore cannot
135: execute. There are several options for building PMI support under
135: SLURM, depending upon the SLURM version you are using:
135: 
135:   version 16.05 or later: you can use SLURM's PMIx support. This
135:   requires that you configure and build SLURM --with-pmix.
135: 
135:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
135:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
135:   install PMI-2. You must then build Open MPI using --with-pmi pointing
135:   to the SLURM PMI library location.
135: 
135: Please configure as appropriate and try again.
135: --------------------------------------------------------------------------
135: *** An error occurred in MPI_Init_thread
135: *** on a NULL communicator
135: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
135: ***    and potentially your MPI job)
135: [n130:46300] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
135: 
135: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/linear_structures
135:    inputs: HeNe2.lsym1.mol  &  x2c.scf.inp
135: 
135: running test: x2c.scf HeNe2.lsym1
48/92 Test #135: linear_structures ................***Failed    4.30 sec
test 137
      Start 137: dft_ac

137: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/dft_ac/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_ac" "--verbose"
137: Test timeout computed to be: 1500
127: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
127: ***    and potentially your MPI job)
127: [n130:46182] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
127: 
127: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/operators_mo_mtx_elements
127:    inputs: Rn_Ne-like.mol  &  Ne.dc_rkb.2fs.scf_prptra_ibetagama_edm.inp
127: 
127: running test with input files ['Ne.dc_rkb.2fs.scf_prptra_alpha_fc.inp', 'Rn_Ne-like.mol'] and args None
127: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=Ne.dc_rkb.2fs.scf_prptra_alpha_fc.inp', '--mol=Rn_Ne-like.mol']
127: 
127:  **** dirac-executable stderr console output : **** 
127: [n130:46324] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
127: [n130:46325] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
127: --------------------------------------------------------------------------
127: The application appears to have been direct launched using "srun",
127: but OMPI was not built with SLURM's PMI support and therefore cannot
127: execute. There are several options for building PMI support under
127: SLURM, depending upon the SLURM version you are using:
127: 
127:   version 16.05 or later: you can use SLURM's PMIx support. This
127:   requires that you configure and build SLURM --with-pmix.
127: 
127:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
127:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
127:   install PMI-2. You must then build Open MPI using --with-pmi pointing
127:   to the SLURM PMI library location.
127: 
127: Please configure as appropriate and try again.
127: --------------------------------------------------------------------------
127: *** An error occurred in MPI_Init_thread
127: *** on a NULL communicator
127: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
127: ***    and potentially your MPI job)
127: [n130:46324] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
127: --------------------------------------------------------------------------
127: The application appears to have been direct launched using "srun",
127: but OMPI was not built with SLURM's PMI support and therefore cannot
127: execute. There are several options for building PMI support under
127: SLURM, depending upon the SLURM version you are using:
127: 
127:   version 16.05 or later: you can use SLURM's PMIx support. This
127:   requires that you configure and build SLURM --with-pmix.
127: 
127:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
127:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
127:   install PMI-2. You must then build Open MPI using --with-pmi pointing
127:   to the SLURM PMI library location.
127: 
127: Please configure as appropriate and try again.
127: --------------------------------------------------------------------------
127: *** An error occurred in MPI_Init_thread
127: *** on a NULL communicator
127: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
127: ***    and potentially your MPI job)
127: [n130:46325] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
127: 
127: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/operators_mo_mtx_elements
127:    inputs: Rn_Ne-like.mol  &  Ne.dc_rkb.2fs.scf_prptra_alpha_fc.inp
49/92 Test #127: operators_mo_mtx_elements ........***Failed   20.64 sec
test 138
      Start 138: dft_betasigma

138: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/dft_betasigma/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_betasigma" "--verbose"
138: Test timeout computed to be: 1500
136: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=gaunt.inp', '--mol=Ne.C2.mol']
136: 
136:  **** dirac-executable stderr console output : **** 
136: [n130:46388] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
136: [n130:46387] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
136: --------------------------------------------------------------------------
136: The application appears to have been direct launched using "srun",
136: but OMPI was not built with SLURM's PMI support and therefore cannot
136: execute. There are several options for building PMI support under
136: SLURM, depending upon the SLURM version you are using:
136: 
136:   version 16.05 or later: you can use SLURM's PMIx support. This
136:   requires that you configure and build SLURM --with-pmix.
136: 
136:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
136:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
136:   install PMI-2. You must then build Open MPI using --with-pmi pointing
136:   to the SLURM PMI library location.
136: 
136: Please configure as appropriate and try again.
136: --------------------------------------------------------------------------
136: *** An error occurred in MPI_Init_thread
136: *** on a NULL communicator
136: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
136: ***    and potentially your MPI job)
136: [n130:46388] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
136: --------------------------------------------------------------------------
136: The application appears to have been direct launched using "srun",
136: but OMPI was not built with SLURM's PMI support and therefore cannot
136: execute. There are several options for building PMI support under
136: SLURM, depending upon the SLURM version you are using:
136: 
136:   version 16.05 or later: you can use SLURM's PMIx support. This
136:   requires that you configure and build SLURM --with-pmix.
136: 
136:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
136:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
136:   install PMI-2. You must then build Open MPI using --with-pmi pointing
136:   to the SLURM PMI library location.
136: 
136: Please configure as appropriate and try again.
136: --------------------------------------------------------------------------
136: *** An error occurred in MPI_Init_thread
136: *** on a NULL communicator
136: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
136: ***    and potentially your MPI job)
136: [n130:46387] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
136: 
136: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/gaunt
136:    inputs: Ne.C2.mol  &  gaunt.inp
136: 
136: running test: gaunt Ne.C2
50/92 Test #136: gaunt ............................***Failed    4.25 sec
test 139
      Start 139: lucita_short

139: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/lucita_short/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/lucita_short" "--verbose"
139: Test timeout computed to be: 1500
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
125: [n130:46157] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: 
125: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/ffpt_dipmom_polariz_relcc
125:    inputs: LiH.sto-6g.C2v.mol  &  LiH.x2c_scf_relcc_analytic-gradient.inp
125: 
125: running test with input files ['LiH.x2c_scf_relcc_orb-unrelaxed_z_-0.0005.inp', 'LiH.sto-6g.C2v.mol'] and args None
125: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=LiH.x2c_scf_relcc_orb-unrelaxed_z_-0.0005.inp', '--mol=LiH.sto-6g.C2v.mol']
125: 
125:  **** dirac-executable stderr console output : **** 
125: [n130:46288] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
125: [n130:46288] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: [n130:46289] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
125: [n130:46289] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: 
125: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/ffpt_dipmom_polariz_relcc
125:    inputs: LiH.sto-6g.C2v.mol  &  LiH.x2c_scf_relcc_orb-unrelaxed_z_-0.0005.inp
125: 
125: running test with input files ['LiH.x2c_scf_relcc_orb-unrelaxed_z_+0.0005.inp', 'LiH.sto-6g.C2v.mol'] and args None
138: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=lda_sigma.inp', '--mol=he.mol']
138: 
138:  **** dirac-executable stderr console output : **** 
138: [n130:46468] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
138: --------------------------------------------------------------------------
138: The application appears to have been direct launched using "srun",
138: but OMPI was not built with SLURM's PMI support and therefore cannot
138: execute. There are several options for building PMI support under
138: SLURM, depending upon the SLURM version you are using:
138: 
138:   version 16.05 or later: you can use SLURM's PMIx support. This
138:   requires that you configure and build SLURM --with-pmix.
138: 
138:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
138:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
138:   install PMI-2. You must then build Open MPI using --with-pmi pointing
138:   to the SLURM PMI library location.
138: 
138: Please configure as appropriate and try again.
138: --------------------------------------------------------------------------
138: *** An error occurred in MPI_Init_thread
138: *** on a NULL communicator
138: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
138: ***    and potentially your MPI job)
138: [n130:46468] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
138: [n130:46467] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
138: --------------------------------------------------------------------------
138: The application appears to have been direct launched using "srun",
138: but OMPI was not built with SLURM's PMI support and therefore cannot
138: execute. There are several options for building PMI support under
138: SLURM, depending upon the SLURM version you are using:
138: 
138:   version 16.05 or later: you can use SLURM's PMIx support. This
138:   requires that you configure and build SLURM --with-pmix.
138: 
138:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
138:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
138:   install PMI-2. You must then build Open MPI using --with-pmi pointing
138:   to the SLURM PMI library location.
138: 
138: Please configure as appropriate and try again.
138: --------------------------------------------------------------------------
138: *** An error occurred in MPI_Init_thread
138: *** on a NULL communicator
138: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
138: ***    and potentially your MPI job)
138: [n130:46467] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
138: 
138: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_betasigma
138:    inputs: he.mol  &  lda_sigma.inp
138: 
138: running test: lda_sigma he
51/92 Test #138: dft_betasigma ....................***Failed    3.58 sec
test 140
      Start 140: projection_analysis_overlaps

140: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/projection_analysis_overlaps/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/projection_analysis_overlaps" "--verbose"
140: Test timeout computed to be: 1500
139: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=He.inp', '--mol=He.mol']
139: 
139:  **** dirac-executable stderr console output : **** 
139: [n130:46531] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
139: [n130:46532] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
139: --------------------------------------------------------------------------
139: The application appears to have been direct launched using "srun",
139: but OMPI was not built with SLURM's PMI support and therefore cannot
139: execute. There are several options for building PMI support under
139: SLURM, depending upon the SLURM version you are using:
139: 
139:   version 16.05 or later: you can use SLURM's PMIx support. This
139:   requires that you configure and build SLURM --with-pmix.
139: 
139:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
139:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
139:   install PMI-2. You must then build Open MPI using --with-pmi pointing
139:   to the SLURM PMI library location.
139: 
139: Please configure as appropriate and try again.
139: --------------------------------------------------------------------------
139: *** An error occurred in MPI_Init_thread
139: *** on a NULL communicator
139: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
139: ***    and potentially your MPI job)
139: [n130:46531] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
139: --------------------------------------------------------------------------
139: The application appears to have been direct launched using "srun",
139: but OMPI was not built with SLURM's PMI support and therefore cannot
139: execute. There are several options for building PMI support under
139: SLURM, depending upon the SLURM version you are using:
139: 
139:   version 16.05 or later: you can use SLURM's PMIx support. This
139:   requires that you configure and build SLURM --with-pmix.
139: 
139:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
139:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
139:   install PMI-2. You must then build Open MPI using --with-pmi pointing
139:   to the SLURM PMI library location.
139: 
139: Please configure as appropriate and try again.
139: --------------------------------------------------------------------------
139: *** An error occurred in MPI_Init_thread
139: *** on a NULL communicator
139: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
139: ***    and potentially your MPI job)
139: [n130:46532] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
139: 
139: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/lucita_short
139:    inputs: He.mol  &  He.inp
139: 
139: running test: He He
52/92 Test #139: lucita_short .....................***Failed    4.21 sec
test 141
      Start 141: lucita_large

141: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/lucita_large/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/lucita_large" "--verbose"
141: Test timeout computed to be: 1500
125: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=LiH.x2c_scf_relcc_orb-unrelaxed_z_+0.0005.inp', '--mol=LiH.sto-6g.C2v.mol']
125: 
125:  **** dirac-executable stderr console output : **** 
125: [n130:46417] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
125: [n130:46417] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: [n130:46418] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
125: [n130:46418] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: 
125: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/ffpt_dipmom_polariz_relcc
125:    inputs: LiH.sto-6g.C2v.mol  &  LiH.x2c_scf_relcc_orb-unrelaxed_z_+0.0005.inp
125: 
125: running test with input files ['LiH.x2c_scf_relcc_orb-unrelaxed_qzz_-0.0005.inp', 'LiH.sto-6g.C2v.mol'] and args None
125: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=LiH.x2c_scf_relcc_orb-unrelaxed_qzz_-0.0005.inp', '--mol=LiH.sto-6g.C2v.mol']
125: 
125:  **** dirac-executable stderr console output : **** 
125: [n130:46559] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
137: 
137: running test with input files ['PBE0gracLB94.inp', 'Ne.mol'] and args None
137: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=PBE0gracLB94.inp', '--mol=Ne.mol']
137: 
137:  **** dirac-executable stderr console output : **** 
137: [n130:46457] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
137: --------------------------------------------------------------------------
137: The application appears to have been direct launched using "srun",
137: but OMPI was not built with SLURM's PMI support and therefore cannot
137: execute. There are several options for building PMI support under
137: SLURM, depending upon the SLURM version you are using:
137: 
137:   version 16.05 or later: you can use SLURM's PMIx support. This
137:   requires that you configure and build SLURM --with-pmix.
137: 
137:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
137:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
137:   install PMI-2. You must then build Open MPI using --with-pmi pointing
137:   to the SLURM PMI library location.
137: 
137: Please configure as appropriate and try again.
137: --------------------------------------------------------------------------
137: *** An error occurred in MPI_Init_thread
137: *** on a NULL communicator
137: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
137: ***    and potentially your MPI job)
137: [n130:46457] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
137: [n130:46458] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
137: --------------------------------------------------------------------------
137: The application appears to have been direct launched using "srun",
137: but OMPI was not built with SLURM's PMI support and therefore cannot
137: execute. There are several options for building PMI support under
137: SLURM, depending upon the SLURM version you are using:
137: 
137:   version 16.05 or later: you can use SLURM's PMIx support. This
137:   requires that you configure and build SLURM --with-pmix.
137: 
137:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
137:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
137:   install PMI-2. You must then build Open MPI using --with-pmi pointing
137:   to the SLURM PMI library location.
137: 
137: Please configure as appropriate and try again.
137: --------------------------------------------------------------------------
137: *** An error occurred in MPI_Init_thread
137: *** on a NULL communicator
137: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
137: ***    and potentially your MPI job)
137: [n130:46458] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
137: 
137: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_ac
137:    inputs: Ne.mol  &  PBE0gracLB94.inp
137: 
137: running test with input files ['GLLBsaopLBalpha.inp', 'Ne.mol'] and args None
137: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=GLLBsaopLBalpha.inp', '--mol=Ne.mol']
137: 
137:  **** dirac-executable stderr console output : **** 
137: [n130:46584] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
137: --------------------------------------------------------------------------
137: The application appears to have been direct launched using "srun",
137: but OMPI was not built with SLURM's PMI support and therefore cannot
137: execute. There are several options for building PMI support under
137: SLURM, depending upon the SLURM version you are using:
137: 
137:   version 16.05 or later: you can use SLURM's PMIx support. This
137:   requires that you configure and build SLURM --with-pmix.
137: 
137:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
137:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
137:   install PMI-2. You must then build Open MPI using --with-pmi pointing
137:   to the SLURM PMI library location.
137: 
137: Please configure as appropriate and try again.
137: --------------------------------------------------------------------------
137: *** An error occurred in MPI_Init_thread
137: *** on a NULL communicator
137: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
137: ***    and potentially your MPI job)
137: [n130:46584] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
137: [n130:46585] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
137: --------------------------------------------------------------------------
137: The application appears to have been direct launched using "srun",
137: but OMPI was not built with SLURM's PMI support and therefore cannot
137: execute. There are several options for building PMI support under
137: SLURM, depending upon the SLURM version you are using:
137: 
137:   version 16.05 or later: you can use SLURM's PMIx support. This
137:   requires that you configure and build SLURM --with-pmix.
137: 
137:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
137:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
137:   install PMI-2. You must then build Open MPI using --with-pmi pointing
137:   to the SLURM PMI library location.
137: 
137: Please configure as appropriate and try again.
137: --------------------------------------------------------------------------
137: *** An error occurred in MPI_Init_thread
137: *** on a NULL communicator
137: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
137: ***    and potentially your MPI job)
137: [n130:46585] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
137: 
137: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_ac
137:    inputs: Ne.mol  &  GLLBsaopLBalpha.inp
53/92 Test #137: dft_ac ...........................***Failed    8.34 sec
test 142
      Start 142: lucita_q_corrections

142: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/lucita_q_corrections/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/lucita_q_corrections" "--verbose"
142: Test timeout computed to be: 1500
140: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--outcmo', '--inp=F.x2c.scf.sto2g.inp', '--mol=F.xyz']
140: 
140:  **** dirac-executable stderr console output : **** 
140: [n130:46610] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
140: --------------------------------------------------------------------------
140: The application appears to have been direct launched using "srun",
140: but OMPI was not built with SLURM's PMI support and therefore cannot
140: execute. There are several options for building PMI support under
140: SLURM, depending upon the SLURM version you are using:
140: 
140:   version 16.05 or later: you can use SLURM's PMIx support. This
140:   requires that you configure and build SLURM --with-pmix.
140: 
140:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
140:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
140:   install PMI-2. You must then build Open MPI using --with-pmi pointing
140:   to the SLURM PMI library location.
140: 
140: Please configure as appropriate and try again.
140: --------------------------------------------------------------------------
140: *** An error occurred in MPI_Init_thread
140: *** on a NULL communicator
140: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
140: ***    and potentially your MPI job)
140: [n130:46610] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
140: [n130:46609] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
140: --------------------------------------------------------------------------
140: The application appears to have been direct launched using "srun",
140: but OMPI was not built with SLURM's PMI support and therefore cannot
140: execute. There are several options for building PMI support under
140: SLURM, depending upon the SLURM version you are using:
140: 
140:   version 16.05 or later: you can use SLURM's PMIx support. This
140:   requires that you configure and build SLURM --with-pmix.
140: 
140:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
140:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
140:   install PMI-2. You must then build Open MPI using --with-pmi pointing
140:   to the SLURM PMI library location.
140: 
140: Please configure as appropriate and try again.
140: --------------------------------------------------------------------------
140: *** An error occurred in MPI_Init_thread
140: *** on a NULL communicator
140: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
140: ***    and potentially your MPI job)
140: [n130:46609] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
140: 
140: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/projection_analysis_overlaps
140:    inputs: F.xyz  &  F.x2c.scf.sto2g.inp
140: 
140: running test: F.x2c.scf.sto2g F
54/92 Test #140: projection_analysis_overlaps .....***Failed    4.73 sec
test 143
      Start 143: basis_contraction

143: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/basis_contraction/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/basis_contraction" "--verbose"
143: Test timeout computed to be: 1500
141: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--mw=60', '--inp=HBr.inp', '--mol=HBr.mol']
141: 
141:  **** dirac-executable stderr console output : **** 
141: [n130:46672] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
141: --------------------------------------------------------------------------
141: The application appears to have been direct launched using "srun",
141: but OMPI was not built with SLURM's PMI support and therefore cannot
141: execute. There are several options for building PMI support under
141: SLURM, depending upon the SLURM version you are using:
141: 
141:   version 16.05 or later: you can use SLURM's PMIx support. This
141:   requires that you configure and build SLURM --with-pmix.
141: 
141:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
141:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
141:   install PMI-2. You must then build Open MPI using --with-pmi pointing
141:   to the SLURM PMI library location.
141: 
141: Please configure as appropriate and try again.
141: --------------------------------------------------------------------------
141: *** An error occurred in MPI_Init_thread
141: *** on a NULL communicator
141: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
141: ***    and potentially your MPI job)
141: [n130:46672] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
141: [n130:46671] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
141: --------------------------------------------------------------------------
141: The application appears to have been direct launched using "srun",
141: but OMPI was not built with SLURM's PMI support and therefore cannot
141: execute. There are several options for building PMI support under
141: SLURM, depending upon the SLURM version you are using:
141: 
141:   version 16.05 or later: you can use SLURM's PMIx support. This
141:   requires that you configure and build SLURM --with-pmix.
141: 
141:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
141:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
141:   install PMI-2. You must then build Open MPI using --with-pmi pointing
141:   to the SLURM PMI library location.
141: 
141: Please configure as appropriate and try again.
141: --------------------------------------------------------------------------
141: *** An error occurred in MPI_Init_thread
141: *** on a NULL communicator
141: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
141: ***    and potentially your MPI job)
141: [n130:46671] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
141: 
141: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/lucita_large
141:    inputs: HBr.mol  &  HBr.inp
141: 
141: running test: HBr HBr
55/92 Test #141: lucita_large .....................***Failed    4.08 sec
test 144
      Start 144: response_nmr_levy-leblond

144: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/response_nmr_levy-leblond/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/response_nmr_levy-leblond" "--verbose"
144: Test timeout computed to be: 1500
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
125: [n130:46559] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: [n130:46560] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
125: [n130:46560] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: 
125: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/ffpt_dipmom_polariz_relcc
125:    inputs: LiH.sto-6g.C2v.mol  &  LiH.x2c_scf_relcc_orb-unrelaxed_qzz_-0.0005.inp
125: 
125: running test with input files ['LiH.x2c_scf_relcc_orb-unrelaxed_qzz_+0.0005.inp', 'LiH.sto-6g.C2v.mol'] and args None
125: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=LiH.x2c_scf_relcc_orb-unrelaxed_qzz_+0.0005.inp', '--mol=LiH.sto-6g.C2v.mol']
125: 
125:  **** dirac-executable stderr console output : **** 
125: [n130:46702] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: [n130:46703] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
125: [n130:46702] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: --------------------------------------------------------------------------
142: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=lucita-q.inp', '--mol=h2o.xyz']
142: 
142:  **** dirac-executable stderr console output : **** 
142: [n130:46732] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
142: --------------------------------------------------------------------------
142: The application appears to have been direct launched using "srun",
142: but OMPI was not built with SLURM's PMI support and therefore cannot
142: execute. There are several options for building PMI support under
142: SLURM, depending upon the SLURM version you are using:
142: 
142:   version 16.05 or later: you can use SLURM's PMIx support. This
142:   requires that you configure and build SLURM --with-pmix.
142: 
142:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
142:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
142:   install PMI-2. You must then build Open MPI using --with-pmi pointing
142:   to the SLURM PMI library location.
142: 
142: Please configure as appropriate and try again.
142: --------------------------------------------------------------------------
142: *** An error occurred in MPI_Init_thread
142: *** on a NULL communicator
142: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
142: ***    and potentially your MPI job)
142: [n130:46732] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
142: [n130:46733] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
142: --------------------------------------------------------------------------
142: The application appears to have been direct launched using "srun",
142: but OMPI was not built with SLURM's PMI support and therefore cannot
142: execute. There are several options for building PMI support under
142: SLURM, depending upon the SLURM version you are using:
142: 
142:   version 16.05 or later: you can use SLURM's PMIx support. This
142:   requires that you configure and build SLURM --with-pmix.
142: 
142:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
142:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
142:   install PMI-2. You must then build Open MPI using --with-pmi pointing
142:   to the SLURM PMI library location.
142: 
142: Please configure as appropriate and try again.
142: --------------------------------------------------------------------------
142: *** An error occurred in MPI_Init_thread
142: *** on a NULL communicator
142: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
142: ***    and potentially your MPI job)
142: [n130:46733] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
142: 
142: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/lucita_q_corrections
142:    inputs: h2o.xyz  &  lucita-q.inp
142: 
142: running test: lucita-q h2o
56/92 Test #142: lucita_q_corrections .............***Failed    4.59 sec
test 145
      Start 145: blockd_twocomp

145: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/blockd_twocomp/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/blockd_twocomp" "--verbose"
145: Test timeout computed to be: 1500
143: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=dhf.inp', '--mol=H2O.large+small.mol']
143: 
143:  **** dirac-executable stderr console output : **** 
143: [n130:46753] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
143: --------------------------------------------------------------------------
143: The application appears to have been direct launched using "srun",
143: but OMPI was not built with SLURM's PMI support and therefore cannot
143: execute. There are several options for building PMI support under
143: SLURM, depending upon the SLURM version you are using:
143: 
143:   version 16.05 or later: you can use SLURM's PMIx support. This
143:   requires that you configure and build SLURM --with-pmix.
143: 
143:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
143:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
143:   install PMI-2. You must then build Open MPI using --with-pmi pointing
143:   to the SLURM PMI library location.
143: 
143: Please configure as appropriate and try again.
143: --------------------------------------------------------------------------
143: *** An error occurred in MPI_Init_thread
143: *** on a NULL communicator
143: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
143: ***    and potentially your MPI job)
143: [n130:46753] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
143: [n130:46752] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
143: --------------------------------------------------------------------------
143: The application appears to have been direct launched using "srun",
143: but OMPI was not built with SLURM's PMI support and therefore cannot
143: execute. There are several options for building PMI support under
143: SLURM, depending upon the SLURM version you are using:
143: 
143:   version 16.05 or later: you can use SLURM's PMIx support. This
143:   requires that you configure and build SLURM --with-pmix.
143: 
143:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
143:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
143:   install PMI-2. You must then build Open MPI using --with-pmi pointing
143:   to the SLURM PMI library location.
143: 
143: Please configure as appropriate and try again.
143: --------------------------------------------------------------------------
143: *** An error occurred in MPI_Init_thread
143: *** on a NULL communicator
143: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
143: ***    and potentially your MPI job)
143: [n130:46752] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
143: 
143: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/basis_contraction
143:    inputs: H2O.large+small.mol  &  dhf.inp
143: 
143: running test: dhf H2O.large+small
57/92 Test #143: basis_contraction ................***Failed    4.51 sec
test 146
      Start 146: dft_cosci

146: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/dft_cosci/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_cosci" "--verbose"
146: Test timeout computed to be: 1500
144: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=levy-leblond.inp', '--mol=H2O.mol']
144: 
144:  **** dirac-executable stderr console output : **** 
144: [n130:46812] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
144: --------------------------------------------------------------------------
144: The application appears to have been direct launched using "srun",
144: but OMPI was not built with SLURM's PMI support and therefore cannot
144: execute. There are several options for building PMI support under
144: SLURM, depending upon the SLURM version you are using:
144: 
144:   version 16.05 or later: you can use SLURM's PMIx support. This
144:   requires that you configure and build SLURM --with-pmix.
144: 
144:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
144:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
144:   install PMI-2. You must then build Open MPI using --with-pmi pointing
144:   to the SLURM PMI library location.
144: 
144: Please configure as appropriate and try again.
144: --------------------------------------------------------------------------
144: *** An error occurred in MPI_Init_thread
144: *** on a NULL communicator
144: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
144: ***    and potentially your MPI job)
144: [n130:46812] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
144: [n130:46813] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
144: --------------------------------------------------------------------------
144: The application appears to have been direct launched using "srun",
144: but OMPI was not built with SLURM's PMI support and therefore cannot
144: execute. There are several options for building PMI support under
144: SLURM, depending upon the SLURM version you are using:
144: 
144:   version 16.05 or later: you can use SLURM's PMIx support. This
144:   requires that you configure and build SLURM --with-pmix.
144: 
144:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
144:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
144:   install PMI-2. You must then build Open MPI using --with-pmi pointing
144:   to the SLURM PMI library location.
144: 
144: Please configure as appropriate and try again.
144: --------------------------------------------------------------------------
144: *** An error occurred in MPI_Init_thread
144: *** on a NULL communicator
144: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
144: ***    and potentially your MPI job)
144: [n130:46813] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
144: 
144: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/response_nmr_levy-leblond
144:    inputs: H2O.mol  &  levy-leblond.inp
144: 
144: running test: levy-leblond H2O
58/92 Test #144: response_nmr_levy-leblond ........***Failed    4.27 sec
test 147
      Start 147: molecular_mean_field_restart

147: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/molecular_mean_field_restart/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/molecular_mean_field_restart" "--verbose"
147: Test timeout computed to be: 1500
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
125: [n130:46703] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: 
125: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/ffpt_dipmom_polariz_relcc
125:    inputs: LiH.sto-6g.C2v.mol  &  LiH.x2c_scf_relcc_orb-unrelaxed_qzz_+0.0005.inp
125: 
125: running test with input files ['LiH.x2c_scf_relcc_orb-relaxed_z_-0.0005.inp', 'LiH.sto-6g.C2v.mol'] and args None
125: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=LiH.x2c_scf_relcc_orb-relaxed_z_-0.0005.inp', '--mol=LiH.sto-6g.C2v.mol']
125: 
125:  **** dirac-executable stderr console output : **** 
125: [n130:46846] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
125: [n130:46846] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: [n130:46845] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
145: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=bss_blockd.ci.inp', '--mol=F.mol']
145: 
145:  **** dirac-executable stderr console output : **** 
145: [n130:46870] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
145: --------------------------------------------------------------------------
145: The application appears to have been direct launched using "srun",
145: but OMPI was not built with SLURM's PMI support and therefore cannot
145: execute. There are several options for building PMI support under
145: SLURM, depending upon the SLURM version you are using:
145: 
145:   version 16.05 or later: you can use SLURM's PMIx support. This
145:   requires that you configure and build SLURM --with-pmix.
145: 
145:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
145:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
145:   install PMI-2. You must then build Open MPI using --with-pmi pointing
145:   to the SLURM PMI library location.
145: 
145: Please configure as appropriate and try again.
145: --------------------------------------------------------------------------
145: *** An error occurred in MPI_Init_thread
145: *** on a NULL communicator
145: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
145: ***    and potentially your MPI job)
145: [n130:46870] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
145: [n130:46871] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
145: --------------------------------------------------------------------------
145: The application appears to have been direct launched using "srun",
145: but OMPI was not built with SLURM's PMI support and therefore cannot
145: execute. There are several options for building PMI support under
145: SLURM, depending upon the SLURM version you are using:
145: 
145:   version 16.05 or later: you can use SLURM's PMIx support. This
145:   requires that you configure and build SLURM --with-pmix.
145: 
145:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
145:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
145:   install PMI-2. You must then build Open MPI using --with-pmi pointing
145:   to the SLURM PMI library location.
145: 
145: Please configure as appropriate and try again.
145: --------------------------------------------------------------------------
145: *** An error occurred in MPI_Init_thread
145: *** on a NULL communicator
145: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
145: ***    and potentially your MPI job)
145: [n130:46871] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
145: 
145: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/blockd_twocomp
145:    inputs: F.mol  &  bss_blockd.ci.inp
145: 
145: running test: bss_blockd.ci F
59/92 Test #145: blockd_twocomp ...................***Failed    4.40 sec
test 148
      Start 148: dirrci_property

148: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/dirrci_property/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dirrci_property" "--verbose"
148: Test timeout computed to be: 1500
146: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=F.x2c.2Paver_bp86_2fs_cosci.inp', '--mol=F.sto-2g.mol']
146: 
146:  **** dirac-executable stderr console output : **** 
146: [n130:46901] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
146: --------------------------------------------------------------------------
146: The application appears to have been direct launched using "srun",
146: but OMPI was not built with SLURM's PMI support and therefore cannot
146: execute. There are several options for building PMI support under
146: SLURM, depending upon the SLURM version you are using:
146: 
146:   version 16.05 or later: you can use SLURM's PMIx support. This
146:   requires that you configure and build SLURM --with-pmix.
146: 
146:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
146:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
146:   install PMI-2. You must then build Open MPI using --with-pmi pointing
146:   to the SLURM PMI library location.
146: 
146: Please configure as appropriate and try again.
146: --------------------------------------------------------------------------
146: *** An error occurred in MPI_Init_thread
146: *** on a NULL communicator
146: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
146: ***    and potentially your MPI job)
146: [n130:46901] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
146: [n130:46902] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
146: --------------------------------------------------------------------------
146: The application appears to have been direct launched using "srun",
146: but OMPI was not built with SLURM's PMI support and therefore cannot
146: execute. There are several options for building PMI support under
146: SLURM, depending upon the SLURM version you are using:
146: 
146:   version 16.05 or later: you can use SLURM's PMIx support. This
146:   requires that you configure and build SLURM --with-pmix.
146: 
146:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
146:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
146:   install PMI-2. You must then build Open MPI using --with-pmi pointing
146:   to the SLURM PMI library location.
146: 
146: Please configure as appropriate and try again.
146: --------------------------------------------------------------------------
146: *** An error occurred in MPI_Init_thread
146: *** on a NULL communicator
146: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
146: ***    and potentially your MPI job)
146: [n130:46902] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
146: 
146: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dft_cosci
146:    inputs: F.sto-2g.mol  &  F.x2c.2Paver_bp86_2fs_cosci.inp
146: 
146: running test: F.x2c.2Paver_bp86_2fs_cosci F.sto-2g
60/92 Test #146: dft_cosci ........................***Failed    4.49 sec
test 149
      Start 149: laplace

149: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/laplace/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/laplace" "--verbose"
149: Test timeout computed to be: 1500
147: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--get=AOMOMAT X2CMAT', '--outcmo', '--inp=scf.inp', '--mol=f2.xyz']
147: 
147:  **** dirac-executable stderr console output : **** 
147: [n130:46956] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
147: [n130:46955] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
147: --------------------------------------------------------------------------
147: The application appears to have been direct launched using "srun",
147: but OMPI was not built with SLURM's PMI support and therefore cannot
147: execute. There are several options for building PMI support under
147: SLURM, depending upon the SLURM version you are using:
147: 
147:   version 16.05 or later: you can use SLURM's PMIx support. This
147:   requires that you configure and build SLURM --with-pmix.
147: 
147:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
147:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
147:   install PMI-2. You must then build Open MPI using --with-pmi pointing
147:   to the SLURM PMI library location.
147: 
147: Please configure as appropriate and try again.
147: --------------------------------------------------------------------------
147: *** An error occurred in MPI_Init_thread
147: *** on a NULL communicator
147: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
147: ***    and potentially your MPI job)
147: [n130:46955] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
147: --------------------------------------------------------------------------
147: The application appears to have been direct launched using "srun",
147: but OMPI was not built with SLURM's PMI support and therefore cannot
147: execute. There are several options for building PMI support under
147: SLURM, depending upon the SLURM version you are using:
147: 
147:   version 16.05 or later: you can use SLURM's PMIx support. This
147:   requires that you configure and build SLURM --with-pmix.
147: 
147:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
147:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
147:   install PMI-2. You must then build Open MPI using --with-pmi pointing
147:   to the SLURM PMI library location.
147: 
147: Please configure as appropriate and try again.
147: --------------------------------------------------------------------------
147: *** An error occurred in MPI_Init_thread
147: *** on a NULL communicator
147: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
147: ***    and potentially your MPI job)
147: [n130:46956] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
147: 
147: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/molecular_mean_field_restart
147:    inputs: f2.xyz  &  scf.inp
147: 
147: running test: scf f2
61/92 Test #147: molecular_mean_field_restart .....***Failed    3.94 sec
test 150
      Start 150: response_complex

150: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/response_complex/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/response_complex" "--verbose"
150: Test timeout computed to be: 1500
125: [n130:46845] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: 
125: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/ffpt_dipmom_polariz_relcc
125:    inputs: LiH.sto-6g.C2v.mol  &  LiH.x2c_scf_relcc_orb-relaxed_z_-0.0005.inp
125: 
125: running test with input files ['LiH.x2c_scf_relcc_orb-relaxed_z_+0.0005.inp', 'LiH.sto-6g.C2v.mol'] and args None
125: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=LiH.x2c_scf_relcc_orb-relaxed_z_+0.0005.inp', '--mol=LiH.sto-6g.C2v.mol']
125: 
125:  **** dirac-executable stderr console output : **** 
125: [n130:46989] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
125: [n130:46989] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: [n130:46988] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
125: --------------------------------------------------------------------------
125: The application appears to have been direct launched using "srun",
125: but OMPI was not built with SLURM's PMI support and therefore cannot
125: execute. There are several options for building PMI support under
125: SLURM, depending upon the SLURM version you are using:
125: 
125:   version 16.05 or later: you can use SLURM's PMIx support. This
125:   requires that you configure and build SLURM --with-pmix.
125: 
125:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
125:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
125:   install PMI-2. You must then build Open MPI using --with-pmi pointing
125:   to the SLURM PMI library location.
125: 
125: Please configure as appropriate and try again.
125: --------------------------------------------------------------------------
125: *** An error occurred in MPI_Init_thread
125: *** on a NULL communicator
125: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
125: ***    and potentially your MPI job)
125: [n130:46988] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
125: 
125: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/ffpt_dipmom_polariz_relcc
125:    inputs: LiH.sto-6g.C2v.mol  &  LiH.x2c_scf_relcc_orb-relaxed_z_+0.0005.inp
62/92 Test #125: ffpt_dipmom_polariz_relcc ........***Failed   43.35 sec
test 151
      Start 151: mp2_energy

151: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/mp2_energy/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/mp2_energy" "--verbose"
151: Test timeout computed to be: 1500
148: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=BeH.inp', '--mol=BeH.mol']
148: 
148:  **** dirac-executable stderr console output : **** 
148: [n130:47013] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
148: --------------------------------------------------------------------------
148: The application appears to have been direct launched using "srun",
148: but OMPI was not built with SLURM's PMI support and therefore cannot
148: execute. There are several options for building PMI support under
148: SLURM, depending upon the SLURM version you are using:
148: 
148:   version 16.05 or later: you can use SLURM's PMIx support. This
148:   requires that you configure and build SLURM --with-pmix.
148: 
148:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
148:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
148:   install PMI-2. You must then build Open MPI using --with-pmi pointing
148:   to the SLURM PMI library location.
148: 
148: Please configure as appropriate and try again.
148: --------------------------------------------------------------------------
148: *** An error occurred in MPI_Init_thread
148: *** on a NULL communicator
148: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
148: ***    and potentially your MPI job)
148: [n130:47013] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
148: [n130:47014] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
148: --------------------------------------------------------------------------
148: The application appears to have been direct launched using "srun",
148: but OMPI was not built with SLURM's PMI support and therefore cannot
148: execute. There are several options for building PMI support under
148: SLURM, depending upon the SLURM version you are using:
148: 
148:   version 16.05 or later: you can use SLURM's PMIx support. This
148:   requires that you configure and build SLURM --with-pmix.
148: 
148:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
148:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
148:   install PMI-2. You must then build Open MPI using --with-pmi pointing
148:   to the SLURM PMI library location.
148: 
148: Please configure as appropriate and try again.
148: --------------------------------------------------------------------------
148: *** An error occurred in MPI_Init_thread
148: *** on a NULL communicator
148: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
148: ***    and potentially your MPI job)
148: [n130:47014] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
148: 
148: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dirrci_property
148:    inputs: BeH.mol  &  BeH.inp
148: 
148: running test: BeH BeH
63/92 Test #148: dirrci_property ..................***Failed    3.31 sec
test 152
      Start 152: dirac_mointegral_export

152: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/dirac_mointegral_export/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dirac_mointegral_export" "--verbose"
152: Test timeout computed to be: 1500
149: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=lap_fix_number.inp', '--mol=h2o.mol']
149: 
149:  **** dirac-executable stderr console output : **** 
149: [n130:47046] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
149: --------------------------------------------------------------------------
149: The application appears to have been direct launched using "srun",
149: but OMPI was not built with SLURM's PMI support and therefore cannot
149: execute. There are several options for building PMI support under
149: SLURM, depending upon the SLURM version you are using:
149: 
149:   version 16.05 or later: you can use SLURM's PMIx support. This
149:   requires that you configure and build SLURM --with-pmix.
149: 
149:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
149:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
149:   install PMI-2. You must then build Open MPI using --with-pmi pointing
149:   to the SLURM PMI library location.
149: 
149: Please configure as appropriate and try again.
149: --------------------------------------------------------------------------
149: *** An error occurred in MPI_Init_thread
149: *** on a NULL communicator
149: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
149: ***    and potentially your MPI job)
149: [n130:47046] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
149: [n130:47045] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
149: --------------------------------------------------------------------------
149: The application appears to have been direct launched using "srun",
149: but OMPI was not built with SLURM's PMI support and therefore cannot
149: execute. There are several options for building PMI support under
149: SLURM, depending upon the SLURM version you are using:
149: 
149:   version 16.05 or later: you can use SLURM's PMIx support. This
149:   requires that you configure and build SLURM --with-pmix.
149: 
149:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
149:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
149:   install PMI-2. You must then build Open MPI using --with-pmi pointing
149:   to the SLURM PMI library location.
149: 
149: Please configure as appropriate and try again.
149: --------------------------------------------------------------------------
149: *** An error occurred in MPI_Init_thread
149: *** on a NULL communicator
149: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
149: ***    and potentially your MPI job)
149: [n130:47045] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
149: 
149: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/laplace
149:    inputs: h2o.mol  &  lap_fix_number.inp
149: 
149: running test: lap_fix_number h2o
64/92 Test #149: laplace ..........................***Failed    3.11 sec
test 153
      Start 153: response_rkbimp_shield

153: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/response_rkbimp_shield/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/response_rkbimp_shield" "--verbose"
153: Test timeout computed to be: 1500
150: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=cpp.inp', '--mol=LiH.mol']
150: 
150:  **** dirac-executable stderr console output : **** 
150: [n130:47106] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
150: --------------------------------------------------------------------------
150: The application appears to have been direct launched using "srun",
150: but OMPI was not built with SLURM's PMI support and therefore cannot
150: execute. There are several options for building PMI support under
150: SLURM, depending upon the SLURM version you are using:
150: 
150:   version 16.05 or later: you can use SLURM's PMIx support. This
150:   requires that you configure and build SLURM --with-pmix.
150: 
150:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
150:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
150:   install PMI-2. You must then build Open MPI using --with-pmi pointing
150:   to the SLURM PMI library location.
150: 
150: Please configure as appropriate and try again.
150: --------------------------------------------------------------------------
150: *** An error occurred in MPI_Init_thread
150: *** on a NULL communicator
150: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
150: ***    and potentially your MPI job)
150: [n130:47106] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
150: [n130:47107] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
150: --------------------------------------------------------------------------
150: The application appears to have been direct launched using "srun",
150: but OMPI was not built with SLURM's PMI support and therefore cannot
150: execute. There are several options for building PMI support under
150: SLURM, depending upon the SLURM version you are using:
150: 
150:   version 16.05 or later: you can use SLURM's PMIx support. This
150:   requires that you configure and build SLURM --with-pmix.
150: 
150:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
150:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
150:   install PMI-2. You must then build Open MPI using --with-pmi pointing
150:   to the SLURM PMI library location.
150: 
150: Please configure as appropriate and try again.
150: --------------------------------------------------------------------------
150: *** An error occurred in MPI_Init_thread
150: *** on a NULL communicator
150: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
150: ***    and potentially your MPI job)
150: [n130:47107] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
150: 
150: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/response_complex
150:    inputs: LiH.mol  &  cpp.inp
150: 
150: running test: cpp LiH
65/92 Test #150: response_complex .................***Failed    3.88 sec
test 154
      Start 154: free-particle_projection

154: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/free-particle_projection/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/free-particle_projection" "--verbose"
154: Test timeout computed to be: 1500
151: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=MP2.inp', '--mol=H2O.mol']
151: 
151:  **** dirac-executable stderr console output : **** 
151: [n130:47132] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
151: --------------------------------------------------------------------------
151: The application appears to have been direct launched using "srun",
151: but OMPI was not built with SLURM's PMI support and therefore cannot
151: execute. There are several options for building PMI support under
151: SLURM, depending upon the SLURM version you are using:
151: 
151:   version 16.05 or later: you can use SLURM's PMIx support. This
151:   requires that you configure and build SLURM --with-pmix.
151: 
151:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
151:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
151:   install PMI-2. You must then build Open MPI using --with-pmi pointing
151:   to the SLURM PMI library location.
151: 
151: Please configure as appropriate and try again.
151: --------------------------------------------------------------------------
151: *** An error occurred in MPI_Init_thread
151: *** on a NULL communicator
151: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
151: ***    and potentially your MPI job)
151: [n130:47132] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
151: [n130:47133] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
151: --------------------------------------------------------------------------
151: The application appears to have been direct launched using "srun",
151: but OMPI was not built with SLURM's PMI support and therefore cannot
151: execute. There are several options for building PMI support under
151: SLURM, depending upon the SLURM version you are using:
151: 
151:   version 16.05 or later: you can use SLURM's PMIx support. This
151:   requires that you configure and build SLURM --with-pmix.
151: 
151:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
151:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
151:   install PMI-2. You must then build Open MPI using --with-pmi pointing
151:   to the SLURM PMI library location.
151: 
151: Please configure as appropriate and try again.
151: --------------------------------------------------------------------------
151: *** An error occurred in MPI_Init_thread
151: *** on a NULL communicator
151: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
151: ***    and potentially your MPI job)
151: [n130:47133] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
151: 
151: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/mp2_energy
151:    inputs: H2O.mol  &  MP2.inp
151: 
151: running test: MP2 H2O
66/92 Test #151: mp2_energy .......................***Failed    4.21 sec
test 155
      Start 155: atomic_start

155: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/atomic_start/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/atomic_start" "--verbose"
155: Test timeout computed to be: 1500
153: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--outcmo', '--inp=rkb.inp', '--mol=He.mol']
153: 
153:  **** dirac-executable stderr console output : **** 
153: [n130:47180] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
153: --------------------------------------------------------------------------
153: The application appears to have been direct launched using "srun",
153: but OMPI was not built with SLURM's PMI support and therefore cannot
153: execute. There are several options for building PMI support under
153: SLURM, depending upon the SLURM version you are using:
153: 
153:   version 16.05 or later: you can use SLURM's PMIx support. This
153:   requires that you configure and build SLURM --with-pmix.
153: 
153:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
153:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
153:   install PMI-2. You must then build Open MPI using --with-pmi pointing
153:   to the SLURM PMI library location.
153: 
153: Please configure as appropriate and try again.
153: --------------------------------------------------------------------------
153: *** An error occurred in MPI_Init_thread
153: *** on a NULL communicator
153: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
153: ***    and potentially your MPI job)
153: [n130:47180] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
153: [n130:47179] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
153: --------------------------------------------------------------------------
153: The application appears to have been direct launched using "srun",
153: but OMPI was not built with SLURM's PMI support and therefore cannot
153: execute. There are several options for building PMI support under
153: SLURM, depending upon the SLURM version you are using:
153: 
153:   version 16.05 or later: you can use SLURM's PMIx support. This
153:   requires that you configure and build SLURM --with-pmix.
153: 
153:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
153:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
153:   install PMI-2. You must then build Open MPI using --with-pmi pointing
153:   to the SLURM PMI library location.
153: 
153: Please configure as appropriate and try again.
153: --------------------------------------------------------------------------
153: *** An error occurred in MPI_Init_thread
153: *** on a NULL communicator
153: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
153: ***    and potentially your MPI job)
153: [n130:47179] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
153: 
153: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/response_rkbimp_shield
153:    inputs: He.mol  &  rkb.inp
153: 
153: running test: rkb He
67/92 Test #153: response_rkbimp_shield ...........***Failed    4.60 sec
test 156
      Start 156: checkpoint_from_Dformat

156: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/checkpoint_from_Dformat/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/checkpoint_from_Dformat" "--verbose"
156: Test timeout computed to be: 1500
152: ERROR: crash during ['/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac_mointegral_export.x', 'fcidump']
152: forrtl: severe (24): end-of-file during read, unit 21, file /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dirac_mointegral_export/MRCONEE
152: Image              PC                Routine            Line        Source             
152: dirac_mointegral_  00000000023C79E9  Unknown               Unknown  Unknown
152: dirac_mointegral_  00000000023FF716  Unknown               Unknown  Unknown
152: dirac_mointegral_  00000000005ACAB3  dirac_mointegral_          79  dirac_mointegral_export.F90
152: dirac_mointegral_  00000000005A5607  MAIN__                    700  dirac_mointegral_export.F90
152: dirac_mointegral_  00000000005A5502  Unknown               Unknown  Unknown
152: libc-2.17.so       00002AF761FD5555  __libc_start_main     Unknown  Unknown
152: dirac_mointegral_  00000000005A5419  Unknown               Unknown  Unknown
152: 
152: running test with input files ['BeH.x2c_scf_relcc.inp', 'BeH.sto-2g.lsym.mol'] and args --get="MDCINT MRCONEE"
152: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=BeH.x2c_scf_relcc.inp', '--mol=BeH.sto-2g.lsym.mol', '--get=MDCINT MRCONEE']
152: 
152:  **** dirac-executable stderr console output : **** 
152: [n130:47185] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
152: --------------------------------------------------------------------------
152: The application appears to have been direct launched using "srun",
152: but OMPI was not built with SLURM's PMI support and therefore cannot
152: execute. There are several options for building PMI support under
152: SLURM, depending upon the SLURM version you are using:
152: 
152:   version 16.05 or later: you can use SLURM's PMIx support. This
152:   requires that you configure and build SLURM --with-pmix.
152: 
152:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
152:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
152:   install PMI-2. You must then build Open MPI using --with-pmi pointing
152:   to the SLURM PMI library location.
152: 
152: Please configure as appropriate and try again.
152: --------------------------------------------------------------------------
152: *** An error occurred in MPI_Init_thread
152: *** on a NULL communicator
152: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
152: ***    and potentially your MPI job)
152: [n130:47185] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
152: [n130:47184] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
152: --------------------------------------------------------------------------
152: The application appears to have been direct launched using "srun",
152: but OMPI was not built with SLURM's PMI support and therefore cannot
152: execute. There are several options for building PMI support under
152: SLURM, depending upon the SLURM version you are using:
152: 
152:   version 16.05 or later: you can use SLURM's PMIx support. This
152:   requires that you configure and build SLURM --with-pmix.
152: 
152:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
152:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
152:   install PMI-2. You must then build Open MPI using --with-pmi pointing
152:   to the SLURM PMI library location.
152: 
152: Please configure as appropriate and try again.
152: --------------------------------------------------------------------------
152: *** An error occurred in MPI_Init_thread
152: *** on a NULL communicator
152: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
152: ***    and potentially your MPI job)
152: [n130:47184] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
152: 
152: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/dirac_mointegral_export
152:    inputs: BeH.sto-2g.lsym.mol  &  BeH.x2c_scf_relcc.inp
152: 
152: running test:  "/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac_mointegral_export.x" fcidump  
68/92 Test #152: dirac_mointegral_export ..........***Failed    4.87 sec
test 157
      Start 157: tutorial_checkpoint

157: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/tutorial_checkpoint/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/tutorial_checkpoint" "--verbose"
157: Test timeout computed to be: 1500
154: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=std.inp', '--mol=Hg.mol']
154: 
154:  **** dirac-executable stderr console output : **** 
154: [n130:47232] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
154: --------------------------------------------------------------------------
154: The application appears to have been direct launched using "srun",
154: but OMPI was not built with SLURM's PMI support and therefore cannot
154: execute. There are several options for building PMI support under
154: SLURM, depending upon the SLURM version you are using:
154: 
154:   version 16.05 or later: you can use SLURM's PMIx support. This
154:   requires that you configure and build SLURM --with-pmix.
154: 
154:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
154:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
154:   install PMI-2. You must then build Open MPI using --with-pmi pointing
154:   to the SLURM PMI library location.
154: 
154: Please configure as appropriate and try again.
154: --------------------------------------------------------------------------
154: *** An error occurred in MPI_Init_thread
154: *** on a NULL communicator
154: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
154: ***    and potentially your MPI job)
154: [n130:47232] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
154: [n130:47231] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
154: --------------------------------------------------------------------------
154: The application appears to have been direct launched using "srun",
154: but OMPI was not built with SLURM's PMI support and therefore cannot
154: execute. There are several options for building PMI support under
154: SLURM, depending upon the SLURM version you are using:
154: 
154:   version 16.05 or later: you can use SLURM's PMIx support. This
154:   requires that you configure and build SLURM --with-pmix.
154: 
154:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
154:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
154:   install PMI-2. You must then build Open MPI using --with-pmi pointing
154:   to the SLURM PMI library location.
154: 
154: Please configure as appropriate and try again.
154: --------------------------------------------------------------------------
154: *** An error occurred in MPI_Init_thread
154: *** on a NULL communicator
154: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
154: ***    and potentially your MPI job)
154: [n130:47231] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
154: 
154: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/free-particle_projection
154:    inputs: Hg.mol  &  std.inp
154: 
154: running test: std Hg
69/92 Test #154: free-particle_projection .........***Failed    3.76 sec
test 158
      Start 158: cosci_energy_blockd

158: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/cosci_energy_blockd/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/cosci_energy_blockd" "--verbose"
158: Test timeout computed to be: 1500
155: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--outcmo', '--inp=H.inp', '--mol=H.mol']
155: 
155:  **** dirac-executable stderr console output : **** 
155: [n130:47280] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
155: --------------------------------------------------------------------------
155: The application appears to have been direct launched using "srun",
155: but OMPI was not built with SLURM's PMI support and therefore cannot
155: execute. There are several options for building PMI support under
155: SLURM, depending upon the SLURM version you are using:
155: 
155:   version 16.05 or later: you can use SLURM's PMIx support. This
155:   requires that you configure and build SLURM --with-pmix.
155: 
155:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
155:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
155:   install PMI-2. You must then build Open MPI using --with-pmi pointing
155:   to the SLURM PMI library location.
155: 
155: Please configure as appropriate and try again.
155: --------------------------------------------------------------------------
155: *** An error occurred in MPI_Init_thread
155: *** on a NULL communicator
155: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
155: ***    and potentially your MPI job)
155: [n130:47280] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
155: [n130:47281] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
155: --------------------------------------------------------------------------
155: The application appears to have been direct launched using "srun",
155: but OMPI was not built with SLURM's PMI support and therefore cannot
155: execute. There are several options for building PMI support under
155: SLURM, depending upon the SLURM version you are using:
155: 
155:   version 16.05 or later: you can use SLURM's PMIx support. This
155:   requires that you configure and build SLURM --with-pmix.
155: 
155:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
155:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
155:   install PMI-2. You must then build Open MPI using --with-pmi pointing
155:   to the SLURM PMI library location.
155: 
155: Please configure as appropriate and try again.
155: --------------------------------------------------------------------------
155: *** An error occurred in MPI_Init_thread
155: *** on a NULL communicator
155: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
155: ***    and potentially your MPI job)
155: [n130:47281] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
155: 
155: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/atomic_start
155:    inputs: H.mol  &  H.inp
155: 
155: running test: H H
70/92 Test #155: atomic_start .....................***Failed    4.41 sec
test 159
      Start 159: visual_elf

159: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/visual_elf/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_elf" "--verbose"
159: Test timeout computed to be: 1500
156: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=hf.inp', '--mol=h2.mol']
156: 
156:  **** dirac-executable stderr console output : **** 
156: [n130:47327] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
156: --------------------------------------------------------------------------
156: The application appears to have been direct launched using "srun",
156: but OMPI was not built with SLURM's PMI support and therefore cannot
156: execute. There are several options for building PMI support under
156: SLURM, depending upon the SLURM version you are using:
156: 
156:   version 16.05 or later: you can use SLURM's PMIx support. This
156:   requires that you configure and build SLURM --with-pmix.
156: 
156:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
156:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
156:   install PMI-2. You must then build Open MPI using --with-pmi pointing
156:   to the SLURM PMI library location.
156: 
156: Please configure as appropriate and try again.
156: --------------------------------------------------------------------------
156: *** An error occurred in MPI_Init_thread
156: *** on a NULL communicator
156: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
156: ***    and potentially your MPI job)
156: [n130:47327] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
156: [n130:47328] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
156: --------------------------------------------------------------------------
156: The application appears to have been direct launched using "srun",
156: but OMPI was not built with SLURM's PMI support and therefore cannot
156: execute. There are several options for building PMI support under
156: SLURM, depending upon the SLURM version you are using:
156: 
156:   version 16.05 or later: you can use SLURM's PMIx support. This
156:   requires that you configure and build SLURM --with-pmix.
156: 
156:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
156:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
156:   install PMI-2. You must then build Open MPI using --with-pmi pointing
156:   to the SLURM PMI library location.
156: 
156: Please configure as appropriate and try again.
156: --------------------------------------------------------------------------
156: *** An error occurred in MPI_Init_thread
156: *** on a NULL communicator
156: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
156: ***    and potentially your MPI job)
156: [n130:47328] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
156: 
156: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/checkpoint_from_Dformat
156:    inputs: h2.mol  &  hf.inp
156: 
156: running test: hf h2
71/92 Test #156: checkpoint_from_Dformat ..........***Failed    4.67 sec
test 160
      Start 160: visual_density_derivs

160: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/visual_density_derivs/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_density_derivs" "--verbose"
160: Test timeout computed to be: 1500
157: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=scf.inp', '--mol=hf.xyz']
157: 
157:  **** dirac-executable stderr console output : **** 
157: [n130:47338] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
157: --------------------------------------------------------------------------
157: The application appears to have been direct launched using "srun",
157: but OMPI was not built with SLURM's PMI support and therefore cannot
157: execute. There are several options for building PMI support under
157: SLURM, depending upon the SLURM version you are using:
157: 
157:   version 16.05 or later: you can use SLURM's PMIx support. This
157:   requires that you configure and build SLURM --with-pmix.
157: 
157:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
157:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
157:   install PMI-2. You must then build Open MPI using --with-pmi pointing
157:   to the SLURM PMI library location.
157: 
157: Please configure as appropriate and try again.
157: --------------------------------------------------------------------------
157: *** An error occurred in MPI_Init_thread
157: *** on a NULL communicator
157: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
157: ***    and potentially your MPI job)
157: [n130:47338] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
157: [n130:47339] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
157: --------------------------------------------------------------------------
157: The application appears to have been direct launched using "srun",
157: but OMPI was not built with SLURM's PMI support and therefore cannot
157: execute. There are several options for building PMI support under
157: SLURM, depending upon the SLURM version you are using:
157: 
157:   version 16.05 or later: you can use SLURM's PMIx support. This
157:   requires that you configure and build SLURM --with-pmix.
157: 
157:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
157:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
157:   install PMI-2. You must then build Open MPI using --with-pmi pointing
157:   to the SLURM PMI library location.
157: 
157: Please configure as appropriate and try again.
157: --------------------------------------------------------------------------
157: *** An error occurred in MPI_Init_thread
157: *** on a NULL communicator
157: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
157: ***    and potentially your MPI job)
157: [n130:47339] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
157: 
157: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/tutorial_checkpoint
157:    inputs: hf.xyz  &  scf.inp
157: 
157: running test: scf hf
72/92 Test #157: tutorial_checkpoint ..............***Failed    4.55 sec
test 161
      Start 161: visual_custom_output

161: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/visual_custom_output/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_custom_output" "--verbose"
161: Test timeout computed to be: 1500
158: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=bss_rkb_blockd.ci.inp', '--mol=F.mol']
158: 
158:  **** dirac-executable stderr console output : **** 
158: [n130:47374] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
158: [n130:47373] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
158: --------------------------------------------------------------------------
158: The application appears to have been direct launched using "srun",
158: but OMPI was not built with SLURM's PMI support and therefore cannot
158: execute. There are several options for building PMI support under
158: SLURM, depending upon the SLURM version you are using:
158: 
158:   version 16.05 or later: you can use SLURM's PMIx support. This
158:   requires that you configure and build SLURM --with-pmix.
158: 
158:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
158:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
158:   install PMI-2. You must then build Open MPI using --with-pmi pointing
158:   to the SLURM PMI library location.
158: 
158: Please configure as appropriate and try again.
158: --------------------------------------------------------------------------
158: *** An error occurred in MPI_Init_thread
158: *** on a NULL communicator
158: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
158: ***    and potentially your MPI job)
158: [n130:47374] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
158: --------------------------------------------------------------------------
158: The application appears to have been direct launched using "srun",
158: but OMPI was not built with SLURM's PMI support and therefore cannot
158: execute. There are several options for building PMI support under
158: SLURM, depending upon the SLURM version you are using:
158: 
158:   version 16.05 or later: you can use SLURM's PMIx support. This
158:   requires that you configure and build SLURM --with-pmix.
158: 
158:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
158:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
158:   install PMI-2. You must then build Open MPI using --with-pmi pointing
158:   to the SLURM PMI library location.
158: 
158: Please configure as appropriate and try again.
158: --------------------------------------------------------------------------
158: *** An error occurred in MPI_Init_thread
158: *** on a NULL communicator
158: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
158: ***    and potentially your MPI job)
158: [n130:47373] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
158: 
158: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/cosci_energy_blockd
158:    inputs: F.mol  &  bss_rkb_blockd.ci.inp
158: 
158: running test: bss_rkb_blockd.ci F
73/92 Test #158: cosci_energy_blockd ..............***Failed    4.24 sec
test 162
      Start 162: polprp_ph

162: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/polprp_ph/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/polprp_ph" "--verbose"
162: Test timeout computed to be: 1500
159: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=elf.inp', '--mol=ar.mol']
159: 
159:  **** dirac-executable stderr console output : **** 
159: [n130:47418] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
159: --------------------------------------------------------------------------
159: The application appears to have been direct launched using "srun",
159: but OMPI was not built with SLURM's PMI support and therefore cannot
159: execute. There are several options for building PMI support under
159: SLURM, depending upon the SLURM version you are using:
159: 
159:   version 16.05 or later: you can use SLURM's PMIx support. This
159:   requires that you configure and build SLURM --with-pmix.
159: 
159:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
159:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
159:   install PMI-2. You must then build Open MPI using --with-pmi pointing
159:   to the SLURM PMI library location.
159: 
159: Please configure as appropriate and try again.
159: --------------------------------------------------------------------------
159: *** An error occurred in MPI_Init_thread
159: *** on a NULL communicator
159: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
159: ***    and potentially your MPI job)
159: [n130:47418] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
159: [n130:47419] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
159: --------------------------------------------------------------------------
159: The application appears to have been direct launched using "srun",
159: but OMPI was not built with SLURM's PMI support and therefore cannot
159: execute. There are several options for building PMI support under
159: SLURM, depending upon the SLURM version you are using:
159: 
159:   version 16.05 or later: you can use SLURM's PMIx support. This
159:   requires that you configure and build SLURM --with-pmix.
159: 
159:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
159:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
159:   install PMI-2. You must then build Open MPI using --with-pmi pointing
159:   to the SLURM PMI library location.
159: 
159: Please configure as appropriate and try again.
159: --------------------------------------------------------------------------
159: *** An error occurred in MPI_Init_thread
159: *** on a NULL communicator
159: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
159: ***    and potentially your MPI job)
159: [n130:47419] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
159: 
159: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_elf
159:    inputs: ar.mol  &  elf.inp
159: 
159: running test: elf ar
74/92 Test #159: visual_elf .......................***Failed    3.94 sec
test 163
      Start 163: pam_test

163: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/pam_test/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/pam_test" "--verbose"
163: Test timeout computed to be: 1500
163: 
163:   DIRAC pam script running:
163: 
163:   user           : milias
163:   machine        : n130
163:   date and time  : 2023-07-24 18:37:57.993526
163:   input dir      : /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4
163:   pam command    : /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam
163:   all pam args   : ['--show']
163:   executable     : /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x
163:   scratch dir    : /scratch/p175-23-t/milias/DIRAC_dirac_output.47503_47503 (availspace=282440.410[GB])
163:   output file    : dirac_output.47503.out
163:   DIRAC run      : parallel (launcher: /storage-apps/easybuild/software/impi/2021.6.0-intel-compilers-2022.1.0/mpi/2021.6.0/bin/mpirun)
163:   local disks    : False
163:   rsh/rcp        : ssh / scp
163:   machine file   : None
163: 
163:   Setting MKL and OPENMP environment to default values (if not set already)
163:    Variabl.MKL_NUM_THREADS =  1
163:    Variabl.MKL_DYNAMIC =  FALSE
163:    Variabl.OMP_NUM_THREADS =  1
163:    Variabl.OMP_DYNAMIC= FALSE
163:   Setting environment variables (as specified explicitly or in .diracrc)
163: Operating system:      Linux
163: Current settings:
163:   Basis directories    /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4:.:/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/basis:/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/basis_dalton:/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/basis_ecp
163:   Scratch directory    /scratch/p175-23-t/milias/DIRAC_dirac_output.47503_47503 (avail=282440.410[GB])
163:   Relevant for parallel builds:
163:     mpi launcher          /storage-apps/easybuild/software/impi/2021.6.0-intel-compilers-2022.1.0/mpi/2021.6.0/bin/mpirun
163:     mpirun extra args     None
163:     global scratch disk   True
163:     Machine file          None
163:     own mpirun command    None
163:   Profiler             None
163:   Debugger             None
163:   Dirac command        None
163:   Dirac executable     /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x
161: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--get=PAMXVC TBMO', '--outcmo', '--inp=dc.inp', '--mol=h2o.mol']
161: 
161:  **** dirac-executable stderr console output : **** 
161: [n130:47479] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
161: [n130:47480] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
161: --------------------------------------------------------------------------
161: The application appears to have been direct launched using "srun",
161: but OMPI was not built with SLURM's PMI support and therefore cannot
161: execute. There are several options for building PMI support under
161: SLURM, depending upon the SLURM version you are using:
161: 
161:   version 16.05 or later: you can use SLURM's PMIx support. This
161:   requires that you configure and build SLURM --with-pmix.
161: 
161:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
161:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
161:   install PMI-2. You must then build Open MPI using --with-pmi pointing
161:   to the SLURM PMI library location.
161: 
161: Please configure as appropriate and try again.
161: --------------------------------------------------------------------------
161: *** An error occurred in MPI_Init_thread
161: *** on a NULL communicator
161: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
161: ***    and potentially your MPI job)
161: [n130:47479] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
161: --------------------------------------------------------------------------
161: The application appears to have been direct launched using "srun",
161: but OMPI was not built with SLURM's PMI support and therefore cannot
161: execute. There are several options for building PMI support under
161: SLURM, depending upon the SLURM version you are using:
161: 
161:   version 16.05 or later: you can use SLURM's PMIx support. This
161:   requires that you configure and build SLURM --with-pmix.
161: 
161:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
161:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
161:   install PMI-2. You must then build Open MPI using --with-pmi pointing
161:   to the SLURM PMI library location.
161: 
161: Please configure as appropriate and try again.
161: --------------------------------------------------------------------------
161: *** An error occurred in MPI_Init_thread
161: *** on a NULL communicator
161: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
161: ***    and potentially your MPI job)
161: [n130:47480] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
161: 
161: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_custom_output
161:    inputs: h2o.mol  &  dc.inp
161: 
161: running test: dc h2o
75/92 Test #161: visual_custom_output .............***Failed    4.46 sec
test 164
      Start 164: basis_input_scripted

164: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/basis_input_scripted/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/basis_input_scripted" "--verbose"
164: Test timeout computed to be: 3600
160: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--outcmo', '--inp=response_ez_ez_dc.inp', '--mol=ne_d2h.mol']
160: 
160:  **** dirac-executable stderr console output : **** 
160: [n130:47486] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
160: [n130:47485] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
160: --------------------------------------------------------------------------
160: The application appears to have been direct launched using "srun",
160: but OMPI was not built with SLURM's PMI support and therefore cannot
160: execute. There are several options for building PMI support under
160: SLURM, depending upon the SLURM version you are using:
160: 
160:   version 16.05 or later: you can use SLURM's PMIx support. This
160:   requires that you configure and build SLURM --with-pmix.
160: 
160:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
160:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
160:   install PMI-2. You must then build Open MPI using --with-pmi pointing
160:   to the SLURM PMI library location.
160: 
160: Please configure as appropriate and try again.
160: --------------------------------------------------------------------------
160: *** An error occurred in MPI_Init_thread
160: *** on a NULL communicator
160: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
160: ***    and potentially your MPI job)
160: [n130:47485] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
160: --------------------------------------------------------------------------
160: The application appears to have been direct launched using "srun",
160: but OMPI was not built with SLURM's PMI support and therefore cannot
160: execute. There are several options for building PMI support under
160: SLURM, depending upon the SLURM version you are using:
160: 
160:   version 16.05 or later: you can use SLURM's PMIx support. This
160:   requires that you configure and build SLURM --with-pmix.
160: 
160:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
160:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
160:   install PMI-2. You must then build Open MPI using --with-pmi pointing
160:   to the SLURM PMI library location.
160: 
160: Please configure as appropriate and try again.
160: --------------------------------------------------------------------------
160: *** An error occurred in MPI_Init_thread
160: *** on a NULL communicator
160: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
160: ***    and potentially your MPI job)
160: [n130:47486] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
160: 
160: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/visual_density_derivs
160:    inputs: ne_d2h.mol  &  response_ez_ez_dc.inp
160: 
160: running test: response_ez_ez_dc ne_d2h
76/92 Test #160: visual_density_derivs ............***Failed    4.65 sec
test 165
      Start 165: nqcc

165: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/nqcc/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/nqcc" "--verbose"
165: Test timeout computed to be: 1500
164: Comprehensive test on basis set files reading.
164: - uses user provided environmental variables BSFILE, ZELEM, RANDOM_BSF_Z
164: - without specified envirovariables it runs through all basis sets each 14 th day in the moth
164: 
164: Nothing to check today, empty pass. If you want me to test some basis sets, specify environmental variables BSFILE, ZELEM, RANDOM_BSF_Z.
164: 
164: Summary: out of total 0 tests, 0 tests crashed. 
77/92 Test #164: basis_input_scripted .............   Passed    0.16 sec
test 166
      Start 166: exacorr_talsh_memory

166: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/exacorr_talsh_memory/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/exacorr_talsh_memory" "--verbose"
166: Test timeout computed to be: 1500
162: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=be.inp', '--mol=be_s.mol']
162: 
162:  **** dirac-executable stderr console output : **** 
162: [n130:47523] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
162: --------------------------------------------------------------------------
162: The application appears to have been direct launched using "srun",
162: but OMPI was not built with SLURM's PMI support and therefore cannot
162: execute. There are several options for building PMI support under
162: SLURM, depending upon the SLURM version you are using:
162: 
162:   version 16.05 or later: you can use SLURM's PMIx support. This
162:   requires that you configure and build SLURM --with-pmix.
162: 
162:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
162:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
162:   install PMI-2. You must then build Open MPI using --with-pmi pointing
162:   to the SLURM PMI library location.
162: 
162: Please configure as appropriate and try again.
162: --------------------------------------------------------------------------
162: *** An error occurred in MPI_Init_thread
162: *** on a NULL communicator
162: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
162: ***    and potentially your MPI job)
162: [n130:47523] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
162: [n130:47524] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
162: --------------------------------------------------------------------------
162: The application appears to have been direct launched using "srun",
162: but OMPI was not built with SLURM's PMI support and therefore cannot
162: execute. There are several options for building PMI support under
162: SLURM, depending upon the SLURM version you are using:
162: 
162:   version 16.05 or later: you can use SLURM's PMIx support. This
162:   requires that you configure and build SLURM --with-pmix.
162: 
162:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
162:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
162:   install PMI-2. You must then build Open MPI using --with-pmi pointing
162:   to the SLURM PMI library location.
162: 
162: Please configure as appropriate and try again.
162: --------------------------------------------------------------------------
162: *** An error occurred in MPI_Init_thread
162: *** on a NULL communicator
162: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
162: ***    and potentially your MPI job)
162: [n130:47524] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
162: 
162: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/polprp_ph
162:    inputs: be_s.mol  &  be.inp
162: 
162: running test: be be_s
78/92 Test #162: polprp_ph ........................***Failed    4.54 sec
test 167
      Start 167: exacorr_talsh_debug

167: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/exacorr_talsh_debug/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/exacorr_talsh_debug" "--verbose"
167: Test timeout computed to be: 1500
163: PAM oriented test based on runtest-in-DIRAC:
163: 
163: ('pam script:', 'python3 /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam')
163: 
163:  Running pam for showing of flags:
163: 
163: Running pam in parallel, with defined DIRAC_MPI_COMMAND =mpirun -np 2
163: 
163: running test with input files ['inptest.inp', 'CN.sto2g.mol'] and args --noarch
163: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=inptest.inp', '--mol=CN.sto2g.mol', '--noarch']
163: 
163:  **** dirac-executable stderr console output : **** 
163: [n130:47568] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
163: --------------------------------------------------------------------------
163: The application appears to have been direct launched using "srun",
163: but OMPI was not built with SLURM's PMI support and therefore cannot
163: execute. There are several options for building PMI support under
163: SLURM, depending upon the SLURM version you are using:
163: 
163:   version 16.05 or later: you can use SLURM's PMIx support. This
163:   requires that you configure and build SLURM --with-pmix.
163: 
163:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
163:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
163:   install PMI-2. You must then build Open MPI using --with-pmi pointing
163:   to the SLURM PMI library location.
163: 
163: Please configure as appropriate and try again.
163: --------------------------------------------------------------------------
163: *** An error occurred in MPI_Init_thread
163: *** on a NULL communicator
163: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
163: ***    and potentially your MPI job)
163: [n130:47568] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
163: [n130:47569] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
163: --------------------------------------------------------------------------
163: The application appears to have been direct launched using "srun",
163: but OMPI was not built with SLURM's PMI support and therefore cannot
163: execute. There are several options for building PMI support under
163: SLURM, depending upon the SLURM version you are using:
163: 
163:   version 16.05 or later: you can use SLURM's PMIx support. This
163:   requires that you configure and build SLURM --with-pmix.
163: 
163:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
163:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
163:   install PMI-2. You must then build Open MPI using --with-pmi pointing
163:   to the SLURM PMI library location.
163: 
163: Please configure as appropriate and try again.
163: --------------------------------------------------------------------------
163: *** An error occurred in MPI_Init_thread
163: *** on a NULL communicator
163: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
163: ***    and potentially your MPI job)
163: [n130:47569] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
163: 
163: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/pam_test
163:    inputs: CN.sto2g.mol  &  inptest.inp
79/92 Test #163: pam_test .........................***Failed    4.48 sec
test 168
      Start 168: exacorr_talsh_nr_contracted

168: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/exacorr_talsh_nr_contracted/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/exacorr_talsh_nr_contracted" "--verbose"
168: Test timeout computed to be: 1500
165: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=dhf.inp', '--mol=H2O.dyall.cv2z.mol']
165: 
165:  **** dirac-executable stderr console output : **** 
165: [n130:47634] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
165: --------------------------------------------------------------------------
165: The application appears to have been direct launched using "srun",
165: but OMPI was not built with SLURM's PMI support and therefore cannot
165: execute. There are several options for building PMI support under
165: SLURM, depending upon the SLURM version you are using:
165: 
165:   version 16.05 or later: you can use SLURM's PMIx support. This
165:   requires that you configure and build SLURM --with-pmix.
165: 
165:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
165:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
165:   install PMI-2. You must then build Open MPI using --with-pmi pointing
165:   to the SLURM PMI library location.
165: 
165: Please configure as appropriate and try again.
165: --------------------------------------------------------------------------
165: *** An error occurred in MPI_Init_thread
165: *** on a NULL communicator
165: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
165: ***    and potentially your MPI job)
165: [n130:47634] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
165: [n130:47633] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
165: --------------------------------------------------------------------------
165: The application appears to have been direct launched using "srun",
165: but OMPI was not built with SLURM's PMI support and therefore cannot
165: execute. There are several options for building PMI support under
165: SLURM, depending upon the SLURM version you are using:
165: 
165:   version 16.05 or later: you can use SLURM's PMIx support. This
165:   requires that you configure and build SLURM --with-pmix.
165: 
165:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
165:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
165:   install PMI-2. You must then build Open MPI using --with-pmi pointing
165:   to the SLURM PMI library location.
165: 
165: Please configure as appropriate and try again.
165: --------------------------------------------------------------------------
165: *** An error occurred in MPI_Init_thread
165: *** on a NULL communicator
165: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
165: ***    and potentially your MPI job)
165: [n130:47633] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
165: 
165: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/nqcc
165:    inputs: H2O.dyall.cv2z.mol  &  dhf.inp
165: 
165: running test: dhf H2O.dyall.cv2z
80/92 Test #165: nqcc .............................***Failed    4.54 sec
test 169
      Start 169: exacorr_talsh_standalone

169: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/exacorr_talsh_standalone/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/exacorr_talsh_standalone" "--verbose"
169: Test timeout computed to be: 1500
166: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=exacorr_talsh_memory.inp', '--mol=H2O_nosym.mol']
166: 
166:  **** dirac-executable stderr console output : **** 
166: [n130:47637] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
166: --------------------------------------------------------------------------
166: The application appears to have been direct launched using "srun",
166: but OMPI was not built with SLURM's PMI support and therefore cannot
166: execute. There are several options for building PMI support under
166: SLURM, depending upon the SLURM version you are using:
166: 
166:   version 16.05 or later: you can use SLURM's PMIx support. This
166:   requires that you configure and build SLURM --with-pmix.
166: 
166:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
166:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
166:   install PMI-2. You must then build Open MPI using --with-pmi pointing
166:   to the SLURM PMI library location.
166: 
166: Please configure as appropriate and try again.
166: --------------------------------------------------------------------------
166: *** An error occurred in MPI_Init_thread
166: *** on a NULL communicator
166: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
166: ***    and potentially your MPI job)
166: [n130:47637] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
166: [n130:47636] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
166: --------------------------------------------------------------------------
166: The application appears to have been direct launched using "srun",
166: but OMPI was not built with SLURM's PMI support and therefore cannot
166: execute. There are several options for building PMI support under
166: SLURM, depending upon the SLURM version you are using:
166: 
166:   version 16.05 or later: you can use SLURM's PMIx support. This
166:   requires that you configure and build SLURM --with-pmix.
166: 
166:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
166:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
166:   install PMI-2. You must then build Open MPI using --with-pmi pointing
166:   to the SLURM PMI library location.
166: 
166: Please configure as appropriate and try again.
166: --------------------------------------------------------------------------
166: *** An error occurred in MPI_Init_thread
166: *** on a NULL communicator
166: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
166: ***    and potentially your MPI job)
166: [n130:47636] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
166: 
166: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/exacorr_talsh_memory
166:    inputs: H2O_nosym.mol  &  exacorr_talsh_memory.inp
166: 
166: running test: exacorr_talsh_memory H2O_nosym
81/92 Test #166: exacorr_talsh_memory .............***Failed    4.63 sec
test 170
      Start 170: exacorr_talsh_lambda

170: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/exacorr_talsh_lambda/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/exacorr_talsh_lambda" "--verbose"
170: Test timeout computed to be: 1500
167: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=cc.inp', '--mol=h2.xyz']
167: 
167:  **** dirac-executable stderr console output : **** 
167: [n130:47674] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
167: --------------------------------------------------------------------------
167: The application appears to have been direct launched using "srun",
167: but OMPI was not built with SLURM's PMI support and therefore cannot
167: execute. There are several options for building PMI support under
167: SLURM, depending upon the SLURM version you are using:
167: 
167:   version 16.05 or later: you can use SLURM's PMIx support. This
167:   requires that you configure and build SLURM --with-pmix.
167: 
167:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
167:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
167:   install PMI-2. You must then build Open MPI using --with-pmi pointing
167:   to the SLURM PMI library location.
167: 
167: Please configure as appropriate and try again.
167: --------------------------------------------------------------------------
167: *** An error occurred in MPI_Init_thread
167: *** on a NULL communicator
167: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
167: ***    and potentially your MPI job)
167: [n130:47674] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
167: [n130:47675] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
167: --------------------------------------------------------------------------
167: The application appears to have been direct launched using "srun",
167: but OMPI was not built with SLURM's PMI support and therefore cannot
167: execute. There are several options for building PMI support under
167: SLURM, depending upon the SLURM version you are using:
167: 
167:   version 16.05 or later: you can use SLURM's PMIx support. This
167:   requires that you configure and build SLURM --with-pmix.
167: 
167:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
167:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
167:   install PMI-2. You must then build Open MPI using --with-pmi pointing
167:   to the SLURM PMI library location.
167: 
167: Please configure as appropriate and try again.
167: --------------------------------------------------------------------------
167: *** An error occurred in MPI_Init_thread
167: *** on a NULL communicator
167: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
167: ***    and potentially your MPI job)
167: [n130:47675] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
167: 
167: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/exacorr_talsh_debug
167:    inputs: h2.xyz  &  cc.inp
167: 
167: running test: cc h2
82/92 Test #167: exacorr_talsh_debug ..............***Failed    4.48 sec
test 172
      Start 172: exacorr_talsh_fock

172: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/exacorr_talsh_fock/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/exacorr_talsh_fock" "--verbose"
172: Test timeout computed to be: 1500
168: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=contracted.inp', '--mol=hf.xyz']
168: 
168:  **** dirac-executable stderr console output : **** 
168: [n130:47715] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
168: [n130:47716] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
168: --------------------------------------------------------------------------
168: The application appears to have been direct launched using "srun",
168: but OMPI was not built with SLURM's PMI support and therefore cannot
168: execute. There are several options for building PMI support under
168: SLURM, depending upon the SLURM version you are using:
168: 
168:   version 16.05 or later: you can use SLURM's PMIx support. This
168:   requires that you configure and build SLURM --with-pmix.
168: 
168:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
168:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
168:   install PMI-2. You must then build Open MPI using --with-pmi pointing
168:   to the SLURM PMI library location.
168: 
168: Please configure as appropriate and try again.
168: --------------------------------------------------------------------------
168: --------------------------------------------------------------------------
168: The application appears to have been direct launched using "srun",
168: but OMPI was not built with SLURM's PMI support and therefore cannot
168: execute. There are several options for building PMI support under
168: SLURM, depending upon the SLURM version you are using:
168: 
168:   version 16.05 or later: you can use SLURM's PMIx support. This
168:   requires that you configure and build SLURM --with-pmix.
168: 
168:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
168:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
168:   install PMI-2. You must then build Open MPI using --with-pmi pointing
168:   to the SLURM PMI library location.
168: 
168: Please configure as appropriate and try again.
168: --------------------------------------------------------------------------
168: *** An error occurred in MPI_Init_thread
168: *** on a NULL communicator
168: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
168: ***    and potentially your MPI job)
168: [n130:47716] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
168: *** An error occurred in MPI_Init_thread
168: *** on a NULL communicator
168: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
168: ***    and potentially your MPI job)
168: [n130:47715] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
168: 
168: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/exacorr_talsh_nr_contracted
168:    inputs: hf.xyz  &  contracted.inp
168: 
168: running test: contracted hf
83/92 Test #168: exacorr_talsh_nr_contracted ......***Failed    3.89 sec
test 173
      Start 173: exacorr_talsh_open

173: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/exacorr_talsh_open/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/exacorr_talsh_open" "--verbose"
173: Test timeout computed to be: 1500
169: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--outcmo', '--inp=ccsd_x2c.inp', '--mol=H2O_nosym.mol']
169: 
169:  **** dirac-executable stderr console output : **** 
169: [n130:47778] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
169: --------------------------------------------------------------------------
169: The application appears to have been direct launched using "srun",
169: but OMPI was not built with SLURM's PMI support and therefore cannot
169: execute. There are several options for building PMI support under
169: SLURM, depending upon the SLURM version you are using:
169: 
169:   version 16.05 or later: you can use SLURM's PMIx support. This
169:   requires that you configure and build SLURM --with-pmix.
169: 
169:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
169:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
169:   install PMI-2. You must then build Open MPI using --with-pmi pointing
169:   to the SLURM PMI library location.
169: 
169: Please configure as appropriate and try again.
169: --------------------------------------------------------------------------
169: *** An error occurred in MPI_Init_thread
169: *** on a NULL communicator
169: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
169: ***    and potentially your MPI job)
169: [n130:47778] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
169: [n130:47779] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
169: --------------------------------------------------------------------------
169: The application appears to have been direct launched using "srun",
169: but OMPI was not built with SLURM's PMI support and therefore cannot
169: execute. There are several options for building PMI support under
169: SLURM, depending upon the SLURM version you are using:
169: 
169:   version 16.05 or later: you can use SLURM's PMIx support. This
169:   requires that you configure and build SLURM --with-pmix.
169: 
169:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
169:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
169:   install PMI-2. You must then build Open MPI using --with-pmi pointing
169:   to the SLURM PMI library location.
169: 
169: Please configure as appropriate and try again.
169: --------------------------------------------------------------------------
169: *** An error occurred in MPI_Init_thread
169: *** on a NULL communicator
169: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
169: ***    and potentially your MPI job)
169: [n130:47779] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
169: 
169: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/exacorr_talsh_standalone
169:    inputs: H2O_nosym.mol  &  ccsd_x2c.inp
169: 
169: running test: ccsd_x2c H2O_nosym
84/92 Test #169: exacorr_talsh_standalone .........***Failed    4.16 sec
test 177
      Start 177: exacorr_talsh_finite_field

177: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/exacorr_talsh_finite_field/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/exacorr_talsh_finite_field" "--verbose"
177: Test timeout computed to be: 1500
170: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=ccsd_x2c.inp', '--mol=H2O_nosym.mol']
170: 
170:  **** dirac-executable stderr console output : **** 
170: [n130:47795] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
170: --------------------------------------------------------------------------
170: The application appears to have been direct launched using "srun",
170: but OMPI was not built with SLURM's PMI support and therefore cannot
170: execute. There are several options for building PMI support under
170: SLURM, depending upon the SLURM version you are using:
170: 
170:   version 16.05 or later: you can use SLURM's PMIx support. This
170:   requires that you configure and build SLURM --with-pmix.
170: 
170:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
170:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
170:   install PMI-2. You must then build Open MPI using --with-pmi pointing
170:   to the SLURM PMI library location.
170: 
170: Please configure as appropriate and try again.
170: --------------------------------------------------------------------------
170: *** An error occurred in MPI_Init_thread
170: *** on a NULL communicator
170: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
170: ***    and potentially your MPI job)
170: [n130:47795] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
170: [n130:47794] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
170: --------------------------------------------------------------------------
170: The application appears to have been direct launched using "srun",
170: but OMPI was not built with SLURM's PMI support and therefore cannot
170: execute. There are several options for building PMI support under
170: SLURM, depending upon the SLURM version you are using:
170: 
170:   version 16.05 or later: you can use SLURM's PMIx support. This
170:   requires that you configure and build SLURM --with-pmix.
170: 
170:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
170:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
170:   install PMI-2. You must then build Open MPI using --with-pmi pointing
170:   to the SLURM PMI library location.
170: 
170: Please configure as appropriate and try again.
170: --------------------------------------------------------------------------
170: *** An error occurred in MPI_Init_thread
170: *** on a NULL communicator
170: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
170: ***    and potentially your MPI job)
170: [n130:47794] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
170: 
170: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/exacorr_talsh_lambda
170:    inputs: H2O_nosym.mol  &  ccsd_x2c.inp
170: 
170: running test: ccsd_x2c H2O_nosym
85/92 Test #170: exacorr_talsh_lambda .............***Failed    4.31 sec
test 183
      Start 183: exacorr_exatensor_memory

183: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/exacorr_exatensor_memory/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/exacorr_exatensor_memory" "--verbose"
183: Test timeout computed to be: 1500
172: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--put=X2CMAT AOMOMAT', '--inp=talsh_cc_h2o.inp', '--mol=H2O_nosym.mol']
172: 
172:  **** dirac-executable stderr console output : **** 
172: [n130:47825] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
172: --------------------------------------------------------------------------
172: The application appears to have been direct launched using "srun",
172: but OMPI was not built with SLURM's PMI support and therefore cannot
172: execute. There are several options for building PMI support under
172: SLURM, depending upon the SLURM version you are using:
172: 
172:   version 16.05 or later: you can use SLURM's PMIx support. This
172:   requires that you configure and build SLURM --with-pmix.
172: 
172:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
172:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
172:   install PMI-2. You must then build Open MPI using --with-pmi pointing
172:   to the SLURM PMI library location.
172: 
172: Please configure as appropriate and try again.
172: --------------------------------------------------------------------------
172: *** An error occurred in MPI_Init_thread
172: *** on a NULL communicator
172: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
172: ***    and potentially your MPI job)
172: [n130:47825] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
172: [n130:47824] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
172: --------------------------------------------------------------------------
172: The application appears to have been direct launched using "srun",
172: but OMPI was not built with SLURM's PMI support and therefore cannot
172: execute. There are several options for building PMI support under
172: SLURM, depending upon the SLURM version you are using:
172: 
172:   version 16.05 or later: you can use SLURM's PMIx support. This
172:   requires that you configure and build SLURM --with-pmix.
172: 
172:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
172:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
172:   install PMI-2. You must then build Open MPI using --with-pmi pointing
172:   to the SLURM PMI library location.
172: 
172: Please configure as appropriate and try again.
172: --------------------------------------------------------------------------
172: *** An error occurred in MPI_Init_thread
172: *** on a NULL communicator
172: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
172: ***    and potentially your MPI job)
172: [n130:47824] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
172: 
172: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/exacorr_talsh_fock
172:    inputs: H2O_nosym.mol  &  talsh_cc_h2o.inp
172: 
172: running test: talsh_cc_h2o H2O_nosym
86/92 Test #172: exacorr_talsh_fock ...............***Failed    4.51 sec
test 196
      Start 196: pcm_energy

196: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/pcm_energy/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/pcm_energy" "--verbose"
196: Test timeout computed to be: 1500
173: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--put=X2CMAT AOMOMAT', '--inp=talsh_cc_beh.inp', '--mol=BeH_nosym.mol']
173: 
173:  **** dirac-executable stderr console output : **** 
173: [n130:47856] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
173: --------------------------------------------------------------------------
173: The application appears to have been direct launched using "srun",
173: but OMPI was not built with SLURM's PMI support and therefore cannot
173: execute. There are several options for building PMI support under
173: SLURM, depending upon the SLURM version you are using:
173: 
173:   version 16.05 or later: you can use SLURM's PMIx support. This
173:   requires that you configure and build SLURM --with-pmix.
173: 
173:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
173:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
173:   install PMI-2. You must then build Open MPI using --with-pmi pointing
173:   to the SLURM PMI library location.
173: 
173: Please configure as appropriate and try again.
173: --------------------------------------------------------------------------
173: *** An error occurred in MPI_Init_thread
173: *** on a NULL communicator
173: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
173: ***    and potentially your MPI job)
173: [n130:47856] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
173: [n130:47857] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
173: --------------------------------------------------------------------------
173: The application appears to have been direct launched using "srun",
173: but OMPI was not built with SLURM's PMI support and therefore cannot
173: execute. There are several options for building PMI support under
173: SLURM, depending upon the SLURM version you are using:
173: 
173:   version 16.05 or later: you can use SLURM's PMIx support. This
173:   requires that you configure and build SLURM --with-pmix.
173: 
173:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
173:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
173:   install PMI-2. You must then build Open MPI using --with-pmi pointing
173:   to the SLURM PMI library location.
173: 
173: Please configure as appropriate and try again.
173: --------------------------------------------------------------------------
173: *** An error occurred in MPI_Init_thread
173: *** on a NULL communicator
173: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
173: ***    and potentially your MPI job)
173: [n130:47857] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
173: 
173: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/exacorr_talsh_open
173:    inputs: BeH_nosym.mol  &  talsh_cc_beh.inp
173: 
173: running test: talsh_cc_beh BeH_nosym
87/92 Test #173: exacorr_talsh_open ...............***Failed    4.43 sec
test 201
      Start 201: x_amfi

201: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/x_amfi/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/x_amfi" "--verbose"
201: Test timeout computed to be: 1500
177: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=talsh_ccsd_x2c_zdip.inp', '--mol=MgF_nosym.mol']
177: 
177:  **** dirac-executable stderr console output : **** 
177: [n130:47920] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
177: [n130:47919] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
177: --------------------------------------------------------------------------
177: The application appears to have been direct launched using "srun",
177: but OMPI was not built with SLURM's PMI support and therefore cannot
177: execute. There are several options for building PMI support under
177: SLURM, depending upon the SLURM version you are using:
177: 
177:   version 16.05 or later: you can use SLURM's PMIx support. This
177:   requires that you configure and build SLURM --with-pmix.
177: 
177:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
177:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
177:   install PMI-2. You must then build Open MPI using --with-pmi pointing
177:   to the SLURM PMI library location.
177: 
177: Please configure as appropriate and try again.
177: --------------------------------------------------------------------------
177: *** An error occurred in MPI_Init_thread
177: *** on a NULL communicator
177: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
177: ***    and potentially your MPI job)
177: [n130:47920] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
177: --------------------------------------------------------------------------
177: The application appears to have been direct launched using "srun",
177: but OMPI was not built with SLURM's PMI support and therefore cannot
177: execute. There are several options for building PMI support under
177: SLURM, depending upon the SLURM version you are using:
177: 
177:   version 16.05 or later: you can use SLURM's PMIx support. This
177:   requires that you configure and build SLURM --with-pmix.
177: 
177:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
177:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
177:   install PMI-2. You must then build Open MPI using --with-pmi pointing
177:   to the SLURM PMI library location.
177: 
177: Please configure as appropriate and try again.
177: --------------------------------------------------------------------------
177: *** An error occurred in MPI_Init_thread
177: *** on a NULL communicator
177: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
177: ***    and potentially your MPI job)
177: [n130:47919] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
177: 
177: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/exacorr_talsh_finite_field
177:    inputs: MgF_nosym.mol  &  talsh_ccsd_x2c_zdip.inp
177: 
177: running test: talsh_ccsd_x2c_zdip MgF_nosym
88/92 Test #177: exacorr_talsh_finite_field .......***Failed    4.45 sec
test 205
      Start 205: pe_cpp

205: Test command: /usr/bin/python "/home/milias/work/software/dirac/dirac_public/test/pe_cpp/test" "--binary-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4" "--work-dir=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/pe_cpp" "--verbose"
205: Test timeout computed to be: 1500
183: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=exacorr_exatensor_memory.inp', '--mol=H2O_nosym.mol']
183: 
183:  **** dirac-executable stderr console output : **** 
183: [n130:47939] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
183: --------------------------------------------------------------------------
183: The application appears to have been direct launched using "srun",
183: but OMPI was not built with SLURM's PMI support and therefore cannot
183: execute. There are several options for building PMI support under
183: SLURM, depending upon the SLURM version you are using:
183: 
183:   version 16.05 or later: you can use SLURM's PMIx support. This
183:   requires that you configure and build SLURM --with-pmix.
183: 
183:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
183:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
183:   install PMI-2. You must then build Open MPI using --with-pmi pointing
183:   to the SLURM PMI library location.
183: 
183: Please configure as appropriate and try again.
183: --------------------------------------------------------------------------
183: *** An error occurred in MPI_Init_thread
183: *** on a NULL communicator
183: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
183: ***    and potentially your MPI job)
183: [n130:47939] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
183: [n130:47940] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
183: --------------------------------------------------------------------------
183: The application appears to have been direct launched using "srun",
183: but OMPI was not built with SLURM's PMI support and therefore cannot
183: execute. There are several options for building PMI support under
183: SLURM, depending upon the SLURM version you are using:
183: 
183:   version 16.05 or later: you can use SLURM's PMIx support. This
183:   requires that you configure and build SLURM --with-pmix.
183: 
183:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
183:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
183:   install PMI-2. You must then build Open MPI using --with-pmi pointing
183:   to the SLURM PMI library location.
183: 
183: Please configure as appropriate and try again.
183: --------------------------------------------------------------------------
183: *** An error occurred in MPI_Init_thread
183: *** on a NULL communicator
183: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
183: ***    and potentially your MPI job)
183: [n130:47940] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
183: 
183: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/exacorr_exatensor_memory
183:    inputs: H2O_nosym.mol  &  exacorr_exatensor_memory.inp
183: 
183: running test: exacorr_exatensor_memory H2O_nosym
89/92 Test #183: exacorr_exatensor_memory .........***Failed    4.49 sec
196: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--inp=nonrel.inp', '--mol=CH4.mol']
196: 
196:  **** dirac-executable stderr console output : **** 
196: [n130:47971] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
196: --------------------------------------------------------------------------
196: The application appears to have been direct launched using "srun",
196: but OMPI was not built with SLURM's PMI support and therefore cannot
196: execute. There are several options for building PMI support under
196: SLURM, depending upon the SLURM version you are using:
196: 
196:   version 16.05 or later: you can use SLURM's PMIx support. This
196:   requires that you configure and build SLURM --with-pmix.
196: 
196:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
196:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
196:   install PMI-2. You must then build Open MPI using --with-pmi pointing
196:   to the SLURM PMI library location.
196: 
196: Please configure as appropriate and try again.
196: --------------------------------------------------------------------------
196: *** An error occurred in MPI_Init_thread
196: *** on a NULL communicator
196: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
196: ***    and potentially your MPI job)
196: [n130:47971] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
196: [n130:47970] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
196: --------------------------------------------------------------------------
196: The application appears to have been direct launched using "srun",
196: but OMPI was not built with SLURM's PMI support and therefore cannot
196: execute. There are several options for building PMI support under
196: SLURM, depending upon the SLURM version you are using:
196: 
196:   version 16.05 or later: you can use SLURM's PMIx support. This
196:   requires that you configure and build SLURM --with-pmix.
196: 
196:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
196:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
196:   install PMI-2. You must then build Open MPI using --with-pmi pointing
196:   to the SLURM PMI library location.
196: 
196: Please configure as appropriate and try again.
196: --------------------------------------------------------------------------
196: *** An error occurred in MPI_Init_thread
196: *** on a NULL communicator
196: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
196: ***    and potentially your MPI job)
196: [n130:47970] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
196: 
196: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/pcm_energy
196:    inputs: CH4.mol  &  nonrel.inp
196: 
196: running test: nonrel CH4
90/92 Test #196: pcm_energy .......................***Failed    4.70 sec
201: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--outcmo', '--get=amfPCEC.h5', '--inp=f.inp', '--mol=f.xyz']
201: 
201:  **** dirac-executable stderr console output : **** 
201: [n130:48001] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
201: [n130:48002] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
201: --------------------------------------------------------------------------
201: The application appears to have been direct launched using "srun",
201: but OMPI was not built with SLURM's PMI support and therefore cannot
201: execute. There are several options for building PMI support under
201: SLURM, depending upon the SLURM version you are using:
201: 
201:   version 16.05 or later: you can use SLURM's PMIx support. This
201:   requires that you configure and build SLURM --with-pmix.
201: 
201:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
201:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
201:   install PMI-2. You must then build Open MPI using --with-pmi pointing
201:   to the SLURM PMI library location.
201: 
201: Please configure as appropriate and try again.
201: --------------------------------------------------------------------------
201: *** An error occurred in MPI_Init_thread
201: *** on a NULL communicator
201: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
201: ***    and potentially your MPI job)
201: [n130:48001] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
201: --------------------------------------------------------------------------
201: The application appears to have been direct launched using "srun",
201: but OMPI was not built with SLURM's PMI support and therefore cannot
201: execute. There are several options for building PMI support under
201: SLURM, depending upon the SLURM version you are using:
201: 
201:   version 16.05 or later: you can use SLURM's PMIx support. This
201:   requires that you configure and build SLURM --with-pmix.
201: 
201:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
201:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
201:   install PMI-2. You must then build Open MPI using --with-pmi pointing
201:   to the SLURM PMI library location.
201: 
201: Please configure as appropriate and try again.
201: --------------------------------------------------------------------------
201: *** An error occurred in MPI_Init_thread
201: *** on a NULL communicator
201: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
201: ***    and potentially your MPI job)
201: [n130:48002] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
201: 
201: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/x_amfi
201:    inputs: f.xyz  &  f.inp
201: 
201: running test: f f
91/92 Test #201: x_amfi ...........................***Failed    4.17 sec
205: ERROR: crash during ['python3', '/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/pam', '--dirac=/home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/dirac.x', '--noarch', '--nobackup', '--put', 'h2o.pot', '--inp=4c.inp', '--mol=h2o.mol']
205: 
205:  **** dirac-executable stderr console output : **** 
205: [n130:48053] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
205: --------------------------------------------------------------------------
205: The application appears to have been direct launched using "srun",
205: but OMPI was not built with SLURM's PMI support and therefore cannot
205: execute. There are several options for building PMI support under
205: SLURM, depending upon the SLURM version you are using:
205: 
205:   version 16.05 or later: you can use SLURM's PMIx support. This
205:   requires that you configure and build SLURM --with-pmix.
205: 
205:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
205:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
205:   install PMI-2. You must then build Open MPI using --with-pmi pointing
205:   to the SLURM PMI library location.
205: 
205: Please configure as appropriate and try again.
205: --------------------------------------------------------------------------
205: *** An error occurred in MPI_Init_thread
205: *** on a NULL communicator
205: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
205: ***    and potentially your MPI job)
205: [n130:48053] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
205: [n130:48052] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
205: --------------------------------------------------------------------------
205: The application appears to have been direct launched using "srun",
205: but OMPI was not built with SLURM's PMI support and therefore cannot
205: execute. There are several options for building PMI support under
205: SLURM, depending upon the SLURM version you are using:
205: 
205:   version 16.05 or later: you can use SLURM's PMIx support. This
205:   requires that you configure and build SLURM --with-pmix.
205: 
205:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
205:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
205:   install PMI-2. You must then build Open MPI using --with-pmi pointing
205:   to the SLURM PMI library location.
205: 
205: Please configure as appropriate and try again.
205: --------------------------------------------------------------------------
205: *** An error occurred in MPI_Init_thread
205: *** on a NULL communicator
205: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
205: ***    and potentially your MPI job)
205: [n130:48052] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
205: 
205: directory: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/test/pe_cpp
205:    inputs: h2o.mol  &  4c.inp
205: 
205: running test: 4c h2o
92/92 Test #205: pe_cpp ...........................***Failed    3.06 sec

2% tests passed, 90 tests failed out of 92

Label Time Summary:
4c                   =  11.96 sec*proc (1 test)
basis                =  12.28 sec*proc (4 tests)
bed                  =   8.90 sec*proc (2 tests)
cc                   = 111.79 sec*proc (15 tests)
ci                   =  24.07 sec*proc (6 tests)
complex              =   3.88 sec*proc (1 test)
cosci                =  12.41 sec*proc (3 tests)
dft                  =  62.61 sec*proc (8 tests)
dirrci               =   3.31 sec*proc (1 test)
ecp                  =   4.20 sec*proc (1 test)
eomcc                =   8.01 sec*proc (2 tests)
exatensor            =   4.49 sec*proc (1 test)
gaunt                =   4.25 sec*proc (1 test)
grid                 =   4.62 sec*proc (1 test)
huckel               =   4.51 sec*proc (1 test)
import               =   5.03 sec*proc (1 test)
krci                 =  17.65 sec*proc (4 tests)
lao                  =  12.92 sec*proc (3 tests)
laplace              =   3.11 sec*proc (1 test)
levelshift           =   3.16 sec*proc (1 test)
localization         =   4.54 sec*proc (1 test)
magnetizabilities    =   8.88 sec*proc (2 tests)
mcscf                =   5.03 sec*proc (1 test)
mp2                  =  19.88 sec*proc (5 tests)
nmr                  =   4.27 sec*proc (1 test)
nonel                =   3.39 sec*proc (1 test)
nqcc                 =   4.54 sec*proc (1 test)
operators            =  20.64 sec*proc (1 test)
pam                  =   4.48 sec*proc (1 test)
pcm                  =   4.70 sec*proc (1 test)
pelib                =   3.06 sec*proc (1 test)
polarizabilities     =   4.21 sec*proc (1 test)
polprp               =   4.54 sec*proc (1 test)
projection           =   8.49 sec*proc (2 tests)
property             =   8.21 sec*proc (2 tests)
pvc                  =   4.31 sec*proc (1 test)
qcorr                =   4.59 sec*proc (1 test)
reladc               =  16.72 sec*proc (4 tests)
response             =  46.60 sec*proc (11 tests)
scf                  =  29.15 sec*proc (5 tests)
shield               =   4.60 sec*proc (1 test)
shielding            =   4.05 sec*proc (1 test)
short                = 595.83 sec*proc (92 tests)
srdft                =   4.03 sec*proc (1 test)
stex                 =   4.42 sec*proc (1 test)
structures           =   4.30 sec*proc (1 test)
talsh                =  34.86 sec*proc (8 tests)
twocomp              =   4.40 sec*proc (1 test)
visual               =  65.73 sec*proc (6 tests)
x2c                  =  15.90 sec*proc (2 tests)
xamfi                =   4.17 sec*proc (1 test)
zora                 =   4.38 sec*proc (1 test)

Total Test time (real) = 150.39 sec

The following tests FAILED:
	 86 - reladc_dip (Failed)
	 88 - xyz_symmetry_recognition (Failed)
	 89 - nmqm_operator (Failed)
	 90 - visual_div_rot (Failed)
	 91 - bed_isotropic (Failed)
	 92 - localization (Failed)
	 93 - krci_properties_perm_dipmom (Failed)
	 94 - huckel_start (Failed)
	 95 - cosci_tmom (Failed)
	 96 - reladc_sipeigv (Failed)
	 97 - bed_anisotropic (Failed)
	 98 - response_lao_magnetiz_dft (Failed)
	 99 - dft_pp86 (Failed)
	100 - bss_energy (Failed)
	101 - acmoin (Failed)
	102 - dft_cam (Failed)
	103 - visual_frac_occupation (Failed)
	104 - eomee_fc_cvs (Failed)
	105 - response_lao_shield (Failed)
	106 - response_lao_magnetiz (Failed)
	107 - krci_properties_omega_tdm (Failed)
	108 - response_hf_polarizability (Failed)
	109 - count_cc_memory (Failed)
	110 - reladc_fano (Failed)
	111 - cosci_methods (Failed)
	112 - response_nonrel (Failed)
	113 - eomip_fc_cvs (Failed)
	114 - symmetry_recognition (Failed)
	115 - reladc_sip (Failed)
	116 - ecp (Failed)
	117 - scf_levelshift (Failed)
	118 - basis_input (Failed)
	119 - visual_gamma5 (Failed)
	120 - pvc_scf (Failed)
	121 - cosci_energy_spinfree (Failed)
	122 - density_at_nuclei (Failed)
	123 - dft_erf_xcfun (Failed)
	124 - mp2_natural_orbitals (Failed)
	125 - ffpt_dipmom_polariz_relcc (Failed)
	126 - mp2_srdft_energies (Failed)
	127 - operators_mo_mtx_elements (Failed)
	128 - response_C6 (Failed)
	129 - bsse (Failed)
	130 - import_mos (Failed)
	131 - stex (Failed)
	132 - zora (Failed)
	133 - dft_grid_export_import (Failed)
	134 - cosci_average (Failed)
	135 - linear_structures (Failed)
	136 - gaunt (Failed)
	137 - dft_ac (Failed)
	138 - dft_betasigma (Failed)
	139 - lucita_short (Failed)
	140 - projection_analysis_overlaps (Failed)
	141 - lucita_large (Failed)
	142 - lucita_q_corrections (Failed)
	143 - basis_contraction (Failed)
	144 - response_nmr_levy-leblond (Failed)
	145 - blockd_twocomp (Failed)
	146 - dft_cosci (Failed)
	147 - molecular_mean_field_restart (Failed)
	148 - dirrci_property (Failed)
	149 - laplace (Failed)
	150 - response_complex (Failed)
	151 - mp2_energy (Failed)
	152 - dirac_mointegral_export (Failed)
	153 - response_rkbimp_shield (Failed)
	154 - free-particle_projection (Failed)
	155 - atomic_start (Failed)
	156 - checkpoint_from_Dformat (Failed)
	157 - tutorial_checkpoint (Failed)
	158 - cosci_energy_blockd (Failed)
	159 - visual_elf (Failed)
	160 - visual_density_derivs (Failed)
	161 - visual_custom_output (Failed)
	162 - polprp_ph (Failed)
	163 - pam_test (Failed)
	165 - nqcc (Failed)
	166 - exacorr_talsh_memory (Failed)
	167 - exacorr_talsh_debug (Failed)
	168 - exacorr_talsh_nr_contracted (Failed)
	169 - exacorr_talsh_standalone (Failed)
	170 - exacorr_talsh_lambda (Failed)
	172 - exacorr_talsh_fock (Failed)
	173 - exacorr_talsh_open (Failed)
	177 - exacorr_talsh_finite_field (Failed)
	183 - exacorr_exatensor_memory (Failed)
	196 - pcm_energy (Failed)
	201 - x_amfi (Failed)
	205 - pe_cpp (Failed)
Errors while running CTest
Output from these tests are in: /home/milias/work/software/dirac/dirac_public/build_openmpi_intel_mklpar_i4/Testing/Temporary/LastTest.log
Use "--rerun-failed --output-on-failure" to re-run the failed cases verbosely.

real	2m30.411s
user	0m30.010s
sys	0m40.033s
Variable DIRAC_MPI_COMMAND was unset. END
