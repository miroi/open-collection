
Job user is SLURM_JOB_USER= mirilias
User job SLURM_JOB_NAME=DIRAC has assigned ID SLURM_JOBID=9372933
This job was submitted from the computer SLURM_SUBMIT_HOST=login23.mogon
 and from the home directory SLURM_SUBMIT_DIR=/gpfs/fs1/home/mirilias/work/projects/open-collection/theoretical_chemistry/software_runs/dirac/servers/mogonII_HIM/test_runs
Job is running on the cluster compute node: SLURM_CLUSTER_NAME=mogon2
and is employing SLURM_JOB_NUM_NODES=1 node/nodes:
SLURM_JOB_NODELIST = z0432

Job partition is SLURM_JOB_PARTITION=devel 

Directory is SLURM_SUBMIT_DIR=/gpfs/fs1/home/mirilias/work/projects/open-collection/theoretical_chemistry/software_runs/dirac/servers/mogonII_HIM/test_runs 

Loading lang/Python/3.6.1-intel-2017.02
  Loading requirement: compiler/GCCcore/6.3.0 compiler/icc/2017.2.174-GCC-6.3.0
    compiler/ifort/2017.2.174-GCC-6.3.0 toolchain/iccifort/2017.2.174-GCC-6.3.0
    mpi/impi/2017.2.174-iccifort-2017.2.174-GCC-6.3.0
    toolchain/iimpi/2017.02-GCC-6.3.0
    numlib/imkl/2017.2.174-iimpi-2017.02-GCC-6.3.0 toolchain/intel/2017.02

 List of all loaded modules:
Currently Loaded Modulefiles:
 1) compiler/GCCcore/6.3.0                             
 2) compiler/icc/2017.2.174-GCC-6.3.0                  
 3) compiler/ifort/2017.2.174-GCC-6.3.0                
 4) toolchain/iccifort/2017.2.174-GCC-6.3.0            
 5) mpi/impi/2017.2.174-iccifort-2017.2.174-GCC-6.3.0  
 6) toolchain/iimpi/2017.02-GCC-6.3.0                  
 7) numlib/imkl/2017.2.174-iimpi-2017.02-GCC-6.3.0     
 8) toolchain/intel/2017.02                            
 9) lang/Python/3.6.1-intel-2017.02                    

Running on host z0432.mogon
Time is Thu Jul  8 16:11:00 CEST 2021 

The node's CPU model name	: Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz
This node has total 40 CPUs available for an EXCLUSIVE job.
This node has SLURM_CPUS_ON_NODE=40 CPUs.
This node has SLURM_NPROCS=4 CPUs allocated for SLURM calculations.

 The memory at the node (in GB)
              total        used        free      shared  buff/cache   available
Mem:             62           2          59           0           0          56
Swap:             0           0           0
Total:           62           2          59



 ifort ?: /cluster/easybuild/broadwell/software/numlib/imkl/2017.2.174-iimpi-2017.02-GCC-6.3.0/bin/ifort
Intel(R) Fortran Intel(R) 64 Compiler for applications running on Intel(R) 64, Version 17.0.2.174 Build 20170213
Copyright (C) 1985-2017 Intel Corporation.  All rights reserved.

mpif90 ?: /home/mirilias/work/software/open-mpi/openmpi-4.1.1-intel-2017.02-i8/bin/mpif90
Intel(R) Fortran Intel(R) 64 Compiler for applications running on Intel(R) 64, Version 17.0.2.174 Build 20170213
Copyright (C) 1985-2017 Intel Corporation.  All rights reserved.

mpicc ?: /home/mirilias/work/software/open-mpi/openmpi-4.1.1-intel-2017.02-i8/bin/mpicc
Intel(R) C Intel(R) 64 Compiler for applications running on Intel(R) 64, Version 17.0.2.174 Build 20170213
Copyright (C) 1985-2017 Intel Corporation.  All rights reserved.

mpicxx ?: /home/mirilias/work/software/open-mpi/openmpi-4.1.1-intel-2017.02-i8/bin/mpicxx
Intel(R) C++ Intel(R) 64 Compiler for applications running on Intel(R) 64, Version 17.0.2.174 Build 20170213
Copyright (C) 1985-2017 Intel Corporation.  All rights reserved.

Python ? :/cluster/easybuild/broadwell/software/lang/Python/3.6.1-intel-2017.02/bin/python
Python 3.6.1
cmake ? which cmake = which: no cmake in (/home/mirilias/work/software/open-mpi/openmpi-4.1.1-intel-2017.02-i8/bin:/cluster/easybuild/broadwell/software/lang/Python/3.6.1-intel-2017.02/bin:/cluster/easybuild/broadwell/software/numlib/imkl/2017.2.174-iimpi-2017.02-GCC-6.3.0/mkl/bin:/cluster/easybuild/broadwell/software/numlib/imkl/2017.2.174-iimpi-2017.02-GCC-6.3.0/bin:/cluster/easybuild/broadwell/software/mpi/impi/2017.2.174-iccifort-2017.2.174-GCC-6.3.0/bin64:/cluster/easybuild/broadwell/software/compiler/ifort/2017.2.174-GCC-6.3.0/compilers_and_libraries_2017.2.174/linux/bin/intel64:/cluster/easybuild/broadwell/software/compiler/icc/2017.2.174-GCC-6.3.0/compilers_and_libraries_2017.2.174/linux/bin/intel64:/cluster/easybuild/broadwell/software/compiler/GCCcore/6.3.0/bin:/home/mirilias/work/software/open-mpi/openmpi-4.1.1-intel-2017.02-i8/bin:/usr/share/Modules/bin:/usr/lpp/mmfs/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/var/cfengine/bin:.)
/var/tmp/slurmd_spool/job9372933/slurm_script: line 101: cmake: command not found

 mpirun ? /home/mirilias/work/software/open-mpi/openmpi-4.1.1-intel-2017.02-i8/bin/mpirun
mpirun (Open MPI) 4.1.1

Report bugs to http://www.open-mpi.org/community/help/

My PATH=/home/mirilias/work/software/open-mpi/openmpi-4.1.1-intel-2017.02-i8/bin:/cluster/easybuild/broadwell/software/lang/Python/3.6.1-intel-2017.02/bin:/cluster/easybuild/broadwell/software/numlib/imkl/2017.2.174-iimpi-2017.02-GCC-6.3.0/mkl/bin:/cluster/easybuild/broadwell/software/numlib/imkl/2017.2.174-iimpi-2017.02-GCC-6.3.0/bin:/cluster/easybuild/broadwell/software/mpi/impi/2017.2.174-iccifort-2017.2.174-GCC-6.3.0/bin64:/cluster/easybuild/broadwell/software/compiler/ifort/2017.2.174-GCC-6.3.0/compilers_and_libraries_2017.2.174/linux/bin/intel64:/cluster/easybuild/broadwell/software/compiler/icc/2017.2.174-GCC-6.3.0/compilers_and_libraries_2017.2.174/linux/bin/intel64:/cluster/easybuild/broadwell/software/compiler/GCCcore/6.3.0/bin:/home/mirilias/work/software/open-mpi/openmpi-4.1.1-intel-2017.02-i8/bin:/usr/share/Modules/bin:/usr/lpp/mmfs/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/var/cfengine/bin:.


ldd /home/mirilias/work/software/dirac/trunk_production/build_openmpi_intel_mklpar_i8/dirac.x:
	linux-vdso.so.1 (0x00007ffcdf3c6000)
	libmkl_intel_ilp64.so => /cluster/easybuild/broadwell/software/numlib/imkl/2017.2.174-iimpi-2017.02-GCC-6.3.0/mkl/lib/intel64/libmkl_intel_ilp64.so (0x00007f1a1b58a000)
	libmkl_intel_thread.so => /cluster/easybuild/broadwell/software/numlib/imkl/2017.2.174-iimpi-2017.02-GCC-6.3.0/mkl/lib/intel64/libmkl_intel_thread.so (0x00007f1a19af8000)
	libmkl_core.so => /cluster/easybuild/broadwell/software/numlib/imkl/2017.2.174-iimpi-2017.02-GCC-6.3.0/mkl/lib/intel64/libmkl_core.so (0x00007f1a18005000)
	libiomp5.so => /cluster/easybuild/broadwell/software/lang/Python/3.6.1-intel-2017.02/lib/libiomp5.so (0x00007f1a17c12000)
	libmpi.so.40 => /home/mirilias/work/software/open-mpi/openmpi-4.1.1-intel-2017.02-i8/lib/libmpi.so.40 (0x00007f1a178c6000)
	libimf.so => /cluster/easybuild/broadwell/software/numlib/imkl/2017.2.174-iimpi-2017.02-GCC-6.3.0/lib/intel64/libimf.so (0x00007f1a173da000)
	libsvml.so => /cluster/easybuild/broadwell/software/numlib/imkl/2017.2.174-iimpi-2017.02-GCC-6.3.0/lib/intel64/libsvml.so (0x00007f1a164cf000)
	libirng.so => /cluster/easybuild/broadwell/software/numlib/imkl/2017.2.174-iimpi-2017.02-GCC-6.3.0/lib/intel64/libirng.so (0x00007f1a1615a000)
	libstdc++.so.6 => /cluster/easybuild/broadwell/software/compiler/GCCcore/6.3.0/lib64/libstdc++.so.6 (0x00007f1a1bfa9000)
	libm.so.6 => /lib64/libm.so.6 (0x00007f1a15dd8000)
	libgcc_s.so.1 => /cluster/easybuild/broadwell/software/compiler/GCCcore/6.3.0/lib64/libgcc_s.so.1 (0x00007f1a1bf81000)
	libirc.so => /cluster/easybuild/broadwell/software/numlib/imkl/2017.2.174-iimpi-2017.02-GCC-6.3.0/lib/intel64/libirc.so (0x00007f1a15b6e000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x00007f1a1594e000)
	libc.so.6 => /lib64/libc.so.6 (0x00007f1a1558b000)
	libdl.so.2 => /lib64/libdl.so.2 (0x00007f1a15387000)
	libz.so.1 => /lib64/libz.so.1 (0x00007f1a15170000)
	libmpi_usempi_ignore_tkr.so.40 => /home/mirilias/work/software/open-mpi/openmpi-4.1.1-intel-2017.02-i8/lib/libmpi_usempi_ignore_tkr.so.40 (0x00007f1a14f65000)
	libmpi_mpifh.so.40 => /home/mirilias/work/software/open-mpi/openmpi-4.1.1-intel-2017.02-i8/lib/libmpi_mpifh.so.40 (0x00007f1a14cef000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f1a1bf14000)
	libopen-rte.so.40 => /home/mirilias/work/software/open-mpi/openmpi-4.1.1-intel-2017.02-i8/lib/libopen-rte.so.40 (0x00007f1a14a28000)
	libopen-orted-mpir.so => /home/mirilias/work/software/open-mpi/openmpi-4.1.1-intel-2017.02-i8/lib/libopen-orted-mpir.so (0x00007f1a14826000)
	libopen-pal.so.40 => /home/mirilias/work/software/open-mpi/openmpi-4.1.1-intel-2017.02-i8/lib/libopen-pal.so.40 (0x00007f1a144e6000)
	libudev.so.1 => /lib64/libudev.so.1 (0x00007f1a14251000)
	librt.so.1 => /lib64/librt.so.1 (0x00007f1a14049000)
	libutil.so.1 => /lib64/libutil.so.1 (0x00007f1a13e45000)
	libintlc.so.5 => /cluster/easybuild/broadwell/software/numlib/imkl/2017.2.174-iimpi-2017.02-GCC-6.3.0/lib/intel64/libintlc.so.5 (0x00007f1a13bdb000)
	libifport.so.5 => /cluster/easybuild/broadwell/software/numlib/imkl/2017.2.174-iimpi-2017.02-GCC-6.3.0/lib/intel64/libifport.so.5 (0x00007f1a139ac000)
	libifcoremt.so.5 => /cluster/easybuild/broadwell/software/numlib/imkl/2017.2.174-iimpi-2017.02-GCC-6.3.0/lib/intel64/libifcoremt.so.5 (0x00007f1a1361c000)
	libmount.so.1 => /lib64/libmount.so.1 (0x00007f1a133c2000)
	libblkid.so.1 => /lib64/libblkid.so.1 (0x00007f1a1316f000)
	libuuid.so.1 => /lib64/libuuid.so.1 (0x00007f1a12f67000)
	libselinux.so.1 => /lib64/libselinux.so.1 (0x00007f1a12d3d000)
	libpcre2-8.so.0 => /lib64/libpcre2-8.so.0 (0x00007f1a12ab9000)

DIRAC scratch directory space, /lustre/miifs03/scratch/m2_him_exp
Filesystem                                      Size  Used Avail Use% Mounted on
10.85.200.10@o2ib2:10.85.200.11@o2ib2:/miifs03  1.1P  386T  735T  35% /lustre/miifs03

 For comparison, /tmp local disk;  df -h /tmp/.
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda3        20G  4.5G   16G  23% /



 For test runs, DIRAC_MPI_COMMAND=mpirun -np 4
ERROR: test cc_N2.ccpVQZ.out failed

.          Electronic energy                        :    -132.62569515579969
.       
.          Other contributions to the total energy
.          Nuclear repulsion energy                 :      23.57243939477000
ERROR                                                      ################# expected: 23.57243927881818 (rel diff: 4.92e-09)
.          SS Coulombic correction                  :       0.00000094289758
ERROR                                                       ################ expected: 9.4289757e-07 (rel diff: 1.06e-08)
.       
.          Sum of all contributions to the energy
.          Total energy                             :    -109.05325481813212
.        Electronic energy :                         -31.554990403700405
ERROR                                                ################### expected: -31.55499034312188 (rel diff: 1.92e-09)
.        SCF energy :                               -109.053254818132714
.       
.       
.        Energy calculations
.        MP2 module active :                       T
.        CCSD module active :                      T
.        CCSD(T) module active :                   T
ERROR: test cc_N2.ccpV5Z.out failed

.          Electronic energy                        :    -132.62742531454091
.       
.          Other contributions to the total energy
.          Nuclear repulsion energy                 :      23.57243939477000
ERROR                                                      ################# expected: 23.57243927881818 (rel diff: 4.92e-09)
.          SS Coulombic correction                  :       0.00000094289758
ERROR                                                       ################ expected: 9.4289757e-07 (rel diff: 1.06e-08)
.       
.          Sum of all contributions to the energy
.          Total energy                             :    -109.05498497687334
.        Electronic energy :                         -31.556553127926012
ERROR                                                ################### expected: -31.556553067335905 (rel diff: 1.92e-09)
.        SCF energy :                               -109.054984976872760
.       
.       
.        Energy calculations
.        MP2 module active :                       T
.        CCSD module active :                      T
.        CCSD(T) module active :                   T

running test: cc N2.ccpVQZ

running test: cc N2.ccpV5Z

running test: lda_numgra h2
passed

running test: lda_numgra_x2c h2
passed

running test: geop_numgra h2
passed

running test: lda h2o
passed
