#!/bin/bash

#SBATCH -J Alfcc

##  partition (queue)
##SBATCH -p cascade
#SBATCH -p knl
##SBATCH -p long
##SBATCH -p flnr-ice

## max. execution time
#SBATCH -t 0-08:00:00

##SBATCH --exclusive

##SBATCH --mem=32GB
#SBATCH --mem=32GB
##SBATCH --mem=264GB
##SBATCH --mem=16GB

# do not restart in the case of nodefail!
#SBATCH --no-requeue
#SBATCH --no-kill

#SBATCH -N 2 -n 4
#SBATCH  --sockets-per-node=1

## memory NO !!!
##SBATCH --mem-per-cpu=28G

## stdout/stderr output file
##SBATCH -o log_slurm_job.%j.%N.std_out_err
#SBATCH -o log_slurm_job.%j.%N.std_out

## mail
##SBATCH --mail-user=siuraksh@jinr.ru
#SBATCH --mail-type=ALL

 
echo Job user is $SLURM_JOB_USER and his job $SLURM_JOB_NAME has assigned ID $SLURM_JOBID
echo This job was submitted from the computer $SLURM_SUBMIT_HOST
echo and from the home directory:
echo $SLURM_SUBMIT_DIR
echo
echo It is running on the cluster compute node:
echo $SLURM_CLUSTER_NAME
echo and is employing $SLURM_JOB_NUM_NODES node/nodes:
echo $SLURM_JOB_NODELIST
echo
echo -e "Job partition is $SLURM_JOB_PARTITION \n"
echo The job requests $SLURM_CPUS_ON_NODE CPU per task.

MACHINEFILE="nodes.$SLURM_JOB_PARTITION.$SLURM_JOB_ID"
# Generate Machinefile for mpi such that hosts are in the same
# order as if run via srun
srun -l /bin/hostname | sort -n | awk '{print $2}' > $MACHINEFILE
echo -e "\ngenerated machinefile for MPI, $MACHINEFILE"; ls -lt $PWD/$MACHINEFILE; echo "containing:"; cat $MACHINEFILE

module purge
module add GVR/v1.0-1
module add  intel/v2021.1
echo -e "\n\n loaded modules:"
module list

 # echo -e "Disk space: df -h $SCM_TMPDIR "
 # df -h $SCM_TMPDIR

ulimit -s unlimited

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"

# CPU model, total numer of CPUs, number of allocated CPUs
echo -e "The node's CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo "This node has total NPROCS=$NPROCS CPUs available for an EXCLUSIVE job."
echo "Based on reserved memory, this node got $SLURM_CPUS_ON_NODE CPUs allocated for SLURM calculations."
echo "This job wants SLURM_NTASKS=$SLURM_NTASKS threads . "

RATIO=$(( $SLURM_CPUS_ON_NODE / $SLURM_NTASKS ))
echo -e "Ratio SLURM_CPUS_ON_NODE/SLURM_NTASKS=$SLURM_CPUS_ON_NODE/$SLURM_NTASKS=\c";echo $(( $SLURM_CPUS_ON_NODE / $SLURM_NTASKS ))
echo -e $RATIO
# set OpenMP threads accordingly
#export USE_OPENMP=1
export OPENBLAS_NUM_THREADS=$RATIO
#echo -e "\nFor openblas internal parallelization, OPENBLAS_NUM_THREADS=$OPENBLAS_NUM_THREADS"


echo -e "\n The total memory at the node (in GB)"
free -t -g
echo -e "\n"

FHIAIMS=/lustre/home/user/m/milias/work/software/fhi-aims/fhi-aims.240507/build_intelmpi/aims.240507.scalapack.mpi.x
echo -e "\nldd $FHIAIMS :"
ldd  $FHIAIMS

#echo -e "\n ifort -V: \c"; ifort -V

echo -e "\nMy PATH=$PATH\n"
echo -e "Python -v :\c"; python -V
echo -e "\n mpirun ? \c"; which mpirun; mpirun --version
echo



export NSCM=$SLURM_CPUS_ON_NODE
#echo -e "\n Running  $ADFBIN/ams.exe $AMSHOME/examples/band/NaCl/NaCl.run  with NSCM=$SLURM_CPUS_ON_NODE"
#$ADFBIN/ams.exe $AMSHOME/examples/band/NaCl/NaCl.run

export OMPI_MCA_oob=^usock
export MKL_NUM_THREADS=1
export MKL_DYNAMIC=FALSE

echo -e "\n Running in SLURM_SUBMIT_DIR=$SLURM_SUBMIT_DIR : "

  # mpirun -np $SLURM_CPUS_ON_NODE $FHIAIMS < /dev/null | tee H2O_test.own
  # mpirun -np $SLURM_CPUS_ON_NODE $FHIAIMS < /dev/null 

project=fcc_Al
out=$project.$SLURM_CLUSTER_NAME.$SLURM_JOB_PARTITION.N$SLURM_JOB_NUM_NODES.n$SLURM_NTASKS.jid$SLURM_JOBID.out
# IntelMPI :
mpirun -f $MACHINEFILE  -np $SLURM_CPUS_ON_NODE $FHIAIMS < /dev/null | tee $out

exit
