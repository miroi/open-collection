#!/bin/bash

## jobname
#SBATCH -J libXCi

##  partition (queue)
##SBATCH -p cascade
##SBATCH -p knl
#SBATCH -p flnr-ice
##SBATCH -p slo-ice
##SBATCH -p ampere

##SBATCH --qos=dirac

## max. execution time
##SBATCH -t 3-00:00:00
#SBATCH -t 0-04:00:00

##SBATCH --exclusive

##SBATCH --mem=640GB
#SBATCH --mem=16GB

##SBATCH --mem-per-cpu=4GB

# do not restart in the case of nodefail!
##SBATCH --no-requeue
##SBATCH --no-kill

#
#SBATCH -N 1
#SBATCH --ntasks-per-node=4


## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

## E-mail
#SBATCH --mail-user=milias@theor.jinr.ru
#SBATCH --mail-type=ALL

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"

echo -e "\nJob user is SLURM_JOB_USER= $SLURM_JOB_USER"
echo -e "User job SLURM_JOB_NAME=$SLURM_JOB_NAME has assigned ID SLURM_JOBID=$SLURM_JOBID"
echo -e "This job was submitted from the computer SLURM_SUBMIT_HOST=$SLURM_SUBMIT_HOST"
echo -e "and from the home directory SLURM_SUBMIT_DIR:"
echo -e "$SLURM_SUBMIT_DIR"

echo -e "Job is running on the cluster compute node: SLURM_CLUSTER_NAME=$SLURM_CLUSTER_NAME"
echo -e "and is employing SLURM_JOB_NUM_NODES=$SLURM_JOB_NUM_NODES node/nodes:"
echo -e "SLURM_JOB_NODELIST = $SLURM_JOB_NODELIST"
echo -e "Job partition is SLURM_JOB_PARTITION=$SLURM_JOB_PARTITION \n"
echo -e "Number of allocated CPUs on each single node, SLURM_CPUS_ON_NODE=$SLURM_CPUS_ON_NODE ."
echo -e "Number of all reserved threads over ALL nodes, SLURM_NTASKS=$SLURM_NTASKS ."
echo -e "Job has reserved memory per node, SLURM_MEM_PER_NODE=$SLURM_MEM_PER_NODE MB of memory"


# ... does not work on GPU
#MACHINEFILE="nodes.$SLURM_JOB_PARTITION.$SLURM_JOB_ID"
# Generate Machinefile for mpi such that hosts are in the same
# order as if run via srun
#srun -l /bin/hostname | sort -n | awk '{print $2}' > $MACHINEFILE
#echo -e "\ngenerated machinefile for MPI, $MACHINEFILE"; ls -lt $PWD/$MACHINEFILE; echo "containing:"; cat $MACHINEFILE

# CPU model, total numer of CPUs, number of allocated CPUs
echo -e "The node's CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo "BTW, this node has total $NPROCS CPUs available for an EXCLUSIVE job."
echo "Based on reserved memory, this node got $SLURM_CPUS_ON_NODE CPUs allocated for SLURM calculations."
echo "This job wants SLURM_NTASKS=$SLURM_NTASKS threads . "

#RATIO=$(( $SLURM_CPUS_ON_NODE / $SLURM_NTASKS ))
#echo -e "Ratio SLURM_CPUS_ON_NODE/SLURM_NTASKS=$SLURM_CPUS_ON_NODE/$SLURM_NTASKS=\c";echo $(( $SLURM_CPUS_ON_NODE / $SLURM_NTASKS ))
#echo -e $RATIO
# set OpenMP threads accordingly
#export USE_OPENMP=1
#export OPENBLAS_NUM_THREADS=$RATIO
#echo -e "\nFor openblas internal parallelization, OPENBLAS_NUM_THREADS=$OPENBLAS_NUM_THREADS"

echo -e "\n adding git module from the  source /cvmfs/nica.jinr.ru/sw/os/login.sh :"
source /cvmfs/nica.jinr.ru/sw/os/login.sh
#echo -e "\n all modules, including /cvmfs/nica.jinr.ru/sw/os/login.sh"; module avail

module purge
module load GVR/v1.0-1
module load Python/v3.10.13
module load intel/oneapi 
#module load gcc/v12.3.0
module load CMake/v3.29.2

echo -e "\n\n All loaded modules:"
module list

echo -e "\n\n cmake ? \c"; which cmake; cmake --version
echo -e "\n python ? ; python -v :\c"; which python; python -V
#echo -e "\n mpiifort ? ;  mpiifort -V: \c";which mpiifort;  mpiifort -V
#echo -e "\n gfortran ? : \c"; which gfortran;  gfortran --version
echo -e "\n ifort ? : \c"; which ifort;  ifort -V
echo -e "\n icc ? : \c"; which icc;  icc -V
#echo -e "\n gcc ? : \c"; which gcc;  gcc --version
#echo -e "\n mpirun ? \c"; which mpirun; mpirun --version


echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"

echo -e "\n The total memory at the node (in GB)"
free -t -g
echo -e "\n"

echo -e "\n\n My PATH=$PATH\n"
echo -e "\n My LD_LIBRARY_PATH=$LD_LIBRARY_PATH "

LIBXC_DIR=/lustre/home/user/m/milias/work/software/libxc
LIBXC=$LIBXC_DIR/libxc-7.0.0

BUILD=build_intel2021.4
INSTALL_DIR=$LIBXC_DIR/install_$BUILD

cd $LIBXC_DIR
echo -e "\nI am in the directory LIBXC_DIR=$LIBXC_DIR; verify via pwd:";pwd

if [ -d "install_$BUILD" ]; then
  echo -e "\nPREVIOUS directory 'install_$BUILD' exists. Removing it.After recreating it again by mkdir."
  rm -rf install_$BUILD
else
  echo -e "\nPREVIOUS directory 'install_$BUILD' does not exist - will be created by mkdir."
fi
mkdir install_$BUILD

cd $LIBXC
echo -e "\n I am in the directory LIBXC=$LIBXC : \c"; pwd
echo -e "Content :"; ls -lt

if [ -d "$BUILD" ]; then
  echo -e "\nDirectory '$BUILD' exists. Removing it..."
  rm -rf "$BUILD"
else
  echo -e "\nDirectory '$BUILD' does not exist - will be created by CMake."
fi


echo -e "\n Doing CMake buildup in LIBXC=$LIBXC :"
# https://libxc.gitlab.io/installation/#building-with-cmake

#cmake -H. -B $BUILD -D CMAKE_C_COMPILER=gcc -D CMAKE_Fortran_COMPILER=gfortran -D ENABLE_FORTRAN=ON  -D CMAKE_INSTALL_PREFIX=$INSTALL_DIR
cmake -H. -B $BUILD -D CMAKE_C_COMPILER=icc -D CMAKE_Fortran_COMPILER=ifort -D ENABLE_FORTRAN=ON  -D CMAKE_INSTALL_PREFIX=$INSTALL_DIR

cd $BUILD
echo -e "\n Im in directory BUILD=$BUILD; pwd=";pwd

echo -e "\n running make"
make

echo -e "\n running make test"
make test

echo -e "\n running make install"
make install

exit
