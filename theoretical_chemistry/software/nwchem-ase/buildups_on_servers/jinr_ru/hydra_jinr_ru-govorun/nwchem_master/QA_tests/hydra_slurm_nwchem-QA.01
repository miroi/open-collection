#!/bin/bash

#SBATCH -J NwQA

##  partition (queue)
##SBATCH -p cascade
##SBATCH -p knl
##SBATCH -p flnr-ice
#SBATCH -p flnr-sod

## max. execution time
##SBATCH -t 7-00:00:00
#SBATCH -t 4-00:00:00

##SBATCH --exclusive

#SBATCH --mem=32GB
##SBATCH --mem=64GB
##SBATCH --mem=164GB
##SBATCH --mem=16GB

# do not restart in the case of nodefail!
#SBATCH --no-requeue
#SBATCH --no-kill

#SBATCH -N 1 -n 4
#SBATCH  --sockets-per-node=1

## memory NO !!!
##SBATCH --mem-per-cpu=28G

## stdout/stderr output file
##SBATCH -o log_slurm_job.%j.%N.std_out_err
#SBATCH -o log_slurm_job.%j.%N.std_out

## mail
#SBATCH --mail-user=milias@theor.jinr.ru
#SBATCH --mail-type=ALL


echo -e "\n Job user is $SLURM_JOB_USER and his job $SLURM_JOB_NAME has assigned ID $SLURM_JOBID"
echo This job was submitted from the computer $SLURM_SUBMIT_HOST
echo and from the home directory:
echo $SLURM_SUBMIT_DIR
echo
echo It is running on the cluster compute node:
echo $SLURM_CLUSTER_NAME
echo and is employing $SLURM_JOB_NUM_NODES node/nodes:
echo $SLURM_JOB_NODELIST
echo
echo -e "Job partition is $SLURM_JOB_PARTITION \n"
echo The job requests $SLURM_CPUS_ON_NODE CPU per task.

#echo "modules at disposal:"
#module avail
#echo
#
module purge
#module load openmpi/v2.1.2-2
module add GVR/v1.0-1
#module add Python/v3.10.13
module add intel/oneapi
module add ELPA/v2025.01.002_oneapi
#module add Python/v3.10.2
#module add gcc/v10.2.0
#module unload openmpi/v1.8.8-1
echo -e "\n\n loaded modules for NWChem compulation :"
module list
  # server's Intel compiler
  echo -e "ifort ?"; which ifort; ifort --version
  echo -e "icc ?"; which icc; icc --version
  echo -e "mpiifort ?"; which mpiifort; mpiifort -V
  echo -e "mpirun ?";  which mpirun; mpirun --version
  echo -e "MKL library, MKLROOT=${MKLROOT}"

#           NWChem on hydra.jinr.ru
# 
 # echo -e "\n\n*** hydra.jinr.ru setting for NWChem compilation:   ***"

  #export NWCHEM_TOP=/home/milias/Work/qch/software/nwchem_suite/nwchem-7.0.2
  #export NWCHEM_TOP=/lustre/home/user/m/milias/work/software/nwchem/nwchem-7.2.3-release
  export NWCHEM_TOP=/zfs/scratch/HybriLITWorkshop2025/milias/software/nwchem/cloned/nwchem_master
  export NWCHEM_EXECUTABLE=$NWCHEM_TOP/bin/LINUX64/nwchem
  echo -e "\n NWCHEM_TOP=$NWCHEM_TOP"

  echo -e "\n\n  ldd NWCHEM_EXECUTABLE = $NWCHEM_EXECUTABLE :"
  ldd  $NWCHEM_EXECUTABLE

cd $NWCHEM_TOP/QA
echo -e "\n\n I am in NWCHEM_TOP/QA=$NWCHEM_TOP/QA;  for control pwd:"; pwd

nproc=${SLURM_CPUS_ON_NODE}
if [ "$1" != "" ] ; then
  nproc=$1
fi
echo -e "nproc=$nproc"
. ./domknwchemenv
./domknwchemrc
status=$?
if [[ $status != 0 ]]; then
  echo "ERROR: The script $NWCHEM_TOP/QA/domknwchemrc failed. Exiting."
  exit $status
fi

echo -e "\n\n Launching NWChem QA ./doqmtests.mpi $nproc ... at \c";date
#./doqmtests.mpi $nproc fast
./doqmtests.mpi $nproc 

echo -e "\n\n QA tests finished at \c";date

  exit 0
