#!/bin/bash

#SBATCH -J NWase

##  partition (queue)
#SBATCH -p cascade
##SBATCH -p flnr-ice

## max. execution time
##SBATCH -t 10-00:00:00
##SBATCH -t 7-00:00:00
#SBATCH -t 0-01:00:00

##SBATCH --exclusive

#SBATCH --mem=8GB
##SBATCH --mem=24GB
##SBATCH --mem=16GB

# do not restart in the case of nodefail!
#SBATCH --no-requeue
#SBATCH --no-kill

#SBATCH -N 1 -n 2
#SBATCH  --sockets-per-node=1


## memory NO !!!
##SBATCH --mem-per-cpu=28G

## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

## mail
#SBATCH --mail-user=miro.ilias@gmail.com
#SBATCH --mail-type=ALL

 
echo Job user is $SLURM_JOB_USER and his job $SLURM_JOB_NAME has assigned ID $SLURM_JOBID
echo This job was submitted from the computer $SLURM_SUBMIT_HOST
echo and from the home directory:
echo $SLURM_SUBMIT_DIR
echo
echo It is running on the cluster compute node:
echo $SLURM_CLUSTER_NAME
echo and is employing $SLURM_JOB_NUM_NODES node/nodes:
echo $SLURM_JOB_NODELIST
echo
echo -e "Job partition is $SLURM_JOB_PARTITION \n"
echo The job requests $SLURM_CPUS_ON_NODE CPU per task.
echo -e "SLURM_NTASKS=$SLURM_NTASKS"

#echo "modules at disposal:"
#module avail
#echo

#
module purge
module add GVR/v1.0-1
module add ELPA/v2020.05.001_intel2018_python365
module unload openmpi/v1.8.8-1
module add Python/v3.10.2

echo -e "\n\n loaded modules:"
module list

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"

# CPU model, total numer of CPUs, number of allocated CPUs
echo -e "The node's CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo "This node has total $NPROCS CPUs available for an EXCLUSIVE job."
#echo "This node has $SLURM_CPUS_ON_NODE CPUs allocated for SLURM calculations."

echo -e "\n The total memory at the node (in GB)"
free -t -g
echo -e "\n"

export PATH=/lustre/home/user/m/milias/.local/bin/ase:$PATH
echo -e "\nMy PATH=$PATH\n"
echo -e "Python -v :\c"; python -V
#echo -e "\n mpirun ? \c"; which mpirun; mpirun --version
echo -e "\n ase info :"; ase info

export NWCHEM_TOP=/lustre/home/user/m/milias/work/software/nwchem/nwchem_master
#export ASE_NWCHEM_COMMAND="mpirun -n $SLURM_NTASKS nwchem PREFIX.nwi > PREFIX.nwo"
export ASE_NWCHEM_COMMAND="mpirun -n  $SLURM_CPUS_ON_NODE nwchem PREFIX.nwi > PREFIX.nwo"

echo -e "\n Running in  $SLURM_SUBMIT_DIR : "

  python3 ase_nwchem_h2_bfgs.py

exit
