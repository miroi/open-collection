#!/bin/bash

#SBATCH -J orca

##  partition (queue)
##SBATCH -p cascade
##SBATCH -p knl
##SBATCH -p flnr-ice
##SBATCH -p slo-ice
#SBATCH -p flnr-sod

## max. execution time
##SBATCH -t 1-00:00:00
#SBATCH -t 0-04:00:00

#SBATCH --mem=32GB

# do not restart in the case of nodefail!
#SBATCH --no-requeue
#SBATCH --no-kill

#SBATCH -N 1 -n 4
#SBATCH  --sockets-per-node=1

#SBATCH -o log_slurm_job.%j.%N.std_out

## E-mail
#SBATCH --mail-user=milias@theor.jinr.ru
#SBATCH --mail-type=ALL

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"

echo -e "\nJob user is SLURM_JOB_USER= $SLURM_JOB_USER"
echo -e "User job SLURM_JOB_NAME=$SLURM_JOB_NAME has assigned ID SLURM_JOBID=$SLURM_JOBID"
echo -e "This job was submitted from the computer SLURM_SUBMIT_HOST=$SLURM_SUBMIT_HOST"
echo -e "and from the home directory SLURM_SUBMIT_DIR:"
echo -e "$SLURM_SUBMIT_DIR"

echo -e "Job is running on the cluster compute node: SLURM_CLUSTER_NAME=$SLURM_CLUSTER_NAME"
echo -e "and is employing SLURM_JOB_NUM_NODES=$SLURM_JOB_NUM_NODES node/nodes:"
echo -e "SLURM_JOB_NODELIST = $SLURM_JOB_NODELIST"
echo -e "Job partition is SLURM_JOB_PARTITION=$SLURM_JOB_PARTITION \n"
echo -e "Number of allocated CPUs on each single node, SLURM_CPUS_ON_NODE=$SLURM_CPUS_ON_NODE ."
echo -e "Number of all reserved threads over ALL nodes, SLURM_NTASKS=$SLURM_NTASKS ."
echo -e "Job has reserved memory per node, SLURM_MEM_PER_NODE=$SLURM_MEM_PER_NODE MB of memory"

# CPU model, total numer of CPUs, number of allocated CPUs
echo -e "The node's CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo "BTW, this node has total $NPROCS CPUs available for an EXCLUSIVE job."
echo "Based on reserved memory, this node got $SLURM_CPUS_ON_NODE CPUs allocated for SLURM calculations."
echo "This job wants SLURM_NTASKS=$SLURM_NTASKS threads . "


echo -e "\n\n The total memory at the node (in GB)"
free -t -g
echo -e "\n"


#echo "modules at disposal:"
#module avail
#echo

#
module purge
module add GVR/v1.0-1
module add Python/v3.10.13
#module add intel/oneapi 
#module add openmpi/v4.1.6_gcc1230

echo -e "\n\n loaded modules:"
module list

#echo -e "\n mpirun ? "; which mpirun; mpirun --version

ulimit -s unlimited

cd $SLURM_SUBMIT_DIR

#export TMPDIR=/lustre/home/user/m/milias/scratch
#df -h /tmp; df -h $TMPDIR
#NMPI=$SLURM_CPUS_ON_NODE

export ORCADIR=/zfs/scratch/HybriLITWorkshop2025/milias/software/orca/orca-6.1.0-f.0_linux_x86-64/bin

#export LD_LIBRARY_PATH=/lustre/home/user/m/milias/work/software/mopac/git_cloned/install/lib64:$LD_LIBRARY_PATH

echo -e "\n\n ldd $ORCADIR/orca :"
ldd $ORCADIR/orca

#project=hellowater
#project=benzene_opt_freq
project=benzene_opt_raman

echo -e "\n\n running: $ORCADIR/orca $project.inp  at \c"; date; echo
$ORCADIR/orca $project.inp

echo -e "\n Finished at \c"; date
exit
