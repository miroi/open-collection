
Running on host comp16.grid.umb.sk
Time is Wed Feb 16 11:02:04 CET 2022 

Job user is milias and his job Test ORCA 5.0.2 has assigned ID 10616
This job was submitted from the computer login.grid.umb.sk
and from the home directory:
/home/milias/Work/open-collection/theoretical_chemistry/software_runs/orca/test_run

It is running on the cluster compute node:
gridbb
and is employing 1 node/nodes:
comp16
  Job partition is compute 

The node's CPU model name	: Intel(R) Xeon(R) CPU           X5670  @ 2.93GHz
This node has total 12 CPUs available for an EXCLUSIVE job on this node.
This node has 8 CPUs allocated for this SLURM job.

 The job requests SLURM_MEM_PER_NODE=2048 memory.

 The total memory at the node (in GB)
              total        used        free      shared  buff/cache   available
Mem:             47           1          39           3           5          41
Swap:             0           0           0
Total:           47           1          39


ipcs -l

------ Messages Limits --------
max queues system wide = 32000
max size of message (bytes) = 8192
default max size of queue (bytes) = 16384

------ Shared Memory Limits --------
max number of segments = 4096
max seg size (kbytes) = 18014398509465599
max total shared memory (kbytes) = 18014398442373116
min seg size (bytes) = 1

------ Semaphore Limits --------
max number of arrays = 128
max semaphores per array = 250
max semaphores system wide = 32000
max ops per semop call = 32
semaphore max value = 32767


PATH=/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/sbudzak/bin/openmpi/4.1.1/bin:/mnt/apps/umb_chem/orca/orca_5_0_2
LD_LIBRARY_PATH=/mnt/apps/umb_chem/orca/orca_5_0_2:/home/sbudzak/bin/openmpi/4.1.1/lib
mpif90 ?
/home/sbudzak/bin/openmpi/4.1.1/bin/mpif90
GNU Fortran (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
Copyright (C) 2015 Free Software Foundation, Inc.

GNU Fortran comes with NO WARRANTY, to the extent permitted by law.
You may redistribute copies of GNU Fortran
under the terms of the GNU General Public License.
For more information about these matters, see the file named COPYING

mpirun ?
/home/sbudzak/bin/openmpi/4.1.1/bin/mpirun
mpirun (Open MPI) 4.1.1

Report bugs to http://www.open-mpi.org/community/help/
ldd orca = ldd /mnt/apps/umb_chem/orca/orca_5_0_2/orca
	not a dynamic executable

 I am in local scratch directory:/mnt/local/milias/orca_job-10616
/mnt/local/milias/orca_job-10616
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda1       1.1T  2.2G  1.1T   1% /mnt/local

real	0m10.002s
user	0m0.000s
sys	0m0.001s

 List of files in the scratch directory, /mnt/local/milias/orca_job-10616:
total 760
-rw-rw-r-- 1 milias milias   4946 Feb 16 11:02 CO_property.txt
-rw-rw-r-- 1 milias milias 727256 Feb 16 11:02 CO.gbw
-rw-rw-r-- 1 milias milias  35438 Feb 16 11:02 CO.densities
-rw-rw-r-- 1 milias milias     96 Feb 16 11:02 CO.inp
Size of the /mnt/local/milias/orca_job-10616 :
764K	/mnt/local/milias/orca_job-10616
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda1       1.1T  2.2G  1.1T   1% /mnt/local
