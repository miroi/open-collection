#!/bin/bash

## jobname
#SBATCH -J QEgpuX

##  partition (queue)
##SBATCH -p cascade
##SBATCH -p knl
##SBATCH -p flnr-ice
##SBATCH -p slo-ice

#SBATCH -p ampere
#SBATCH -w ampere02

##SBATCH -p dgx
##SBATCH -w dgx03


##SBATCH --qos=dirac

## max. execution time
##SBATCH -t 3-00:00:00
#SBATCH -t 0-02:00:00

##SBATCH --exclusive

##SBATCH --mem=640GB
#SBATCH --mem=64GB

##SBATCH --mem-per-cpu=4GB

# do not restart in the case of nodefail!
##SBATCH --no-requeue
##SBATCH --no-kill

#
#SBATCH -N 1
#SBATCH --ntasks-per-node=4

#Set the number of GPUs per node
#SBATCH --gres=gpu:1

## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

## E-mail
#SBATCH --mail-user=milias@theor.jinr.ru
#SBATCH --mail-type=ALL

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"

echo -e "\nJob user is SLURM_JOB_USER= $SLURM_JOB_USER"
echo -e "User job SLURM_JOB_NAME=$SLURM_JOB_NAME has assigned ID SLURM_JOBID=$SLURM_JOBID"
echo -e "This job was submitted from the computer SLURM_SUBMIT_HOST=$SLURM_SUBMIT_HOST"
echo -e "and from the home directory SLURM_SUBMIT_DIR:"
echo -e "$SLURM_SUBMIT_DIR"

echo -e "Job is running on the cluster compute node: SLURM_CLUSTER_NAME=$SLURM_CLUSTER_NAME"
echo -e "and is employing SLURM_JOB_NUM_NODES=$SLURM_JOB_NUM_NODES node/nodes:"
echo -e "SLURM_JOB_NODELIST = $SLURM_JOB_NODELIST"
echo -e "Job partition is SLURM_JOB_PARTITION=$SLURM_JOB_PARTITION \n"
echo -e "Number of allocated CPUs on each single node, SLURM_CPUS_ON_NODE=$SLURM_CPUS_ON_NODE ."
echo -e "Number of all reserved threads over ALL nodes, SLURM_NTASKS=$SLURM_NTASKS ."
echo -e "Job has reserved memory per node, SLURM_MEM_PER_NODE=$SLURM_MEM_PER_NODE MB of memory"


# ... does not work on GPU
#MACHINEFILE="nodes.$SLURM_JOB_PARTITION.$SLURM_JOB_ID"
# Generate Machinefile for mpi such that hosts are in the same
# order as if run via srun
#srun -l /bin/hostname | sort -n | awk '{print $2}' > $MACHINEFILE
#echo -e "\ngenerated machinefile for MPI, $MACHINEFILE"; ls -lt $PWD/$MACHINEFILE; echo "containing:"; cat $MACHINEFILE

# CPU model, total numer of CPUs, number of allocated CPUs
echo -e "The node's CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo "BTW, this node has total $NPROCS CPUs available for an EXCLUSIVE job."
echo "Based on reserved memory, this node got $SLURM_CPUS_ON_NODE CPUs allocated for SLURM calculations."
echo "This job wants SLURM_NTASKS=$SLURM_NTASKS threads . "

#RATIO=$(( $SLURM_CPUS_ON_NODE / $SLURM_NTASKS ))
#echo -e "Ratio SLURM_CPUS_ON_NODE/SLURM_NTASKS=$SLURM_CPUS_ON_NODE/$SLURM_NTASKS=\c";echo $(( $SLURM_CPUS_ON_NODE / $SLURM_NTASKS ))
#echo -e $RATIO
# set OpenMP threads accordingly
#export USE_OPENMP=1
#export OPENBLAS_NUM_THREADS=$RATIO
#echo -e "\nFor openblas internal parallelization, OPENBLAS_NUM_THREADS=$OPENBLAS_NUM_THREADS"

echo -e "\n adding git module from source /cvmfs/nica.jinr.ru/sw/os/login.sh :"
source /cvmfs/nica.jinr.ru/sw/os/login.sh
echo -e "\n all modules, including /cvmfs/nica.jinr.ru/sw/os/login.sh"; module avail


module purge
#module load GVR/v1.0-1 gcc/v11.2.0 fftw/v3.3.10_gcc1120 LAPACK/v3.9.0 openmpi/v4.1.1_gcc1120
#module load GVR/v1.0-1 intel/v2021.1 Python/v3.10.13 CMake/v3.29.2
module load GVR/v1.0-1 Python/v3.10.13 CMake/v3.29.2 
#module load LAPACK/v3.12.0_gcc1230

#module load fftw/v3.3.10_gcc1120
#module load fftw/v3.3.10_openmpi503

module load git

# NVHPC with OpenMPI 
module load cuda/v12.4

# IntelMPI+MKL
#module load intel/oneapi

echo -e "\n\n All loaded modules:"
module list

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"

echo -e "\n The total memory at the node (in GB)"
free -t -g
echo -e "\n"

echo -e "\n ls $QE :"; ls $QE

echo -e "\n\n My PATH=$PATH\n"
echo -e "\n My LD_LIBRARY_PATH=$LD_LIBRARY_PATH "
echo -e "\n python -v :\c"; python -V
#echo -e "\n ifort -V: \c"; ifort -V
#echo -e "\n mpiifort -V: \c"; mpiifort -V
#echo -e "\n Intel MKL library ? MKLROOT=$MKLROOT "
#echo -e "\n mpirun ? \c"; which mpirun; mpirun --version
echo -e "\n cmake ? \c"; which cmake; cmake --version
echo -e "\n git ? \c"; which git; git --version

echo -e "\n nvidia-smi ? \c"; which nvidia-smi; nvidia-smi

echo -e "\n\n nvcc ? ; \c"; which nvcc; nvcc --version
echo -e "\n nvfortran ? ; \c"; which nvfortran; nvfortran --version
echo -e "\n pgfortran ? ; \c"; which pgfortran; pgfortran --version

echo -e "\n ls  /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/cuda/v12.1/Linux_x86_64/23.5/compilers:";
ls  /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/cuda/v12.1/Linux_x86_64/23.5/compilers
echo -e "\n ls  /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/cuda/v12.1/Linux_x86_64/23.5/compilers/bin:";
ls  /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/cuda/v12.1/Linux_x86_64/23.5/compilers/bin
echo -e "\n ls  /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/cuda/v12.1/Linux_x86_64/23.5/compilers/lib:";
ls  /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/cuda/v12.1/Linux_x86_64/23.5/compilers/lib

# for running jobs from your homedir, use ...
#cd $SLURM_SUBMIT_DIR

#QE=/lustre/home/user/m/milias/work/software/quantum-espresso/qe-7.3.1
QE=/lustre/home/user/m/milias/work/software/quantum-espresso/qe-develop/q-e
BUILD=build_gpu_cuda_nvfor_nvcc_cuda-openmpi

cd $QE; echo -e "\n\nI am in the directory QE=$QE"; pwd

if [ -d "$BUILD" ]; then
  echo -e "\n The directory $QE/$BUILD does exist."
  echo -e "Its old content :"; ls -lt $QE/$BUILD/.
  echo -e "Deleting it first !"
   /bin/rm -rf $BUILD
   mkdir $BUILD
   echo -e "\n The directory $QE/$BUILD was recreated."
else
  echo -e "\n The directory $QE/$BUILD does not exist, so creating it."
  mkdir $BUILD
fi

cd $BUILD
echo -e "\n Current directory where this SLURM job is running QE/BUILD=$QE/$BUILD";pwd
echo " It has the disk space of (df -h) :"; df -h .

echo -e "\n\n\n Running cmake :"
#cmake -DQE_ENABLE_CUDA=ON  -DCMAKE_C_COMPILER=nvc -DCMAKE_Fortran_COMPILER=mpif90 -DQE_ENABLE_MPI_GPU_AWARE=ON ..
#cmake -DQE_ENABLE_CUDA=ON  -DCMAKE_C_COMPILER=nvc -DCMAKE_Fortran_COMPILER=nvfortran -DQE_ENABLE_MPI_GPU_AWARE=ON ..
#cmake -DQE_ENABLE_CUDA=ON  -DCMAKE_C_COMPILER=nvc -DQE_ENABLE_MPI_GPU_AWARE=ON ..
#cmake -DQE_ENABLE_CUDA=ON  -DCMAKE_C_COMPILER=nvc -DCMAKE_Fortran_COMPILER=pgfortran -DQE_ENABLE_MPI_GPU_AWARE=OFF ..
#cmake -DQE_ENABLE_CUDA=ON  -DCMAKE_C_COMPILER=nvc -DCMAKE_Fortran_COMPILER=mpifort  ..
#cmake -DQE_ENABLE_CUDA=ON  -DCMAKE_Fortran_COMPILER=mpifort  ..
#cmake -DQE_ENABLE_CUDA=ON  -DCMAKE_C_COMPILER=nvcc -DCMAKE_Fortran_COMPILER=pgfortran  ..
#cmake -DCMAKE_C_COMPILER=nvc -DCMAKE_Fortran_COMPILER=nvfortran -DQE_ENABLE_MPI=OFF -DQE_ENABLE_OPENMP=ON -DQE_ENABLE_CUDA=ON -DQE_FFTW_VENDOR=Internal -DQE_LAPACK_INTERNAL=ON ..
# ... this is proper
#cmake -DCMAKE_C_COMPILER=nvc -DCMAKE_Fortran_COMPILER=pgfortran -DQE_ENABLE_MPI=OFF -DQE_ENABLE_OPENMP=ON -DQE_ENABLE_CUDA=ON ..
#cmake -DCMAKE_C_COMPILER=nvc -DCMAKE_Fortran_COMPILER=pgfortran -DQE_ENABLE_MPI=ON -DQE_ENABLE_OPENMP=ON -DQE_ENABLE_CUDA=ON -DQE_LAPACK_INTERNAL=OFF  -DQE_ENABLE_SCALAPACK=OFF -DQE_ENABLE_ELPA=OFF -DQE_ENABLE_LIBXC=ON -DLIBXC_ROOT=/lustre/home/user/m/milias/work/software/libxc  -DQE_ENABLE_HDF5=OFF ..

#cmake -DCMAKE_C_COMPILER=nvc -DCMAKE_Fortran_COMPILER=pgfortran -DQE_ENABLE_MPI=OFF -DQE_ENABLE_OPENMP=ON -DQE_ENABLE_CUDA=ON -DQE_LAPACK_INTERNAL=OFF  -DQE_ENABLE_SCALAPACK=OFF -DQE_ENABLE_ELPA=OFF -DQE_ENABLE_LIBXC=ON -DLIBXC_ROOT=/lustre/home/user/m/milias/work/software/libxc  -DQE_ENABLE_HDF5=OFF ..

#cmake -DCMAKE_C_COMPILER=nvc -DCMAKE_Fortran_COMPILER=pgfortran -DQE_ENABLE_MPI=OFF -DQE_ENABLE_OPENMP=ON -DQE_ENABLE_CUDA=ON -DQE_LAPACK_INTERNAL=OFF  -DQE_ENABLE_SCALAPACK=OFF -DQE_ENABLE_ELPA=OFF -DQE_ENABLE_LIBXC=OFF  -DQE_ENABLE_HDF5=OFF ..

#cmake -DCMAKE_C_COMPILER=nvc -DCMAKE_Fortran_COMPILER=nvfortran -DQE_ENABLE_MPI=OFF -DQE_ENABLE_OPENMP=ON -DQE_ENABLE_CUDA=ON -DQE_LAPACK_INTERNAL=OFF  -DQE_ENABLE_SCALAPACK=OFF -DQE_ENABLE_ELPA=OFF -DQE_ENABLE_LIBXC=OFF  -DQE_ENABLE_HDF5=OFF ..

cmake -DCMAKE_C_COMPILER=nvc -DCMAKE_Fortran_COMPILER=nvfortran -DQE_ENABLE_MPI=ON -DQE_ENABLE_OPENMP=ON -DQE_ENABLE_CUDA=ON  -DQE_ENABLE_LIBXC=OFF  -DQE_ENABLE_HDF5=OFF -DQE_FFTW_VENDOR=Internal  -DQE_ENABLE_SCALAPACK=ON ..

echo -e "\n\n  running :  make -j $SLURM_CPUS_ON_NODE "
make -j $SLURM_CPUS_ON_NODE

echo -e "\n\n  ls -lt $QE/$BUILD/bin : "
ls -lt $QE/$BUILD/bin

if [ -f "$QE/$BUILD/bin/pw.x" ]; then
  echo -e "\nFile pw.x exists. Fine, continuing"
else
  echo -e "\n Regular file pw.x does not exist, exit ! "; exit -2
fi

echo -e "\n\n ldd $QE/$BUILD/bin/pw.x :  "
ldd $QE/$BUILD/bin/pw.x

export OMP_NUM_THREADS=$SLURM_CPUS_ON_NODE
echo -e "\n\n OMP_NUM_THREADS=$OMP_NUM_THREADS"

#echo -e "\n\n running :  ctest  -j $SLURM_CPUS_ON_NODE : "
echo -e "\n\n running :  ctest -V   "
#ctest  -j $SLURM_CPUS_ON_NODE
ctest -V 

exit
