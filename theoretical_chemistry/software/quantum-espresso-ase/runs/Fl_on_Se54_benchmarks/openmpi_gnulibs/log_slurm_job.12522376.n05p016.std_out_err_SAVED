
Running on host n05p016
Time is Fri May 30 10:52:34 MSK 2025 


Job user is SLURM_JOB_USER= milias
User job SLURM_JOB_NAME=FlSe54qe4 has assigned ID SLURM_JOBID=12522376
This job was submitted from the computer SLURM_SUBMIT_HOST=space06.hydra.local
and from the home directory SLURM_SUBMIT_DIR:
/lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/quantum-espresso-ase/runs/Fl_on_Se54_benchmarks/openmpi_gnulibs
Job is running on the cluster compute node: SLURM_CLUSTER_NAME=gvr
and is employing SLURM_JOB_NUM_NODES=2 node/nodes:
SLURM_JOB_NODELIST = n05p[016-017]
Job partition is SLURM_JOB_PARTITION=slo-ice 

Number of allocated CPUs on each single node, SLURM_CPUS_ON_NODE=20 .
Number of all reserved threads over ALL nodes, SLURM_NTASKS=40 .
Job has reserved memory per node, SLURM_MEM_PER_NODE=122880 MB of memory

generated machinefile for MPI, nodes.slo-ice.12522376
-rw-r--r-- 1 milias hybrilit 320 May 30 10:52 /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/quantum-espresso-ase/runs/Fl_on_Se54_benchmarks/openmpi_gnulibs/nodes.slo-ice.12522376
containing:
n05p016
n05p016
n05p016
n05p016
n05p016
n05p016
n05p016
n05p016
n05p016
n05p016
n05p016
n05p016
n05p016
n05p016
n05p016
n05p016
n05p016
n05p016
n05p016
n05p016
n05p017
n05p017
n05p017
n05p017
n05p017
n05p017
n05p017
n05p017
n05p017
n05p017
n05p017
n05p017
n05p017
n05p017
n05p017
n05p017
n05p017
n05p017
n05p017
n05p017
The node's CPU model name	: Intel(R) Xeon(R) Platinum 8368Q CPU @ 2.60GHz
BTW, this node has total 152 CPUs available for an EXCLUSIVE job.
Based on reserved memory, this node got 20 CPUs allocated for SLURM calculations.
This job wants SLURM_NTASKS=40 threads . 


 loaded modules:
Currently Loaded Modulefiles:
  1) BASE/1.0                 4) LAPACK/v3.12.0_gcc1230
  2) gcc/v12.3.0              5) fftw/v3.3.7-5
  3) openmpi/v5.0.3_gcc1230

Running on host n05p016
Time is Fri May 30 10:52:35 MSK 2025 


 The total memory at the node (in GB)
              total        used        free      shared  buff/cache   available
Mem:           1991         108        1339           1         543        1879
Swap:             3           0           3
Total:         1995         108        1343



 ldd  /lustre/home/user/m/milias/work/software/quantum-espresso/qe-develop/q-e/build_openmpi/bin/pw.x :
	linux-vdso.so.1 =>  (0x00007ffcc676c000)
	libfftw3.so.3 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/fftw/v3.3.7-5/lib/libfftw3.so.3 (0x00002b9b410fb000)
	liblapack.so.3 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/LAPACK/v3.12.0_gcc1230/lib64/liblapack.so.3 (0x00002b9b413ff000)
	libblas.so.3 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/LAPACK/v3.12.0_gcc1230/lib64/libblas.so.3 (0x00002b9b41ce3000)
	libmpi_usempif08.so.40 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib/libmpi_usempif08.so.40 (0x00002b9b41f86000)
	libmpi_usempi_ignore_tkr.so.40 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib/libmpi_usempi_ignore_tkr.so.40 (0x00002b9b421c6000)
	libmpi_mpifh.so.40 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib/libmpi_mpifh.so.40 (0x00002b9b423d6000)
	libmpi.so.40 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib/libmpi.so.40 (0x00002b9b42641000)
	libgfortran.so.5 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/gcc/v12.3.0/lib64/libgfortran.so.5 (0x00002b9b42b9e000)
	libm.so.6 => /lib64/libm.so.6 (0x00002b9b43064000)
	libgcc_s.so.1 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/gcc/v12.3.0/lib64/libgcc_s.so.1 (0x00002b9b43366000)
	libquadmath.so.0 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/gcc/v12.3.0/lib64/libquadmath.so.0 (0x00002b9b43584000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x00002b9b437c9000)
	libc.so.6 => /lib64/libc.so.6 (0x00002b9b439e5000)
	libopen-pal.so.80 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib/libopen-pal.so.80 (0x00002b9b43db3000)
	librt.so.1 => /lib64/librt.so.1 (0x00002b9b440bc000)
	libpmix.so.2 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib/libpmix.so.2 (0x00002b9b442c4000)
	libutil.so.1 => /lib64/libutil.so.1 (0x00002b9b446f3000)
	libevent_core-2.1.so.7 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib/libevent_core-2.1.so.7 (0x00002b9b448f6000)
	libevent_pthreads-2.1.so.7 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib/libevent_pthreads-2.1.so.7 (0x00002b9b44b2b000)
	libhwloc.so.15 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib/libhwloc.so.15 (0x00002b9b44d2e000)
	libdl.so.2 => /lib64/libdl.so.2 (0x00002b9b44f87000)
	/lib64/ld-linux-x86-64.so.2 (0x00002b9b40ed7000)


 My PATH=/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/fftw/v3.3.7-5/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/gcc/v12.3.0/bin:/lustre/home/user/m/milias/work/software/wien2k/wien2k_23.2_intelserial_mkl:/lustre/home/user/m/milias/work/software/wien2k/wien2k_23.2_intelserial_mkl/SRC_structeditor/bin:/lustre/home/user/m/milias/work/software/wien2k/wien2k_23.2_intelserial_mkl/SRC_IRelast/script-elastic:/lustre/home/user/m/milias/work/software/ams/linux.openmpi/ams2021.107/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:.:/lustre/home/user/m/milias/work/software/wien2k/wien2k_23.2_intelserial_mkl:.:/lustre/home/user/m/milias/.local/bin:/lustre/home/user/m/milias/bin


My LD_LIBRARY_PATH=/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/fftw/v3.3.7-5/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/LAPACK/v3.12.0_gcc1230/lib64:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/LAPACK/v3.12.0_gcc1230/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/lib:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/gcc/v12.3.0/lib64:..... 
Python -v :Python 2.7.5

 mpif90 -V: gfortran: error: unrecognized command-line option ‘-V’
gfortran: fatal error: no input files
compilation terminated.

 mpirun ? /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v5.0.3_gcc1230/bin/mpirun
mpirun (Open MPI) 5.0.3

Report bugs to https://www.open-mpi.org/community/help/

 Current directory where this SLURM job is running /lustre/home/user/m/milias/work/projects/open-collection/theoretical_chemistry/software/quantum-espresso-ase/runs/Fl_on_Se54_benchmarks/openmpi_gnulibs
 It has the disk space of (df -h) :
Filesystem                               Size  Used Avail Use% Mounted on
10.220.25.3@o2ib,10.220.25.4@o2ib:/home  650T  523T  128T  81% /lustre/home


 Running QE pw.x OpenMPI parallel job:
Note: The following floating-point exceptions are signalling: IEEE_DENORMAL
Note: The following floating-point exceptions are signalling: IEEE_DENORMAL
Note: The following floating-point exceptions are signalling: IEEE_DENORMAL
Note: The following floating-point exceptions are signalling: IEEE_DENORMAL
Note: The following floating-point exceptions are signalling: IEEE_DENORMAL
Note: The following floating-point exceptions are signalling: IEEE_DENORMAL
Note: The following floating-point exceptions are signalling: IEEE_DENORMAL


 QE finished on : Sun Jun  1 22:53:07 MSK 2025
