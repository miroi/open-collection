#!/bin/bash

## jobname
#SBATCH -J FlSe54qe5

##  partition (queue)
##SBATCH -p cascade
##SBATCH -p knl
##SBATCH -p flnr-ice
#SBATCH -p slo-ice

##SBATCH --qos=dirac

## max. execution time
#SBATCH -t 3-00:00:00
##SBATCH -t 0-08:00:00
##SBATCH -t 30-00:00:00

##SBATCH --exclusive

##SBATCH --mem=640GB
#SBATCH --mem=300GB
##SBATCH --mem=16GB

##SBATCH --mem-per-cpu=10GB

# do not restart in the case of nodefail!
##SBATCH --no-requeue
##SBATCH --no-kill

#
#SBATCH -N 1
#SBATCH --ntasks-per-node=30


## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

## E-mail
#SBATCH --mail-user=milias@theor.jinr.ru
#SBATCH --mail-type=ALL

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"

echo -e "\nJob user is SLURM_JOB_USER= $SLURM_JOB_USER"
echo -e "User job SLURM_JOB_NAME=$SLURM_JOB_NAME has assigned ID SLURM_JOBID=$SLURM_JOBID"
echo -e "This job was submitted from the computer SLURM_SUBMIT_HOST=$SLURM_SUBMIT_HOST"
echo -e "and from the home directory SLURM_SUBMIT_DIR:"
echo -e "$SLURM_SUBMIT_DIR"

echo -e "Job is running on the cluster compute node: SLURM_CLUSTER_NAME=$SLURM_CLUSTER_NAME"
echo -e "and is employing SLURM_JOB_NUM_NODES=$SLURM_JOB_NUM_NODES node/nodes:"
echo -e "SLURM_JOB_NODELIST = $SLURM_JOB_NODELIST"
echo -e "Job partition is SLURM_JOB_PARTITION=$SLURM_JOB_PARTITION \n"
echo -e "Number of allocated CPUs on each single node, SLURM_CPUS_ON_NODE=$SLURM_CPUS_ON_NODE ."
echo -e "Number of all reserved threads over ALL nodes, SLURM_NTASKS=$SLURM_NTASKS ."
echo -e "Job has reserved memory per node, SLURM_MEM_PER_NODE=$SLURM_MEM_PER_NODE MB of memory"

MACHINEFILE="nodes.$SLURM_JOB_PARTITION.$SLURM_JOB_ID"
# Generate Machinefile for mpi such that hosts are in the same
# order as if run via srun
srun -l /bin/hostname | sort -n | awk '{print $2}' > $MACHINEFILE
echo -e "\ngenerated machinefile for MPI, $MACHINEFILE"; ls -lt $PWD/$MACHINEFILE; echo "containing:"; cat $MACHINEFILE

# CPU model, total numer of CPUs, number of allocated CPUs
echo -e "The node's CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo -e "lscpu :";lscpu
echo "BTW, this node has total $NPROCS CPUs available for an EXCLUSIVE job."
echo "Based on reserved memory, this node got $SLURM_CPUS_ON_NODE CPUs allocated for SLURM calculations."
echo "This job wants SLURM_NTASKS=$SLURM_NTASKS threads . "

#RATIO=$(( $SLURM_CPUS_ON_NODE / $SLURM_NTASKS ))
#echo -e "Ratio SLURM_CPUS_ON_NODE/SLURM_NTASKS=$SLURM_CPUS_ON_NODE/$SLURM_NTASKS=\c";echo $(( $SLURM_CPUS_ON_NODE / $SLURM_NTASKS ))
#echo -e $RATIO
# set OpenMP threads accordingly
#export USE_OPENMP=1
#export OPENBLAS_NUM_THREADS=$RATIO
#echo -e "\nFor openblas internal parallelization, OPENBLAS_NUM_THREADS=$OPENBLAS_NUM_THREADS"


module purge
#module load GVR/v1.0-1 gcc/v11.2.0 fftw/v3.3.10_gcc1120 LAPACK/v3.9.0 openmpi/v4.1.1_gcc1120
module load GVR/v1.0-1 intel/v2021.1 Python/v3.10.13 ELPA/v2025.01.002_oneapi


echo -e "\n\n loaded modules:"
module list

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"

echo -e "\n The total memory at the node (in GB)"
free -t -g
echo -e "\n"

#QE=/lustre/home/user/m/milias/work/software/quantum-espresso/openmpi/qe-7.3.1
#QE=/lustre/home/user/m/milias/work/software/quantum-espresso/qe-7.3.1
#QE=/lustre/home/user/m/milias/work/software/quantum-espresso/qe-develop/q-e/build_intelmpi_mkl
#QE=/lustre/home/user/m/milias/work/software/quantum-espresso/qe-develop/q-e/build_intelmpi_mkl_xh
#QE=/lustre/home/user/m/milias/work/software/quantum-espresso/qe-develop/q-e/build_intelmpimkl_dbg
#QE=/lustre/home/user/m/milias/work/software/quantum-espresso/qe-develop/q-e/build_intelmpi_mkl_scalapack
#QE=/lustre/home/user/m/milias/work/software/quantum-espresso/qe-develop/q-e/build_intelmpi_mkl_elpa
QE=/lustre/home/user/m/milias/work/software/quantum-espresso/qe-develop/q-e/build_intelmpi_mkl_elpa_xh

echo -e "\n ldd  $QE/bin/pw.x :"
ldd  $QE/bin/pw.x

echo -e "\n\n My PATH=$PATH\n"
echo -e "\nMy LD_LIBRARY_PATH=$LD_LIBRARY_PATH "
echo -e "Python -v :\c"; python -V
echo -e "\n ifort -V: \c"; ifort -V
echo -e "\n mpiifort -V: \c"; mpiifort -V
echo -e "\n mpirun ? \c"; which mpirun; mpirun --version

# for running jobs from your homedir, use ...
cd $SLURM_SUBMIT_DIR

echo -e "\n Current directory where this SLURM job is running `pwd`"
echo " It has the disk space of (df -h) :"
df -h .
echo

# https://community.intel.com/t5/Intel-MPI-Library/Unable-to-run-bstrap-proxy-error-with-intel-oneapi-mpi-2021-8/m-p/1538989/highlight/true#M11093
#export I_MPI_HYDRA_IFACE="ib0"

project=../Se54_r_Fl_2steprelax
inp=$project.in
ln -sf ../Fl-6spd_r.upf  .
ln -sf ../Se_r.upf  .

#out=$project.$SLURM_CLUSTER_NAME.$SLURM_JOBID.out$SLURM_CPUS_ON_NODE
#out=$project.$SLURM_CLUSTER_NAME.$SLURM_JOB_PARTITION.N$SLURM_JOB_NUM_NODES.n$SLURM_NTASKS.omp$OPENBLAS_NUM_THREADS.id$SLURM_JOBID.out
out=$project.$SLURM_CLUSTER_NAME.$SLURM_JOB_PARTITION.N$SLURM_JOB_NUM_NODES.n$SLURM_NTASKS.jid$SLURM_JOBID.out

#echo -e "\n Running QE pw.x OpenMPI parallel job:"
#  OpenMPI : #
#export OMPI_MCA_btl=^openib
#mpirun -v -np $SLURM_NTASKS --hostfile $MACHINEFILE  $QE/bin/pw.x -npool $SLURM_JOB_NUM_NODES  < $inp > $out

# IntelMPI :
echo -e "\n\n Running QE pw.x IntelMPI parallel job  on \c";date; echo
mpirun -f $MACHINEFILE -np $SLURM_NTASKS  $QE/bin/pw.x -npool $SLURM_JOB_NUM_NODES  < $inp > $out

echo -e "\n\n QE finished on : \c";date
exit
