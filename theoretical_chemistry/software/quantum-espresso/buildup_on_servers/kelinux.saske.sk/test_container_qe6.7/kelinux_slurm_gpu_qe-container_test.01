#!/bin/bash

#SBATCH --job-name=qe-test       # create a short name for your job
#SBATCH --nodes=1 --constraint=k20m     # node count
#SBATCH --ntasks-per-node=4      # number of tasks per node

#SBATCH --partition=short
#SBATCH --time=00:20:00          # total run time limit (HH:MM:SS)

## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

#SBATCH --mail-user=Miroslav.Ilias@umb.sk


echo Job user is $SLURM_JOB_USER and his job $SLURM_JOB_NAME has assigned ID $SLURM_JOBID
echo This job was submitted from the computer $SLURM_SUBMIT_HOST
echo and from the home directory:
echo $SLURM_SUBMIT_DIR
echo
echo It is running on the cluster compute node:
echo $SLURM_CLUSTER_NAME
echo and is employing $SLURM_JOB_NUM_NODES node/nodes:
echo $SLURM_JOB_NODELIST
echo
echo -e "Job partition is: $SLURM_JOB_PARTITION \n"
#echo The job requests $SLURM_CPUS_ON_NODE CPU per task.
echo -e "\n SLURM_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK"

echo -e "avaiable modules on the node:"
module avail

#module purge
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

echo -e "\n ... launching singularity run = first container test :"

singularity exec /lustre/home/ilias/containers/quantum_espresso_v6.7.sif /bin/ls -lt
singularity exec /lustre/home/ilias/containers/quantum_espresso_v6.7.sif /bin/df -h
singularity exec /lustre/home/ilias/containers/quantum_espresso_v6.7.sif /bin/date
singularity exec /lustre/home/ilias/containers/quantum_espresso_v6.7.sif pw.x

exit
