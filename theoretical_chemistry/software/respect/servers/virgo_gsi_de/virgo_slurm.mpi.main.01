#!/bin/bash
#SBATCH -J Fm2rsp2

## max. execution time
##SBATCH -t 0-1:30:00
##SBATCH -t 7-00:00:00
#SBATCH -t 0-08:00:00

##  partition (queue)
##SBATCH -p long
#SBATCH -p main

## Node count - take only one node for OpenMP-MKL
#SBATCH -N 1

## CPU count  - max. 40 per node
##SBATCH -n 24
##SBATCH -n 6

##SBATCH --exclusive

## total memory 
#SBATCH --mem=24G

## memory per CPU
##SBATCH --mem-per-cpu=3G

## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

## mail
#SBATCH --mail-user=M.Ilias@gsi.de 
#SBATCH --mail-type=ALL

 
echo Job user is $SLURM_JOB_USER and his job $SLURM_JOB_NAME has assigned ID $SLURM_JOBID
echo This job was submitted from the computer $SLURM_SUBMIT_HOST
echo and from the home directory:
echo $SLURM_SUBMIT_DIR
echo
echo It is running on the cluster compute node:
echo $SLURM_CLUSTER_NAME
echo and is employing $SLURM_JOB_NUM_NODES node/nodes:
echo $SLURM_JOB_NODELIST
echo The job requests $SLURM_CPUS_ON_NODE CPU per task.

module use /cvmfs/it.gsi.de/modulefiles/
echo "modules at disposal:"
module avail
echo
#module unload compiler/intel/17.0
module unload openmpi3/3.1.4
module unload gnu8/8.3.0
# this is wanted compiler ...
#module load compiler/intel/15.0
echo "loaded modules:"
module list

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"
echo -e "\nMy PATH=$PATH\n"

# CPU model, total numer of CPUs, number of allocated CPUs
echo -e "The node's CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo "This node has $NPROCS CPUs available."
#
echo "(i) This node has SLURM_CPUS_ON_NODE=$SLURM_CPUS_ON_NODE CPUs allocated for SLURM calculations. Enough ?"
echo "(ii) This node has SLURM_JOB_CPUS_PER_NODE=$SLURM_JOB_CPUS_PER_NODE CPUs allocated for SLURM calculations."

echo -e "\n The memory at the node (in GB)"
free -t -g
echo

## set internal OpenMP parallelization for MKL - Slurm CPU count
#export MKL_NUM_THREADS=$SLURM_CPUS_ON_NODE

# no OpenMP multithreading, use full OpenMPI 
export MKL_NUM_THREADS=1
echo -e  "\nInternal MKL parallelization upon SLURM CPU count, MKL_NUM_THREADS=$MKL_NUM_THREADS\n"
export OMP_NUM_THREADS=1
export MKL_DYNAMIC="FALSE"
export OMP_DYNAMIC="FALSE"

#export RSP=/lustre/nyx/ukt/milias/work/software/respect/ReSpect_3.4.2_beta_x86_64_OpenMPI-1.8.1_intel-14.0
#export RSP=/lustre/nyx/ukt/milias/work/software/respect/ReSpect_4.0.0_beta_x86_64_OpenMPI-2.0.0_intel-15.0
export RSP=/lustre/nyx/ukt/milias/work/software/respect/ReSpect_5.2.0_beta_x86_64_OMP_intel-17.1_static

# My own OpenMPI-Intel installation for ReSpect-OpenMPI
#export PATH=/lustre/ukt/milias/bin/openmpi-2.0.0-intel-15/bin:$PATH
#export LD_LIBRARY_PATH=/lustre/ukt/milias/bin/openmpi-2.0.0-intel-15/lib:$LD_LIBRARY_PATH

echo -e "\n Python -v :\c"; python -V
#echo -e "cmake ? which cmake = \c"; which cmake
#echo -e "ctest ? which ctest = \c"; which ctest
#echo -e "ctest --version \c"; ctest --version

#echo -e "which mpirun  ? \c"; which mpirun
#echo -e "mpirun --version ? \c"; mpirun --version
#echo -e "ifort -V ? \c"; ifort -V
#echo -e "icc -V ? \c"; icc -V

echo -e "\n ldd $RSP/ReSpect-mDKS.x ..."
ldd $RSP/ReSpect-mDKS.x

# for running jobs from your homedir, use ...
cd $SLURM_SUBMIT_DIR

echo -e "\n Current directory where this SLURM job is running `pwd`"
echo " It has the disk space of (df -h) :"
df -h .

export TMPDIR=/lustre/ukt/milias/scratch

NMPI=$SLURM_CPUS_ON_NODE
#NMPI=24
#NMPI=12
#NMPI=6

echo -e "\n running $RSP/respect --scratch=$TMPDIR --inp=Fm2.dkh2.scf.v2z --nt=$NMPI ..."
#$RSP/respect --scratch=$TMPDIR --inp=Fm2.dkh2.scf.v2z --np=$NMPI
$RSP/respect --scratch=$TMPDIR --inp=Fm2.dkh2.scf.v2z --nt=$NMPI

#echo -e "\n running $RSP/respect --scratch=$TMPDIR --restart=Fm2.dkh2.scf.v2z  --inp=Fm2.dc.pbe.vtz  --np=$NMPI ..."
#$RSP/respect --scratch=$TMPDIR --restart=Fm2.dkh2.scf.v2z  --inp=Fm2.dc.pbe.vtz  --np=$NMPI
#

#/bin/rm -r $TMPDIR
