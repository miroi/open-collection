#!/bin/bash
#SBATCH -J TiC

##  partition (queue)
##SBATCH -p devel
#SBATCH -p short

## max. execution time
#SBATCH -t 0-10:00:00

# number of nodes
#SBATCH -N 1

##SBATCH --exclusive

#SBATCH --mem=32GB
##SBATCH --mem=24GB

# do not restart in the case of nodefail!
#SBATCH --no-requeue
#SBATCH --no-kill

## CPU count  - max. 40 per node ?
#SBATCH -n 4

## memory NO !!!
##SBATCH --mem-per-cpu=28G

## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

## mail
#SBATCH --mail-user=Miroslav.Ilias@umb.sk
#SBATCH --mail-type=ALL

 
echo Job user is $SLURM_JOB_USER and his job $SLURM_JOB_NAME has assigned ID $SLURM_JOBID
echo This job was submitted from the computer $SLURM_SUBMIT_HOST
echo and from the home directory:
echo $SLURM_SUBMIT_DIR
echo
echo It is running on the cluster compute node:
echo $SLURM_CLUSTER_NAME
echo and is employing $SLURM_JOB_NUM_NODES node/nodes:
echo $SLURM_JOB_NODELIST
echo
echo -e "Job partition is $SLURM_JOB_PARTITION \n"
echo The job requests $SLURM_CPUS_ON_NODE CPU per task.

echo -e "Modules loading:"
module use /lustre/home/utils/easybuild_old/modules/all/
module unload all

module load imkl/2021.2.0-iimpi-2021a
module load FFTW/3.3.9-intel-2021a
#module load ELPA/2021.11.001-foss-2022a

module unload OpenMPI/4.1.4-GCC-11.3.0
module unload openmpi3/3.1.0
#
echo -e "\n\n All loaded modules:"
module list

echo -e "\n Check of compilers and libraries:"
which tcsh; tcsh --version
which mpiifort; mpiifort --version
which mpirun; mpirun --version
echo "Intel MKL library, MKLROOT=$MKLROOT"

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"

# CPU model, total numer of CPUs, number of allocated CPUs
echo -e "The node's CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo "This node has total $NPROCS CPUs available for EXCLUSIVE job."
#echo "This node has $SLURM_CPUS_ON_NODE CPUs allocated for SLURM calculations."

echo -e "\n The total memory at the node (in GB)"
free -t -g
echo -e "\n"

## set internal OpenMP parallelization for MKL - Slurm CPU count
#export MKL_NUM_THREADS=$SLURM_CPUS_ON_NODE
#export MKL_NUM_THREADS=24
#export MKL_NUM_THREADS=$NPROCS
#echo -e  "\nInternal MKL parallelization MKL_NUM_THREADS=$MKL_NUM_THREADS\n"
export OMP_NUM_THREADS=1
export MKL_DYNAMIC="FALSE"
export OMP_DYNAMIC="FALSE"

#
       case=TiC
       inp=$case.struct
#
export WIEN2k=/lustre/home/ilias/work/qch/software/wien2k/WIEN2k_23.2_intelmpi_mkl
export WIENROOT=$WIEN2K
SCR=Wien2k_23.2_job.$SLURM_JOB_PARTITION.N$SLURM_JOB_NUM_NODES.n$SLURM_NTASKS.jid$SLURM_JOBID
export SCRATCH=/lustre/home/ilias/scratch/$SCR/$case
echo -e "Creating scratch directory, SCRATCH=$SCRATCH"
mkdir -p $SCRATCH

cd $SCRATCH
echo -e "Im in SCRATCH :"; pwd

# first link Wien2k executables into the directory
 ln -sf $WIEN2k/dstart             $PWD/.
 ln -sf $WIEN2k/init_lapw          $PWD/.
 ln -sf $WIEN2k/instgen_lapw       $PWD/.
 ln -sf $WIEN2k/kgen               $PWD/.
 ln -sf $WIEN2k/lapw0              $PWD/.
 ln -sf $WIEN2k/lapw0_mpi          $PWD/.
 ln -sf $WIEN2k/lapw0_para         $PWD/.
 ln -sf $WIEN2k/lapw0_lapw         $PWD/.
 ln -sf $WIEN2k/lapw1              $PWD/.
 ln -sf $WIEN2k/lapw1cpara         $PWD/.
 ln -sf $WIEN2k/lapw1c             $PWD/.
 ln -sf $WIEN2k/lapw2              $PWD/.
 ln -sf $WIEN2k/lapw3              $PWD/.
 ln -sf $WIEN2k/lapw5              $PWD/.
 ln -sf $WIEN2k/lapw7              $PWD/.
 ln -sf $WIEN2k/lapwso             $PWD/.
 ln -sf $WIEN2k/lapwso_mpi         $PWD/.
 ln -sf $WIEN2k/lcore              $PWD/.
 ln -sf $WIEN2k/lorentz            $PWD/.
 ln -sf $WIEN2k/lstart             $PWD/.
 ln -sf $WIEN2k/mixer              $PWD/.
 ln -sf $WIEN2k/nn                 $PWD/.
 ln -sf $WIEN2k/run_lapw           $PWD/.
 ln -sf $WIEN2k/runsp_lapw         $PWD/.
 ln -sf $WIEN2k/setrmt_lapw        $PWD/.
 ln -sf $WIEN2k/sgroup             $PWD/.
 ln -sf $WIEN2k/symmetry           $PWD/.
 ln -sf $WIEN2k/testconv           $PWD/.
 ln -sf $WIEN2k/testconv_lapw      $PWD/.
 ln -sf $WIEN2k/x                  $PWD/.
 ln -sf $WIEN2k/x_lapw             $PWD/.

# link input file into scratch dir
 ln -sf $SLURM_SUBMIT_DIR/$inp     $PWD/.
 ls -lt $PWD/$inp


 instgen_lapw < $inp

 echo 2|x nn

 x sgroup

 x symmetry

 instgen_lapw -s -up

 # provide PBE and -6.0Ry
 echo -e 13 '\n'-6.0 | x lstart

 # provide k space
 echo 30 | x kgen

 #prepare files for dstart, based on Table 4.1 in https://euler.phys.cmu.edu/cluster/WIEN2k/4Files_Program.html
 cp $case.in0_st  $case.in0
 cp $case.in1_st  $case.in1
 cat $case.in2_ls  $case.in2_sy > $case.in2_st; cp $case.in2_st $case.in2a; cp $case.in2_st $case.in2

 echo -e "\n running x dstart :"
 x dstart

 # run own SCF, but prepare input files first
 cp $case.inc_st  $case.inc
 cp $case.inm_st  $case.inm
 cp $case.inq_st  $case.inq
 run_lapw -ec 0.0001 -NI

 echo -e "\n Finished; check for ':ENE' in outputs."

 echo -e "List of files in $SCRATCH :"; ls -lt $SCRATCH/*


exit
