Start Singularity container /cvmfs/vae.gsi.de/vae23/containers/vae23-user_container_20230617T0229.sif
INFO:    Environment variable SINGULARITYENV_PATH is set, but APPTAINERENV_PATH is preferred
INFO:    Environment variable SINGULARITYENV_LD_LIBRARY_PATH is set, but APPTAINERENV_LD_LIBRARY_PATH is preferred
WARNING: While bind mounting '/etc/slurm:/etc/slurm': destination is already in the mount point list
WARNING: While bind mounting '/var/run/munge:/var/run/munge': destination is already in the mount point list
WARNING: While bind mounting '/var/spool/slurm:/var/spool/slurm': destination is already in the mount point list
WARNING: While bind mounting '/var/lib/sss/pipes/nss:/var/lib/sss/pipes/nss': destination is already in the mount point list
WARNING: While bind mounting '/cvmfs:/cvmfs': destination is already in the mount point list
Job user is milias and his job LvO2@Qgp has assigned ID 3173983
This job was submitted from the computer lxbk1135
and from the home directory:
/lustre/ukt/milias/work/projects/open-collection/theoretical_chemistry/software/wien2k/runs/TsOO_on_quartz_G/TsOOQg

It is running on the cluster compute node:
virgo
and is employing 1 node/nodes:
lxbk1172

Job partition is main 

The job requests SLURM_NTASKS=32.

Sparc modules loading:

 All loaded modules, spack find --loaded:
-- linux-debian10-x86_64 / gcc@10.2.0 ---------------------------
amdfftw@3.0
amdscalapack@3.2
autoconf@2.69
autoconf-archive@2022.02.11
automake@1.16.5
bison@3.8.2
bzip2@1.0.8
ca-certificates-mozilla@2022-10-11
cmake@3.24.3
curl@7.85.0
diffutils@3.8
elpa@2021.11.001
expat@2.4.8
gawk@5.1.1
gdbm@1.23
gettext@0.21.1
glib@2.74.1
gmp@6.2.1
hwloc@2.8.0
json-c@0.16
krb5@1.19.3
libbsd@0.11.5
libedit@3.1-20210216
libevent@2.1.12
libffi@3.4.2
libgcrypt@1.10.1
libgpg-error@1.46
libiconv@1.16
libmd@1.0.4
libpciaccess@0.16
libsigsegv@2.13
libtool@2.4.7
libxml2@2.10.1
lz4@1.9.4
m4@1.4.19
meson@0.63.3
mpfr@4.1.0
munge@0.5.13
ncurses@6.1
ninja@1.11.1
numactl@2.0.14
openblas@0.3.21
openmpi@4.1.5
openssh@9.1p1
openssl@1.1.1s
pcre2@10.39
perl@5.16.3
pigz@2.7
pkgconf@1.8.0
pmix@3.2.2
py-pip@22.2.2
py-setuptools@59.4.0
py-wheel@0.37.1
python@3.10.8
rdma-core@22.4
readline@8.1.2
slurm@21-08-8-2
sqlite@3.39.4
tar@1.34
texinfo@6.5
ucx@1.9.0
util-linux-uuid@2.38.1
util-macros@1.19.3
xz@5.2.7
zlib@1.2.13
zstd@1.5.2

 Check of compilers and libraries:
/usr/bin/tcsh
tcsh 6.20.00 (Astron) 2016-11-24 (x86_64-unknown-linux) options wide,nls,dl,al,kan,sm,rh,nd,color,filec
/cvmfs/vae.gsi.de/vae23/spack-0.19/opt/linux-debian10-x86_64/gcc-10.2.0/openmpi-4.1.5-phbdvrf3few3givo575jlifx6dhnfgk7/bin/mpif90
GNU Fortran (Spack GCC) 10.2.0
Copyright (C) 2020 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

/cvmfs/vae.gsi.de/vae23/spack-0.19/opt/linux-debian10-x86_64/gcc-10.2.0/openmpi-4.1.5-phbdvrf3few3givo575jlifx6dhnfgk7/bin/mpirun
mpirun (Open MPI) 4.1.5

Report bugs to http://www.open-mpi.org/community/help/
OpenBLAS library:
-- linux-debian10-x86_64 / gcc@10.2.0 ---------------------------
openblas@0.3.21  /cvmfs/vae.gsi.de/vae23/spack-0.19/opt/linux-debian10-x86_64/gcc-10.2.0/openblas-0.3.21-q7nhojttkz52xuf4zkxk7vvgllqnxh34
cmake
libopenblas.a
libopenblas-r0.3.21.a
libopenblas-r0.3.21.so
libopenblas.so
libopenblas.so.0
pkgconfig

Running on host lxbk1172
Time is Sat 24 Jun 2023 06:25:45 PM CEST 

The node's CPU model name	: AMD EPYC 7713 64-Core Processor
This node has total 256 CPUs available for an EXCLUSIVE job.
SLURM_CPUS_ON_NODE=64
SLURM_NTASKS_PER_SOCKET=
SLURM_NTASKS_PER_NODE=
SLURM_NTASKS_PER_CORE=
SLURM_PARTITION=
SLURM_MEM_PER_NODE=131072

 The total memory at the node (in GB)
              total        used        free      shared  buff/cache   available
Mem:            503          45         386           1          71         452
Swap:             9           0           9
Total:          513          45         395


WIENROOT=/lustre/ukt/milias/work/software/wien2k/Wien2k_23.2_gnu_openmpi_openblas

Creating scratch directory, SCRATCH=/lustre/ukt/milias/scratch/Wien2k_23.2_job.main.N1.n32.jid3173983/TsOOQg
I am in $SCRATCH directrory :
/lustre/ukt/milias/scratch/Wien2k_23.2_job.main.N1.n32.jid3173983/TsOOQg
cp: cannot stat '/lustre/ukt/milias/work/projects/open-collection/theoretical_chemistry/software/wien2k/runs/TsOO_on_quartz_G/TsOOQg/TsOOQg.vsp_sp_SAVED': No such file or directory

 number of hosts in .machines, SLURM_NTASKS=32, counter=33

 content of file /lustre/ukt/milias/scratch/Wien2k_23.2_job.main.N1.n32.jid3173983/TsOOQg/.machines:
# nodes for parallel job 
omp_global:4 
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost
1:localhost

 running 'x -p dstart' :
starting parallel dstart at Sat 24 Jun 2023 06:25:46 PM CEST
-------- .machine0 : processors
running dstart in single mode
STOP DSTART ENDS
209.349u 2.041s 1:30.77 232.8%	0+0k 159976+1513008io 438pf+0w

 running 'run_lapw -p -ec 0.00001 -i 1000 -NI'
STOP  LAPW0 END
Killed
[4]    Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >> .time1_$loop
Killed
[5]    Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >> .time1_$loop
Killed
[26]   Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >> .time1_$loop
Killed
[28]   Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >> .time1_$loop
Killed
[3]    Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >> .time1_$loop
Killed
[2]    Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >> .time1_$loop
Killed
[8]    Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >> .time1_$loop
Killed
[6]    Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >> .time1_$loop
Killed
[14]   Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >> .time1_$loop
Killed
[11]   Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >> .time1_$loop
Killed
[1]    Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >> .time1_$loop
Killed
Killed
Killed
Killed
Killed
Killed
Killed
Killed
Killed
Killed
Killed
Killed
Killed
Killed
Killed
Killed
Killed
Killed
Killed
Killed
slurmstepd: error: *** JOB 3173983 ON lxbk1172 CANCELLED AT 2023-06-25T02:25:14 DUE TO TIME LIMIT ***
slurmstepd: error: Detected 31 oom-kill event(s) in StepId=3173983.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.
