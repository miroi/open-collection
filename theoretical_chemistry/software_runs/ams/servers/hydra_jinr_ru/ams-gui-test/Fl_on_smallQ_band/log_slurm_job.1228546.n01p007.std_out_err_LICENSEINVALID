slurmstepd: error: xcpuinfo_hwloc_topo_load: failed (load will be required after read failures).
Job user is milias and his job FlonQBAND has assigned ID 1228546
This job was submitted from the computer space13.hydra.local
and from the home directory:
/zfs/store5.hydra.local/user/m/milias/work/projects/open-collection/theoretical_chemistry/software_runs/ams/servers/hydra_jinr_ru/ams-gui-test/Fl_on_smallQ_band

It is running on the cluster compute node:
gvr
and is employing 1 node/nodes:
n01p007

Job partition is knl 

The job requests 4 CPU per task.
modules at disposal:

-------------- /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/modulefiles --------------
ABINIT/v8.10.3_intel2018              gcc/v9.1.0-1
AmberTools/v20                        GEANT4/v4.10.07.p01_gcc910
AMS/v2021.102                         Ginac/v1.7.3-1
BASE/1.0                              GROMACS/v2019.3
CLN/v1.3.4-1                          GROMACS/v5.1.3_gcc485_cuda80
CMake/v3.16.5                         GSL/v1.16-1
CMake/v3.19.1                         GSL/v2.6
COMSOL/v5.6                           GVR/v1.0-1
cuda/v10.0-1                          intel/v2018.1.163-9
cuda/v10.1-1                          intel/v2019.3.199
cuda/v8.0-1                           intel/v2021.1
cuda/v9.2                             intel-qs/v20-07-14
DIRAC/v19.0_intel2018                 intel-qs/v21-01-14
ELPA/v2020.05.001_intel2018_python365 java/v8u181
EMACS/v25.3                           java/v8u91-1
EOS/v1.0                              LAMMPS/v12.12.18
FairRoot/oct17p1                      LAPACK/v3.9.0
FairRoot/v16.06_gcc485                libpqxx/v7.0_gcc910
FairRoot/v18.0.4                      Maple/v2020.2
FairRoot/v18.2.0_gcc485               Mathematica/v11.2-1
FairRoot/v18.2.1_gcc485               MATLAB/R2020b
FairRoot/v18.4.2_gcc1120              ndm-lite/v1.0
FairSoft/apr21patches_gcc1120         opencv/v4.1.0
FairSoft/june19p1_gcc485              openmpi/v1.8.8-1
FairSoft/june19p2_gcc485              openmpi/v2.1.2-2
FairSoft/may16p1_gcc485               openmpi/v3.1.2
FairSoft/may18p1                      openmpi/v3.1.3
FairSoft/oct17p1                      openmpi/v3.1.3_psm2
fftw/v3.3.7-2                         PandaRoot/dec17p2b
fftw/v3.3.7-5                         PandaRoot/may19
FLAIR/v2.3.0                          PostgreSQL/v12.1_gcc910
FLUKA/v2011.2x-8                      protobuf/v3.11.3
FLUKA/v2020.0.3                       Python/v2.7.10-3
gcc/v10.2.0                           Python/v3.6.5
gcc/v11.2.0                           quantum-espresso/v6.4.1
gcc/v4.9.3-1                          reduce-algebra/svn-4830
gcc/v5.3.0-1                          ROOT/v6-18-00
gcc/v6.2.0-2                          SMASH_gcc485/v1.8
gcc/v7.2.0-1                          TRNG/v4.24_gcc102
gcc/v8.2.0-1                          Valgrind/v3.16.1_gcc485
gcc/v8.3.0                            zlib/v1.2.11-1



 loaded modules:
Currently Loaded Modulefiles:
  1) BASE/1.0           2) openmpi/v2.1.2-2
Disk space: df -h /tmp 
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda3       134G  6.1G  121G   5% /
ADF environmental variables activated on
ADF AMSHOME=/zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103
ADF SCM_TMPDIR=/tmp
ADF SCMLICENSE=/zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/license.txt
SCM_OPENGL_SOFTWARE=1

Running on host n01p007.gvr.local
Time is Wed Sep  8 20:45:29 MSK 2021 

The node's CPU model name	: Intel(R) Xeon Phi(TM) CPU 7290 @ 1.50GHz
This node has total 288 CPUs available for anEXCLUSIVE job.

 The total memory at the node (in GB)
              total        used        free      shared  buff/cache   available
Mem:             94          11          28           0          54          81
Swap:             3           0           3
Total:           98          12          31



ldd /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/ams.exe:
	linux-vdso.so.1 =>  (0x00007fffe1ab9000)
	libprimme.so => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/lib/libprimme.so (0x00002abf502e3000)
	libplumed.so.2.5.0 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/lib/libplumed.so.2.5.0 (0x00002abf50501000)
	libmkl_intel_lp64.so.1 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/libmath/libmkl_intel_lp64.so.1 (0x00002abf5070c000)
	libmkl_cdft_core.so.1 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/libmath/libmkl_cdft_core.so.1 (0x00002abf51447000)
	libmkl_scalapack_lp64.so.1 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/libmath/libmkl_scalapack_lp64.so.1 (0x00002abf5166f000)
	libmkl_blacs_openmpi_lp64.so.1 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/libmath/libmkl_blacs_openmpi_lp64.so.1 (0x00002abf51f9a000)
	libmkl_sequential.so.1 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/libmath/libmkl_sequential.so.1 (0x00002abf521e1000)
	libmkl_core.so.1 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/libmath/libmkl_core.so.1 (0x00002abf53dec000)
	libmpi_mpifh.so.20 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v2.1.2-2/lib/libmpi_mpifh.so.20 (0x00002abf5bdb2000)
	libmpi.so.20 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v2.1.2-2/lib/libmpi.so.20 (0x00002abf5c004000)
	librt.so.1 => /lib64/librt.so.1 (0x00002abf5c337000)
	libdl.so.2 => /lib64/libdl.so.2 (0x00002abf5c53f000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x00002abf5c743000)
	libstdc++.so.6 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/lib/libstdc++.so.6 (0x00002abf5c95f000)
	libresolv.so.2 => /lib64/libresolv.so.2 (0x00002abf5cd3e000)
	libiomp5.so => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/lib/libiomp5.so (0x00002abf5cf57000)
	libc.so.6 => /lib64/libc.so.6 (0x00002abf5d2f9000)
	libgcc_s.so.1 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/lib/libgcc_s.so.1 (0x00002abf5d6c7000)
	libplumedKernel.so.2.5.0 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/lib/libplumedKernel.so.2.5.0 (0x00002abf5d8df000)
	libm.so.6 => /lib64/libm.so.6 (0x00002abf5f0fc000)
	libopen-rte.so.20 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v2.1.2-2/lib/libopen-rte.so.20 (0x00002abf5f3fe000)
	libopen-pal.so.20 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v2.1.2-2/lib/libopen-pal.so.20 (0x00002abf5f6c1000)
	libutil.so.1 => /lib64/libutil.so.1 (0x00002abf5f9dc000)
	/lib64/ld-linux-x86-64.so.2 (0x00002abf500bf000)
	libz.so.1 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/lib/../zlib/lib/libz.so.1 (0x00002abf5fbdf000)

My PATH=/zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v2.1.2-2/bin:/zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/zfs/hybrilit.jinr.ru/user/m/milias/.local/bin:/zfs/hybrilit.jinr.ru/user/m/milias/bin:.

Python -v :Python 2.7.5

 mpirun ? /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v2.1.2-2/bin/mpirun
mpirun (Open MPI) 2.1.2rc4

Report bugs to http://www.open-mpi.org/community/help/


 Running /zfs/store5.hydra.local/user/m/milias/work/projects/open-collection/theoretical_chemistry/software_runs/ams/servers/hydra_jinr_ru/ams-gui-test/Fl_on_smallQ_band/ runfile.run  : 


 Parallel Execution: Process Information
 ==============================================================================
 Rank   Node Name                              NodeID   MyNodeRank  NodeMaster
    0   n01p007.gvr.local                         0          0          0
    1   n01p007.gvr.local                         0          1         -1
    2   n01p007.gvr.local                         0          2         -1
    3   n01p007.gvr.local                         0          3         -1
 ==============================================================================


May use up to 46828MB of RAM as shared memory on node 0
 
 ---------------
 LICENSE INVALID
 ---------------
 
 Your license does not include module AMS version 2021.103 on this machine.
 
 Module AMS
 Version 2021.103
 Machine: Linuxn01p007.gvr.localA4:BF:01:10:4D:36
 
 License file: 
 /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103
 /license.txt
 Account: 
 Dr. Jan Busa / Joint Institute for Nuclear Research (JINR) / Dubna / RUSSIA 202
 1-08-31 09:14:26
 No License termination date found
 
 No modules activated on this node.
 
 
 =====================================================
 You need to install a valid license file.
 Mail SCM (license@scm.com) the following information:
 
 Module AMS version 2021.103
 
 SCM User ID: u24355
 release: 2021.103
 :gvr.local:
 :Linuxn01p007.gvr.localA4:BF:01:10:4D:36:
 :ncores       2:
 :CPU Model Intel(R) Xeon Phi(TM) CPU 7290 @ 1.50GHz:
 :DMY  8- 9-2021:
 :SRCID  5762139:
 =====================================================
 
 ---------------
 LICENSE INVALID
 ---------------
 
 Your license does not include module AMS version 2021.103 on this machine.
 
 Module AMS
 Version 2021.103
 Machine: Linuxn01p007.gvr.localA4:BF:01:10:4D:36
 
 License file: 
 /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103
 /license.txt
 Account: 
 Dr. Jan Busa / Joint Institute for Nuclear Research (JINR) / Dubna / RUSSIA 202
 1-08-31 09:14:26
 No License termination date found
 
 No modules activated on this node.
 
 
 =====================================================
 You need to install a valid license file.
 Mail SCM (license@scm.com) the following information:
 
 Module AMS version 2021.103
 
 SCM User ID: u24355
 release: 2021.103
 :gvr.local:
 :Linuxn01p007.gvr.localA4:BF:01:10:4D:36:
 :ncores       2:
 :CPU Model Intel(R) Xeon Phi(TM) CPU 7290 @ 1.50GHz:
 :DMY  8- 9-2021:
 :SRCID  5762139:
 =====================================================
LICENSE INVALID
LICENSE INVALID
 
 ---------------
 LICENSE INVALID
 ---------------
 
 Your license does not include module AMS version 2021.103 on this machine.
 
 Module AMS
 Version 2021.103
 Machine: Linuxn01p007.gvr.localA4:BF:01:10:4D:36
 
 License file: 
 /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103
 /license.txt
 Account: 
 Dr. Jan Busa / Joint Institute for Nuclear Research (JINR) / Dubna / RUSSIA 202
 1-08-31 09:14:26
 No License termination date found
 
 No modules activated on this node.
 
 
 =====================================================
 You need to install a valid license file.
 Mail SCM (license@scm.com) the following information:
 
 Module AMS version 2021.103
 
 SCM User ID: u24355
 release: 2021.103
 :gvr.local:
 :Linuxn01p007.gvr.localA4:BF:01:10:4D:36:
 :ncores       2:
 :CPU Model Intel(R) Xeon Phi(TM) CPU 7290 @ 1.50GHz:
 :DMY  8- 9-2021:
 :SRCID  5762139:
 =====================================================
LICENSE INVALID
--------------------------------------------------------------------------
mpirun has exited due to process rank 3 with PID 208772 on
node n01p007 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpirun (as reported here).

You can avoid this message by specifying -quiet on the mpirun command line.
--------------------------------------------------------------------------
