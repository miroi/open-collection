Job user is milias and his job FlonQBAND has assigned ID 1250687
This job was submitted from the computer space15.hydra.local
and from the home directory:
/zfs/store5.hydra.local/user/m/milias/work/projects/open-collection/theoretical_chemistry/software_runs/ams/servers/hydra_jinr_ru/ams-gui-test/Fl_on_smallQ_band

It is running on the cluster compute node:
gvr
and is employing 1 node/nodes:
n01p007

Job partition is knl 

The job requests 4 CPU per task.
modules at disposal:

-------------- /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/modulefiles --------------
ABINIT/v8.10.3_intel2018              gcc/v9.1.0-1
AmberTools/v20                        GEANT4/v4.10.07.p01_gcc910
AMS/v2021.102_intel                   Ginac/v1.7.3-1
AMS/v2021.103_openmpi                 GROMACS/v2019.3
BASE/1.0                              GROMACS/v5.1.3_gcc485_cuda80
CLN/v1.3.4-1                          GSL/v1.16-1
CMake/v3.16.5                         GSL/v2.6
CMake/v3.19.1                         GVR/v1.0-1
COMSOL/v5.6                           intel/oneapi
cuda/v10.0-1                          intel/v2018.1.163-9
cuda/v10.1-1                          intel/v2019.3.199
cuda/v8.0-1                           intel/v2021.1
cuda/v9.2                             intel-qs/v20-07-14
DIRAC/v19.0_intel2018                 intel-qs/v21-01-14
ELPA/v2020.05.001_intel2018_python365 java/v8u181
EMACS/v25.3                           java/v8u91-1
EOS/v1.0                              LAMMPS/v12.12.18
FairRoot/oct17p1                      LAPACK/v3.9.0
FairRoot/v16.06_gcc485                libpqxx/v7.0_gcc910
FairRoot/v18.0.4                      Maple/v2020.2
FairRoot/v18.2.0_gcc485               Mathematica/v11.2-1
FairRoot/v18.2.1_gcc485               MATLAB/R2020b
FairRoot/v18.4.2_gcc1120              ndm-lite/v1.0
FairSoft/apr21patches_gcc1120         opencv/v4.1.0
FairSoft/june19p1_gcc485              openmpi/v1.8.8-1
FairSoft/june19p2_gcc485              openmpi/v2.1.2-2
FairSoft/may16p1_gcc485               openmpi/v3.1.2
FairSoft/may18p1                      openmpi/v3.1.3
FairSoft/oct17p1                      openmpi/v3.1.3_psm2
fftw/v3.3.7-2                         PandaRoot/dec17p2b
fftw/v3.3.7-5                         PandaRoot/may19
FLAIR/v2.3.0                          PostgreSQL/v12.1_gcc910
FLUKA/v2011.2x-8                      protobuf/v3.11.3
FLUKA/v2020.0.3                       Python/v2.7.10-3
gcc/v10.2.0                           Python/v3.6.5
gcc/v11.2.0                           quantum-espresso/v6.4.1
gcc/v4.9.3-1                          reduce-algebra/svn-4830
gcc/v5.3.0-1                          ROOT/v6-18-00
gcc/v6.2.0-2                          SMASH_gcc485/v1.8
gcc/v7.2.0-1                          TRNG/v4.24_gcc102
gcc/v8.2.0-1                          Valgrind/v3.16.1_gcc485
gcc/v8.3.0                            zlib/v1.2.11-1



 loaded modules:
Currently Loaded Modulefiles:
  1) BASE/1.0           2) openmpi/v2.1.2-2
Disk space: df -h /tmp 
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda3       134G  6.8G  120G   6% /
ADF environmental variables activated on
ADF AMSHOME=/zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103
ADF SCM_TMPDIR=/tmp
ADF SCMLICENSE=/zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/license.txt
SCM_OPENGL_SOFTWARE=1

Running on host n01p007.gvr.local
Time is Wed Sep 22 17:45:51 MSK 2021 

The node's CPU model name	: Intel(R) Xeon Phi(TM) CPU 7290 @ 1.50GHz
This node has total 288 CPUs available for anEXCLUSIVE job.

 The total memory at the node (in GB)
              total        used        free      shared  buff/cache   available
Mem:             94          41           0           1          52          51
Swap:             3           0           3
Total:           98          41           3



ldd /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/ams.exe:
	linux-vdso.so.1 =>  (0x00007ffff4912000)
	libprimme.so => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/lib/libprimme.so (0x00002b7473a28000)
	libplumed.so.2.5.0 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/lib/libplumed.so.2.5.0 (0x00002b7473c46000)
	libmkl_intel_lp64.so.1 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/libmath/libmkl_intel_lp64.so.1 (0x00002b7473e51000)
	libmkl_cdft_core.so.1 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/libmath/libmkl_cdft_core.so.1 (0x00002b7474b8c000)
	libmkl_scalapack_lp64.so.1 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/libmath/libmkl_scalapack_lp64.so.1 (0x00002b7474db4000)
	libmkl_blacs_openmpi_lp64.so.1 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/libmath/libmkl_blacs_openmpi_lp64.so.1 (0x00002b74756df000)
	libmkl_sequential.so.1 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/libmath/libmkl_sequential.so.1 (0x00002b7475926000)
	libmkl_core.so.1 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/libmath/libmkl_core.so.1 (0x00002b7477531000)
	libmpi_mpifh.so.20 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v2.1.2-2/lib/libmpi_mpifh.so.20 (0x00002b747f4f7000)
	libmpi.so.20 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v2.1.2-2/lib/libmpi.so.20 (0x00002b747f749000)
	librt.so.1 => /lib64/librt.so.1 (0x00002b747fa7c000)
	libdl.so.2 => /lib64/libdl.so.2 (0x00002b747fc84000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x00002b747fe88000)
	libstdc++.so.6 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/lib/libstdc++.so.6 (0x00002b74800a4000)
	libresolv.so.2 => /lib64/libresolv.so.2 (0x00002b7480483000)
	libiomp5.so => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/lib/libiomp5.so (0x00002b748069c000)
	libc.so.6 => /lib64/libc.so.6 (0x00002b7480a3e000)
	libgcc_s.so.1 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/lib/libgcc_s.so.1 (0x00002b7480e0c000)
	libplumedKernel.so.2.5.0 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/lib/libplumedKernel.so.2.5.0 (0x00002b7481024000)
	libm.so.6 => /lib64/libm.so.6 (0x00002b7482841000)
	libopen-rte.so.20 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v2.1.2-2/lib/libopen-rte.so.20 (0x00002b7482b43000)
	libopen-pal.so.20 => /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v2.1.2-2/lib/libopen-pal.so.20 (0x00002b7482e06000)
	libutil.so.1 => /lib64/libutil.so.1 (0x00002b7483121000)
	/lib64/ld-linux-x86-64.so.2 (0x00002b7473804000)
	libz.so.1 => /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin/lib/../zlib/lib/libz.so.1 (0x00002b7483324000)

My PATH=/zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103/bin:/cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v2.1.2-2/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:.:/zfs/hybrilit.jinr.ru/user/m/milias/.local/bin:/zfs/hybrilit.jinr.ru/user/m/milias/bin

Python -v :Python 2.7.5

 mpirun ? /cvmfs/hybrilit.jinr.ru/sw/slc7_x86-64/openmpi/v2.1.2-2/bin/mpirun
mpirun (Open MPI) 2.1.2rc4

Report bugs to http://www.open-mpi.org/community/help/


 Running /zfs/store5.hydra.local/user/m/milias/work/projects/open-collection/theoretical_chemistry/software_runs/ams/servers/hydra_jinr_ru/ams-gui-test/Fl_on_smallQ_band/ runfile.run  : 


 Parallel Execution: Process Information
 ==============================================================================
 Rank   Node Name                              NodeID   MyNodeRank  NodeMaster
    0   n01p007.gvr.local                         0          0          0
    1   n01p007.gvr.local                         0          1         -1
    2   n01p007.gvr.local                         0          2         -1
    3   n01p007.gvr.local                         0          3         -1
 ==============================================================================


May use up to 46828MB of RAM as shared memory on node 0

 *******************************************************************************
 *                                                                             *
 *  --------------------------------                                           *
 *   Amsterdam Modeling Suite (AMS)              2021.103                      *
 *  --------------------------------                                           *
 *                                               r96797 2021-08-20             *
 *                                                                             *
 *                                                                             *
 *                              =================                              *
 *                              |               |                              *
 *                              |     A M S     |                              *
 *                              |               |                              *
 *                              =================                              *
 *                                                                             *
 *                                                                             *
 *   Online information and documentation:  https://www.scm.com/support/       *
 *   E-mail:  support@scm.com   info@scm.com                                   *
 *                                                                             *
 *   Scientific publications using AMS results must be properly referenced     *
 *   See the User Manuals (or the web site) for recommended citations          *
 *   The terms and conditions of the End User License Agreement apply to       *
 *   the use of AMS, https://www.scm.com/license-terms/                        *
 *                                                                             *
 ***********************  x86_64_linux_intel / openmpi  ************************
 
 Licensed to: Dr. Jan Busa / Joint Institute for Nuclear Research (JINR) / Dubna / RUSSIA 2021-09-22 15:15:47
 SCM User ID: u24355
  
 AMS 2021.103  RunTime: Sep22-2021 17:47:48  ShM Nodes: 1  Procs: 4

 AMS jobname: ams
 AMS jobid  : 1049402724

 Start   directory: /zfs/store5.hydra.local/user/m/milias/work/projects/open-collection/theoretical_chemistry/software_runs/ams/servers/hydra_jinr_ru/ams-gui-test/Fl_on_smallQ_band/
 Results directory: /zfs/store5.hydra.local/user/m/milias/work/projects/open-collection/theoretical_chemistry/software_runs/ams/servers/hydra_jinr_ru/ams-gui-test/Fl_on_smallQ_band/ams.results/
 Scratch directory: /tmp/amstmp_ams_kid0.1049402724/


 Communication costs MPI_COMM_WORLD: 141601.682 usec per message, 150.5898 usec per 8-byte item
 Communication costs for intra-node: 156004.802 usec per message, 145.5931 usec per 8-byte item

 RNG seed: -397286555 -31552776 1097759848 157836292 1557566886 -1139669598 -673060519 -1152164614
 
 ---------------
 LICENSE INVALID
 ---------------
 
 Your license does not include module BAND version 2021.103 on this machine.
 
 Module BAND
 Version 2021.103
 Machine: Linuxn01p007.gvr.localA4:BF:01:10:4D:36
 
 License file: 
 /zfs/hybrilit.jinr.ru/user/m/milias/work/software/ams/linux.openmpi/ams2021.103
 /license.txt
 Account: 
 Dr. Jan Busa / Joint Institute for Nuclear Research (JINR) / Dubna / RUSSIA 202
 1-09-22 15:15:47
 No License termination date found
 
 Modules activated on this node:
 
         MLPOT version 2021.199
       ppMLPOT version 1024.000
    FORCEFIELD version 2021.199
    ppFORCEFIE version 1024.000
        ADFGUI version 2021.199
       BANDGUI version 2021.199
       DFTBGUI version 2021.199
     REAXFFGUI version 2021.199
      MOPACGUI version 2021.199
         QEGUI version 2021.199
           GUI version 2021.199
           AMS version 2021.199
         Utils version 2021.199
 
 Number of procs you are allowed to use for:
 
 BAND   :    1024 procs
 AMS    :    1024 procs
 
 =====================================================
 You need to install a valid license file.
 Mail SCM (license@scm.com) the following information:
 
 Module BAND version 2021.103
 
 SCM User ID: u24355
 release: 2021.103
 :gvr.local:
 :Linuxn01p007.gvr.localA4:BF:01:10:4D:36:
 :ncores       2:
 :CPU Model Intel(R) Xeon Phi(TM) CPU 7290 @ 1.50GHz:
 :DMY 22- 9-2021:
 :SRCID  8942721:
 =====================================================
LICENSE INVALID
--------------------------------------------------------------------------
mpirun has exited due to process rank 0 with PID 78131 on
node n01p007 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpirun (as reported here).

You can avoid this message by specifying -quiet on the mpirun command line.
--------------------------------------------------------------------------
