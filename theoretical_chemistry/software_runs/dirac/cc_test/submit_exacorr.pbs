#!/bin/bash

#BSUB -nnodes 512
#BSUB -W 02:00
#BSUB -P CHP109
#BSUB -J dirac
#BSUB -o dirac.out.%J
#BSUB -e dirac.err.%J
###BSUB -alloc_flags "gpumps smt1"

# Load modules:
module load gcc/8.1.1
module load essl
module load cuda
module load git
module load cmake
module load python
module load netlib-lapack

# Set full path to dirac.x and exacorr.x:
export DIRAC_PATH=/gpfs/alpine/world-shared/chp109/DIRAC_INSTALL
# Set full paths to basis sets accessible on compute nodes:
export BASDIR_PATH=/gpfs/alpine/world-shared/chp109/DIRAC_INSTALL/basis:/gpfs/alpine/world-shared/chp109/DIRAC_INSTALL/basis_dalton:/gpfs/alpine/world-shared/chp109/DIRAC_INSTALL/basis_ecp

# Set the path to the scratch directory:
export DIRAC_TMPDIR=/gpfs/alpine/scratch/div/chp109/scr
#export DIRAC_TMPDIR=$LS_SUBCWD

#ExaTENSOR specific:
export QF_NUM_PROCS=1024          #total number of MPI processes
export QF_PROCS_PER_NODE=2        #number of MPI processes per logical node (logical nodes are created by node resource isolation)
export QF_CORES_PER_PROCESS=21    #number of physical CPU cores per MPI process (no less than 1)
export QF_MEM_PER_PROCESS=160000  #host RAM memory limit per MPI process in MB
export QF_NVMEM_PER_PROCESS=0     #non-volatile memory limit per MPI process in MB
export QF_HOST_BUFFER_SIZE=200000 #host buffer size per MPI process in MB (must be less than QF_MEM_PER_PROCESS)
export QF_GPUS_PER_PROCESS=3      #number of discrete NVIDIA GPU's per MPI process (optional)
export QF_MICS_PER_PROCESS=0      #number of discrete Intel Xeon Phi's per MPI process (optional)
export QF_AMDS_PER_PROCESS=0      #number of discrete AMD GPU's per MPI process (optional)
export QF_NUM_THREADS=21          #initial number of CPU threads per MPI process (irrelevant)

#OpenMP generic:
export OMP_NUM_THREADS=$QF_NUM_THREADS #initial number of OpenMP threads per MPI process
export OMP_DYNAMIC=false               #no OpenMP dynamic threading
export OMP_NESTED=true                 #OpenMP nested parallelism is mandatory
export OMP_MAX_ACTIVE_LEVELS=3         #max number of OpenMP nesting levels (at least 3)
export OMP_THREAD_LIMIT=256            #max total number of OpenMP threads per process
export OMP_WAIT_POLICY=PASSIVE         #idle thread behavior

#OpenMP thread binding:
export OMP_PLACES_DEFAULT=cores                                      #default thread binding to CPU logical cores
#export OMP_PLACES_POWER9="{0},{4},{8},{12},{28:56},{16},{20},{24}"  #Summit 21-core SMT4 Power9 socket thread binding (even logical cores do computing)
#export OMP_PLACES=$OMP_PLACES_POWER9
export OMP_PROC_BIND="close,spread,spread" #nest1: Functional threads (DSVU)
                                           #nest2: TAVP-WRK:Dispatcher spawns coarse-grain Executors
                                           #nest3: TAVP-WRK:Dispatcher:Executor spawns execution threads in CP-TAL kernels
#Summit specific:
export PAMI_IBV_ADAPTER_AFFINITY=1
export PAMI_IBV_DEVICE_NAME="mlx5_0:1,mlx5_3:1"
export PAMI_IBV_DEVICE_NAME_1="mlx5_3:1,mlx5_0:1"
export PAMI_IBV_ENABLE_OOO_AR=1                   #adaptive routing is default
export PAMI_ENABLE_STRIPING=1                     #increases network bandwidth, also increases latency
export PAMI_IBV_DISABLE_ODP=0                     #ODP (requires CAPI for performance)
#export PAMI_IBV_ENABLE_TAG_MATCHING=1            #hardware tag matching
export PAMI_IBV_ENABLE_DCT=1                      #reduces MPI_Init() time at large scale
#unset PAMI_IBV_ENABLE_DCT
#export PAMI_IBV_QP_SERVICE_LEVEL=8
#export PAMI_PMIX_DATACACHE=1
#export PAMI_IBV_DEBUG_CQE=1                      #CQE error debugging
#export PAMI_IBV_DEBUG_QP_TIMEOUT=22
#export PAMI_IBV_DEBUG_RNR_RETRY=9
#export OMPI_LD_PRELOAD_POSTPEND=$OLCF_SPECTRUM_MPI_ROOT/lib/libmpitrace.so

#cp $DIRAC_PATH/dirac.x ./
#cp $DIRAC_PATH/pam ./

#export DIRWRK=1424000000 
#export DIRMAX=2048000000 
#export DIRNOD=1424000000
#export BASDIR=$BASDIR_PATH  
#export DIRPAR=1 
#export GLBSCR=1  

export DIRAC_MPI_COMMAND="jsrun --smpiargs='-async' --smpiargs='-mca common_pami_use_odp 1' -D PAMI_IBV_DISABLE_ODP=0 -n $QF_NUM_PROCS -r $QF_PROCS_PER_NODE -a 1 -c $QF_CORES_PER_PROCESS -g $QF_GPUS_PER_PROCESS -bnone"
#./pam --nw=1424 --mw=1424 --aw=2048 --incmo --outcmo --keep_scratch --mol=UF6_2.xyz --inp=exacc.inp
jsrun --smpiargs='-async' --smpiargs='-mca common_pami_use_odp 1' -D PAMI_IBV_DISABLE_ODP=0 -n $QF_NUM_PROCS -r $QF_PROCS_PER_NODE -a 1 -c $QF_CORES_PER_PROCESS -g $QF_GPUS_PER_PROCESS -bnone ./exec_exa.sh
