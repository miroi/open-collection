#!/bin/bash
#SBATCH -J DIRtests

##  partition (queue)
##SBATCH -p cpu
##SBATCH -p test

## max. execution time
#SBATCH -t 1-00:00:00

# number of nodes
#SBATCH -N 1

##SBATCH --exclusive

##SBATCH --mem=32GB
#SBATCH --mem=24GB

# do not restart in the case of nodefail!
#SBATCH --no-requeue
#SBATCH --no-kill

## CPU count  - max. 40 per node ?
#SBATCH -n 8

## memory NO !!!
##SBATCH --mem-per-cpu=28G

## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

## mail
#SBATCH --mail-user=Miroslav.Ilias@umb.sk
#SBATCH --mail-type=ALL

 
echo Job user is $SLURM_JOB_USER and his job $SLURM_JOB_NAME has assigned ID $SLURM_JOBID
echo This job was submitted from the computer $SLURM_SUBMIT_HOST
echo and from the home directory:
echo $SLURM_SUBMIT_DIR
echo
echo It is running on the cluster compute node:
echo $SLURM_CLUSTER_NAME
echo and is employing $SLURM_JOB_NUM_NODES node/nodes:
echo $SLURM_JOB_NODELIST
echo
echo -e "Job partition is $SLURM_JOB_PARTITION \n"
echo The job requests $SLURM_CPUS_ON_NODE CPU per task.

echo "modules at disposal:"
module avail
echo

#
module load intel
module load cmake
echo -e "\n\n loaded modules:"
module list

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"

# CPU model, total numer of CPUs, number of allocated CPUs
echo -e "The node's CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo "This node has total $NPROCS CPUs available for EXCLUSIVE job."
#echo "This node has $SLURM_CPUS_ON_NODE CPUs allocated for SLURM calculations."

echo -e "\n The total memory at the node (in GB)"
free -t -g
echo -e "\n"

## set internal OpenMP parallelization for MKL - Slurm CPU count
#export MKL_NUM_THREADS=$SLURM_CPUS_ON_NODE
#export MKL_NUM_THREADS=24
#export MKL_NUM_THREADS=$NPROCS
#echo -e  "\nInternal MKL parallelization MKL_NUM_THREADS=$MKL_NUM_THREADS\n"
export OMP_NUM_THREADS=1
export MKL_DYNAMIC="FALSE"
export OMP_DYNAMIC="FALSE"

##   My OpenMPI-Intel-i8 installation   ##
export PATH=/zfs/hybrilit.jinr.ru/user/m/milias/work/software/openmpi/openmpi-4.0.4-intel19.3-i8/bin:$PATH
export LD_LIBRARY_PATH=/zfs/hybrilit.jinr.ru/user/m/milias/work/software/openmpi/openmpi-4.0.4-intel19.3-i8/lib:$LD_LIBRARY_PATH

#
export DIRAC_ROOTDIR=/zfs/hybrilit.jinr.ru/user/m/milias/work/software/dirac/master_production
export DIRAC_BUILDDIR=$DIRAC_ROOTDIR/build_openmpi_intel_i8_mklpar
#
export PAM=$DIRAC_BUILDDIR/pam
export BASDIR=$DIRAC_ROOTDIR/basis_dalton:$DIRAC_ROOTDIR/basis:$DIRAC_ROOTDIR/basis_ecp


echo -e "\nldd $DIRAC_BUILDDIR/dirac.x:"
ldd $DIRAC_BUILDDIR/dirac.x
echo -e "\n ifort -V: \c"; ifort -V

echo -e "\nMy PATH=$PATH\n"
echo -e "Python -v :\c"; python -V
echo -e "cmake ? which cmake = \c"; which cmake
echo -e "ctest ? which ctest = \c"; which ctest
echo -e "ctest --version \c"; ctest --version
echo -e "\n mpirun ? \c"; which mpirun; mpirun --version
echo

#export DIRAC_TMPDIR=$SLURM_SUBMIT_DIR
export DIRAC_TMPDIR=/zfs/hybrilit.jinr.ru/user/m/milias/scratch

echo -e "\nDIRAC scratch directory space, $DIRAC_TMPDIR"
df -h $DIRAC_TMPDIR/.
echo -e "\n For comparison, /tmp local disk;  df -h /tmp/."; df -h /tmp
echo -e "\n"

#----------------------------------------------
# run few tests with ctest in $DIRAC_BUILDDIR
#----------------------------------------------
echo -e "\n Running short test suite on the hostname=\c";hostname -f
cd $DIRAC_BUILDDIR
export DIRAC_MPI_COMMAND="mpirun -np 2"
echo -e "DIRAC_MPI_COMMAND=$DIRAC_MPI_COMMAND"
export MKL_NUM_THREADS=1
time ctest -j4 -VV -L short
unset DIRAC_MPI_COMMAND
echo -e "Variable DIRAC_MPI_COMMAND was unset."

# for running jobs from your homedir
cd $SLURM_SUBMIT_DIR

#$PAM --noarch --mw=2200 --nw=900 --aw=7800  --mol=Sg-CO_6.v3z.D2h.mol --inp=Sg-CO6.x2c_a4p.2fs_scf_relcc54ce_v26.inp   --put "DFCOEF.Sg-CO_6.x2c_a4p.scf.v3z=DFCOEF" --suffix=out_mpi
#$PAM --noarch --mw=2200 --nw=900 --aw=8000  --mol=Bh-CO_4.x2c_a4p_bp86_v2z.Cs.mol --inp=Bh-CO_4.x2c_a4.scf_relcc_o_m1.2_v100.inp --keep_scratch
#$PAM --noarch --mpi=1 --gb=100.5 --ag=120.5  --mol=Bh-CO_4.x2c_a4p_bp86_v2z.Cs.mol --inp=Bh-CO_4.x2c_a4.relcc_o20-19_v15.inp  --scratchfull=/lustre/nyx/ukt/milias/scratch/milias/DIRAC_Bh-CO_4.x2c_a4.scf_relcc_o20-19_v15_Bh-CO_4.x2c_a4p_bp86_v2z.Cs_5327

#THISMPI=10
#THISMPI=4
#let "NUMTHR=$NPROCS/$THISMPI"
#export MKL_NUM_THREADS=$NUMTHR
#export MKL_NUM_THREADS=2
#echo -e "\nThis node has $NPROCS CPUs available for this EXCLUSIVE JOB and dirac.x is running via $THISMPI threads."
#echo -e "Therefore, for the MKL internal parallelization, number of calculated threads=$MKL_NUM_THREADS \n"
#
#$PAM --noarch --mpi=$THISMPI --gb=23 --ag=24 --mol=TsOH.dirac_geom.v2z.Cs.mol --inp=TsOH.geomopt-x2c_a.scf_relcc-46e-1000v.inp  --suffix=i17mkl_mpi$THISMPI-omp$MKL_NUM_THREADS-$SLURM_JOB_PARTITION-$SLURM_JOBID-out --put="DFPCMO.TsOH.Cs_v2z=DFPCMO"
#$PAM --noarch --mpi=$THISMPI --gb=19 --ag=20 --mol=TsOH.acv3z.Cs.mol  --inp=TsOH.x2c_a2p.scf_relcc-46e-5000v.inp  --suffix=mkl_mpi$THISMPI-omp$MKL_NUM_THREADS-$SLURM_JOB_PARTITION-$SLURM_JOBID-out --put="DFPCMO.TsOH.Cs_acv3z=DFPCMO"
#

exit
