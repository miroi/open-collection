#!/bin/bash
#SBATCH -J Si

## max. execution time
##SBATCH -t 7-00:00:00
#SBATCH -t 0-00:20:00

# number of nodes
#SBATCH -N 1
#SBATCH --exclusive
##SBATCH --mem=220GB
#SBATCH --mem=40GB

## CPU count  - max. 40 per node ?
#SBATCH -n 4
## memory per CPU
###SBATCH --mem-per-cpu=4G

##  partition (queue)
##SBATCH -p parallel
##SBATCH -p long
##SBATCH -p main
#SBATCH -p debug

## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

## mail
#SBATCH --mail-user=M.Ilias@gsi.de 
#SBATCH --mail-type=ALL

 
echo Job user is $SLURM_JOB_USER and his job $SLURM_JOB_NAME has assigned ID $SLURM_JOBID
echo This job was submitted from the computer $SLURM_SUBMIT_HOST
echo and from the home directory:
echo $SLURM_SUBMIT_DIR
echo
echo It is running on the cluster compute node:
echo $SLURM_CLUSTER_NAME
echo and is employing $SLURM_JOB_NUM_NODES node/nodes:
echo $SLURM_JOB_NODELIST
echo
echo -e "Job partition is : $SLURM_JOB_PARTITION \n"
echo The job exclusive job has $SLURM_CPUS_ON_NODE CPU on the node.

echo -e "\n Memory per node,  SLURM_MEM_PER_NODE=$SLURM_MEM_PER_NODE MB"
if [[ $SLURM_MEM_PER_CPU ]]; then
  echo -e "\n Memory pre 1 CPU, SLURM_MEM_PER_CPU=$SLURM_MEM_PER_CPU MB"
fi


module use /cvmfs/it.gsi.de/modulefiles/
echo -e "All modules at disposal at the node:"
module avail
echo

source /lustre/ukt/milias/bin/intel-mpi-mkl_2019.4/intel/impi/2019.4.243/intel64/bin/mpivars.sh 
source /lustre/ukt/milias/bin/intel-mpi-mkl_2019.4/intel/bin/compilervars.sh intel64 -platform linux  
source /lustre/ukt/milias/bin/intel-mpi-mkl_2019.4/intel/compilers_and_libraries_2019.4.243/linux/mpi/intel64/bin/mpivars.sh
module load compiler/intel/19.0 ; echo -e "Activated Intel 19.0, MKL & Intel-MPI"
which mpiifort; mpiifort --version
which mpiicc; mpiicc --version
which mpiicpc;  mpiicpc --version
which mpirun; mpirun --version

echo -e "\n\n loaded modules:"
module list

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"

# CPU model, total numer of CPUs, number of allocated CPUs
echo -e "The node's CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo "This node has total $NPROCS CPUs available for EXCLUSIVE job."
#echo "This node has $SLURM_CPUS_ON_NODE CPUs allocated for SLURM calculations."

echo -e "\n the memory at the node (in GB)"
free -t -g
echo -e "\n"

## set internal OpenMP parallelization for MKL - Slurm CPU count
#export MKL_NUM_THREADS=$SLURM_CPUS_ON_NODE
#export MKL_NUM_THREADS=24
#export MKL_NUM_THREADS=$NPROCS
#echo -e  "\nInternal MKL parallelization MKL_NUM_THREADS=$MKL_NUM_THREADS\n"
export OMP_NUM_THREADS=1
export MKL_DYNAMIC="FALSE"
export OMP_DYNAMIC="FALSE"

echo -e "\n ifort -V: \c"; ifort -V

echo -e "\nMy PATH=$PATH\n"
echo -e "Python -v :\c"; python -V
echo -e "cmake ? which cmake = \c"; which cmake
echo -e "ctest ? which ctest = \c"; which ctest
echo -e "ctest --version \c"; ctest --version
echo -e "\n mpirun ? \c"; which mpirun; mpirun --version
echo

echo -e "\n For comparison, /tmp local disk;  df -h /tmp/."; df -h /tmp
echo -e "\n"
FLEUR_BUILD=/lustre/ukt/milias/work/software/fleur/fleur_release/build

ldd $FLEUR_BUILD/inpgen
ldd $FLEUR_BUILD/fleur_MPI

# for running jobs from your homedir
cd $SLURM_SUBMIT_DIR

#$PAM --noarch --mpi=1 --gb=100.5 --ag=120.5  --mol=Bh-CO_4.x2c_a4p_bp86_v2z.Cs.mol --inp=Bh-CO_4.x2c_a4.relcc_o20-19_v15.inp  --scratchfull=/lustre/nyx/ukt/milias/scratch/milias/DIRAC_Bh-CO_4.x2c_a4.scf_relcc_o20-19_v15_Bh-CO_4.x2c_a4p_bp86_v2z.Cs_5327

#THISMPI=$SLURM_CPUS_ON_NODE
#THISMPI=1
#let "NUMTHR=$NPROCS/$THISMPI"
#export MKL_NUM_THREADS=$NPROCS
#export MKL_NUM_THREADS=$SLURM_CPUS_ON_NODE
#echo -e "\nThis node has $NPROCS CPUs available for this EXCLUSIVE JOB and dirac.x is running via $THISMPI threads."
#echo -e "Therefore, for the MKL internal parallelization, number of calculated threads=$MKL_NUM_THREADS \n"

# $PAM --noarch --mpi=$THISMPI --gb=218 --ag=220 --mol=Rh-Hax-CO_4.sozora_bp_tz2p.xyz  --inp=Rh-Hax-CO_4.x2c_a4p.scf1fs.v3z.relcc-58ce-v12au.inp  --suffix=i17mkl_mpi$THISMPI-omp$MKL_NUM_THREADS-$SLURM_JOB_PARTITION-$SLURM_JOBID-out --put "DFPCMO.RhHCO4.x2c_a4p.v3z.adf-xyz=DFPCMO"

echo -e "\n running $FLEUR_BUILD/inpgen < inp_Si on \"`hostname`\"  "

  $FLEUR_BUILD/inpgen < inp_Si 

echo -e "\n running mpirun -np 4 $FLEUR_BUILD/fleur_MPI "

  mpirun -np 4 $FLEUR_BUILD/fleur_MPI


exit
