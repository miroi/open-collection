#!/bin/bash
#SBATCH -J NWcont

## max. execution time
##SBATCH -t 7-00:00:00
##SBATCH -t 0-08:00:00
#SBATCH -t 0-00:30:00

# number of nodes
#SBATCH -N 1
##SBATCH --exclusive
##SBATCH --mem=120GB
#SBATCH --mem=4GB

# do not restart in the case of nodefail!
#SBATCH --no-requeue
#SBATCH --no-kill

## CPU count  - max. 40 per node ?
#SBATCH -n 4

##  partition (queue)
##SBATCH -p parallel
##SBATCH -p long
##SBATCH -p main
#SBATCH -p debug

## memory NO !!!
##SBATCH --mem-per-cpu=28G

## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

## mail
#SBATCH --mail-user=M.Ilias@gsi.de 
#SBATCH --mail-type=ALL

 
echo Job user is $SLURM_JOB_USER and his job $SLURM_JOB_NAME has assigned ID $SLURM_JOBID
echo This job was submitted from the computer $SLURM_SUBMIT_HOST
echo and from the home directory:
echo $SLURM_SUBMIT_DIR
echo
echo It is running on the cluster compute node:
echo $SLURM_CLUSTER_NAME
echo and is employing $SLURM_JOB_NUM_NODES node/nodes:
echo $SLURM_JOB_NODELIST
echo
echo -e "Job partition is: $SLURM_JOB_PARTITION \n"
echo The job requests $SLURM_CPUS_ON_NODE CPU per task.

module use /cvmfs/it.gsi.de/modulefiles/
#echo "modules at disposal:"
#module avail
#module load cmake/3.15.3
#module load compiler/intel/17.4

#echo -e "\n\n loaded modules:"
#module list

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"

# CPU model, total numer of CPUs, number of allocated CPUs
echo -e "The node's CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo "This node has total $NPROCS CPUs available for an EXCLUSIVE job."
#
echo "This node has $SLURM_CPUS_ON_NODE CPUs allocated for SLURM calculations."

echo -e "\n the memory at the node (in GB)"
free -t -g
echo -e "\n"

## set internal OpenMP parallelization for MKL - Slurm CPU count
#export MKL_NUM_THREADS=$SLURM_CPUS_ON_NODE
#export MKL_NUM_THREADS=24
#export MKL_NUM_THREADS=$NPROCS
#echo -e  "\nInternal MKL parallelization MKL_NUM_THREADS=$MKL_NUM_THREADS\n"
export OMP_NUM_THREADS=1
export MKL_DYNAMIC="FALSE"
export OMP_DYNAMIC="FALSE"


echo -e "\nMy PATH=$PATH\n"
#echo -e "Python -v :\c"; python -V

echo -e "\n SINGULARITY_CONTAINERS=$SINGULARITY_CONTAINERS \n"


# for running jobs from your homedir
cd $SLURM_SUBMIT_DIR

npr=$SLURM_CPUS_ON_NODE/2
echo -e "npr=$npr"

singularity run $SINGULARITY_CONTAINERS/nwchem-dev.ompi40x.ifort.skylake_latest.sif   mpirun -np $npr  nwchem h2o_scf_6-31g.nw > out_$SLURM_CPUS_ON_NODE

exit
