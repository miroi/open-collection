#!/bin/bash
#SBATCH --job-name=qe-test       # create a short name for your job
#SBATCH --nodes=1     # node count
#SBATCH --nodelist=comp[17-23,39-46]  # 2x NVIDIA Tesla M2070 6GB RAM

##SBATCH --constraint="gpu"  # Nvidia
#SBATCH --ntasks-per-node=6      # number of tasks per node
##SBATCH --cpus-per-task=1        # cpu-cores per task (>1 if multi-threaded tasks)
##SBATCH --mem=32G                # total memory per node
##SBATCH --gres=gpu:2             # number of gpus per node
##SBATCH --time=00:15:00          # total run time limit (HH:MM:SS)

## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

#SBATCH --mail-type=begin        # send email when job begins
#SBATCH --mail-type=end          # send email when job ends
#SBATCH --mail-user=Miroslav.Ilias@umb.sk


echo Job user is $SLURM_JOB_USER and his job $SLURM_JOB_NAME has assigned ID $SLURM_JOBID
echo This job was submitted from the computer $SLURM_SUBMIT_HOST
echo and from the home directory:
echo $SLURM_SUBMIT_DIR
echo
echo It is running on the cluster compute node:
echo $SLURM_CLUSTER_NAME
echo and is employing $SLURM_JOB_NUM_NODES node/nodes:
echo $SLURM_JOB_NODELIST
echo
echo -e "Job partition is $SLURM_JOB_PARTITION \n"
#echo The job requests $SLURM_CPUS_ON_NODE CPU per task.

module avail

#module purge
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

srun --mpi=pmi2 \
singularity run --nv \
     /lustre/home/ilias/containers/quantum_espresso_v6.7.sif \
     pw.x -input ausurf.in -npool 2

exit
