#!/bin/bash

#SBATCH --job-name=qe-test       # create a short name for your job
#SBATCH --nodes=1 --constraint=k20m     # node count
#SBATCH --ntasks-per-node=6      # number of tasks per node

#SBATCH --partition=devel
#SBATCH --time=01:00:00          # total run time limit (HH:MM:SS)

## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

#SBATCH --mail-user=Miroslav.Ilias@umb.sk


echo Job user is $SLURM_JOB_USER and his job $SLURM_JOB_NAME has assigned ID $SLURM_JOBID
echo This job was submitted from the computer $SLURM_SUBMIT_HOST
echo and from the home directory:
echo $SLURM_SUBMIT_DIR
echo
echo It is running on the cluster compute node:
echo $SLURM_CLUSTER_NAME
echo and is employing $SLURM_JOB_NUM_NODES node/nodes:
echo $SLURM_JOB_NODELIST
echo
echo -e "Job partition is $SLURM_JOB_PARTITION \n"
#echo The job requests $SLURM_CPUS_ON_NODE CPU per task.

module avail

#module purge
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

srun --mpi=pmi2 \
singularity run --nv \
     /lustre/home/ilias/containers/quantum_espresso_v6.7.sif \
     pw.x -input ausurf.in -npool 2

exit
