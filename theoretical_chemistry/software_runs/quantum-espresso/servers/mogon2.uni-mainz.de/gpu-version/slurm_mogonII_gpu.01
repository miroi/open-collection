#!/bin/bash

#SBATCH -J GPUtest

#SBATCH -A m2_jgu-gpu-qe-calcs
#SBATCH -p m2_gpu

# ... other SBATCH statements
#SBATCH --gres=gpu:1

## max. execution time
##SBATCH -t 4-00:00:00
#SBATCH -t 0-00:30:00

# number of nodes
#SBATCH -N 1
##SBATCH --exclusive
##SBATCH --mem=120GB

## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

## mail
#SBATCH --mail-user=M.Ilias@gsi.de 
#SBATCH --mail-type=ALL

echo -e "\nJob user is SLURM_JOB_USER= $SLURM_JOB_USER"
echo -e "User job SLURM_JOB_NAME=$SLURM_JOB_NAME has assigned ID SLURM_JOBID=$SLURM_JOBID"

echo -e "This job was submitted from the computer SLURM_SUBMIT_HOST=$SLURM_SUBMIT_HOST"
echo -e " and from the home directory SLURM_SUBMIT_DIR=$SLURM_SUBMIT_DIR"
echo -e "Job is running on the cluster compute node: SLURM_CLUSTER_NAME=$SLURM_CLUSTER_NAME"
echo -e "and is employing SLURM_JOB_NUM_NODES=$SLURM_JOB_NUM_NODES node/nodes:"
echo -e "SLURM_JOB_NODELIST = $SLURM_JOB_NODELIST"
echo -e "Job partition is SLURM_JOB_PARTITION=$SLURM_JOB_PARTITION \n"
echo -e "Default memory per node, DefMemPerNode=$DefMemPerNode"
echo -e "Maximum memory per node, MaxMemPerNode=$MaxMemPerNode"
echo -e "Default memory per CPU, DefMemPerCPU=$DefMemPerCPU"
echo -e "Maximum memory per CPU, MaxMemPerNode=$MaxMemPerCPU"


echo -e "\n Modules :"

module purge
module load compiler/NVHPC/21.7
module load lang/Python
module list

echo -e  "\n CUDA_HOME = $CUDA_HOME "

which python; python -V

python /home/mirilias/work/software/quantum_espresso/gpu-qe-6.8/q-e-qe-6.8/dev-tools/get_device_props.py


exit



