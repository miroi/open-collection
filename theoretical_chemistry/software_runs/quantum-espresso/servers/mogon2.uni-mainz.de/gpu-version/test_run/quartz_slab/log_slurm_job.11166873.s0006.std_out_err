
Job user is SLURM_JOB_USER=mirilias
Job partition is: 
User's job is SLURM_JOB_NAME=QuartzV has assigned ID SLURM_JOBID=11166873
This job was submitted from the computer SLURM_SUBMIT_HOST=login22.mogon
and from the home directory: SLURM_SUBMIT_DIR=/gpfs/fs1/home/mirilias/work/projects/valeria-band/quartz/alpha_quartz/cut_010_O3_4x4_Vicinal/slab/qe/single_point
It is running on the cluster compute node: SLURM_CLUSTER_NAME=mogon2
Job  is employing SLURM_JOB_NUM_NODES=14 node/nodes:
SLURM_JOB_NODELIST=s[0006-0008,0010-0011,0015-0019,0026,0028-0030]
The job requests SLURM_CPUS_ON_NODE=24 CPU per task.

The following have been reloaded with a version change:
  1) compiler/GCCcore/11.2.0 => compiler/GCCcore/8.3.0

List of loaded modules:

Currently Loaded Modules:
  1) devel/CMake/3.21.1
  2) compiler/intel-compilers/2022.0.2
  3) lib/UCX/1.11.2-GCCcore-11.2.0
  4) mpi/impi/2021.5.1-intel-compilers-2022.0.2
  5) system/CUDA/11.4.2
  6) compiler/GCCcore/8.3.0
  7) compiler/PGI/20.4-GCC-8.3.0
  8) system/CUDAcore/11.2.2
  9) compiler/NVHPC/21.7
 10) numlib/imkl/2022.0.2

Running on host s0006.mogon
Time is Sat Jul  9 21:56:36 CEST 2022 


My PATH=/cluster/easybuild/broadwell/software/NVHPC/21.7:/cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/bin:/cluster/easybuild/broadwell/software/CUDAcore/11.2.2:/cluster/easybuild/broadwell/software/CUDAcore/11.2.2/nvvm/bin:/cluster/easybuild/broadwell/software/CUDAcore/11.2.2/bin:/cluster/easybuild/broadwell/software/PGI/20.4-GCC-8.3.0:/cluster/easybuild/broadwell/software/PGI/20.4-GCC-8.3.0/linux86-64/20.4/bin:/cluster/easybuild/broadwell/software/compiler/GCCcore/8.3.0/bin:/cluster/easybuild/broadwell/software/CUDA/11.4.2/nvvm/bin:/cluster/easybuild/broadwell/software/CUDA/11.4.2/bin:/cluster/easybuild/broadwell/software/impi/2021.5.1-intel-compilers-2022.0.2/mpi/2021.5.1/libfabric/bin:/cluster/easybuild/broadwell/software/impi/2021.5.1-intel-compilers-2022.0.2/mpi/2021.5.1/bin:/cluster/easybuild/broadwell/software/UCX/1.11.2-GCCcore-11.2.0/bin:/cluster/easybuild/broadwell/software/intel-compilers/2022.0.2/compiler/2022.0.2/linux/bin/intel64:/cluster/easybuild/broadwell/software/intel-compilers/2022.0.2/compiler/2022.0.2/linux/bin:/cluster/easybuild/broadwell/software/CMake/3.21.1/bin:/usr/lpp/mmfs/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/var/cfengine/bin:.

My LD_LIBRARY_PATH=/cluster/easybuild/broadwell/software/imkl/2022.0.2/mkl/2022.0.2/lib/intel64:/cluster/easybuild/broadwell/software/imkl/2022.0.2/compiler/2022.0.2/linux/compiler/lib/intel64_lin:/cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib:/cluster/easybuild/broadwell/software/CUDAcore/11.2.2/nvvm/lib64:/cluster/easybuild/broadwell/software/CUDAcore/11.2.2/extras/CUPTI/lib64:/cluster/easybuild/broadwell/software/CUDAcore/11.2.2/lib:/cluster/easybuild/broadwell/software/PGI/20.4-GCC-8.3.0/linux86-64/20.4/lib:/cluster/easybuild/broadwell/software/compiler/GCCcore/8.3.0/lib/gcc/x86_64-pc-linux-gnu/8.3.0:/cluster/easybuild/broadwell/software/compiler/GCCcore/8.3.0/lib64:/cluster/easybuild/broadwell/software/compiler/GCCcore/8.3.0/lib:/cluster/easybuild/broadwell/software/CUDA/11.4.2/nvvm/lib64:/cluster/easybuild/broadwell/software/CUDA/11.4.2/extras/CUPTI/lib64:/cluster/easybuild/broadwell/software/CUDA/11.4.2/lib:/cluster/easybuild/broadwell/software/impi/2021.5.1-intel-compilers-2022.0.2/mpi/2021.5.1/libfabric/lib:/cluster/easybuild/broadwell/software/impi/2021.5.1-intel-compilers-2022.0.2/mpi/2021.5.1/lib/release:/cluster/easybuild/broadwell/software/impi/2021.5.1-intel-compilers-2022.0.2/mpi/2021.5.1/lib:/cluster/easybuild/broadwell/software/UCX/1.11.2-GCCcore-11.2.0/lib:/cluster/easybuild/broadwell/software/intel-compilers/2022.0.2/tbb/2021.5.1/lib/intel64/gcc4.8:/cluster/easybuild/broadwell/software/intel-compilers/2022.0.2/compiler/2022.0.2/linux/compiler/lib/intel64_lin:/cluster/easybuild/broadwell/software/intel-compilers/2022.0.2/compiler/2022.0.2/linux/lib/x64:/cluster/easybuild/broadwell/software/intel-compilers/2022.0.2/compiler/2022.0.2/linux/lib

The node's CPU model name	: Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz
This node has 48 CPUs available.
(i) This node has SLURM_CPUS_ON_NODE=24 CPUs allocated for SLURM calculations.

 The TOTAL memory at the node (in GB); free -t -g
              total        used        free      shared  buff/cache   available
Mem:            125           2         122           0           0         119
Swap:            48           0          48
Total:          174           2         171


 Dependencies of QE main executable, ldd /home/mirilias/work/software/quantum_espresso/qe-devel/build_intelmpi_gpu/bin/pw.x:
	linux-vdso.so.1 (0x00007ffe4f933000)
	libmpifort.so.12 => /cluster/easybuild/broadwell/software/impi/2021.5.1-intel-compilers-2022.0.2/mpi/2021.5.1/lib/libmpifort.so.12 (0x00007f69ef875000)
	libmpi.so.12 => /cluster/easybuild/broadwell/software/impi/2021.5.1-intel-compilers-2022.0.2/mpi/2021.5.1/lib/release/libmpi.so.12 (0x00007f69ee040000)
	librt.so.1 => /usr/lib/gcc/x86_64-redhat-linux/8/../../../../lib64/librt.so.1 (0x00007f69ede38000)
	libpthread.so.0 => /usr/lib/gcc/x86_64-redhat-linux/8/../../../../lib64/libpthread.so.0 (0x00007f69edc18000)
	libdl.so.2 => /usr/lib/gcc/x86_64-redhat-linux/8/../../../../lib64/libdl.so.2 (0x00007f69eda14000)
	libacchost.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libacchost.so (0x00007f69ed7a4000)
	libmkl_intel_lp64.so.2 => /cluster/easybuild/broadwell/software/imkl/2022.0.2/mkl/2022.0.2/lib/intel64/libmkl_intel_lp64.so.2 (0x00007f69ec904000)
	libmkl_intel_thread.so.2 => /cluster/easybuild/broadwell/software/imkl/2022.0.2/mkl/2022.0.2/lib/intel64/libmkl_intel_thread.so.2 (0x00007f69e91a2000)
	libmkl_core.so.2 => /cluster/easybuild/broadwell/software/imkl/2022.0.2/mkl/2022.0.2/lib/intel64/libmkl_core.so.2 (0x00007f69e4ded000)
	libiomp5.so => /cluster/easybuild/broadwell/software/imkl/2022.0.2/compiler/2022.0.2/linux/compiler/lib/intel64_lin/libiomp5.so (0x00007f69e49cc000)
	libm.so.6 => /usr/lib/gcc/x86_64-redhat-linux/8/../../../../lib64/libm.so.6 (0x00007f69e464a000)
	libcufft.so.10 => /cluster/easybuild/broadwell/software/CUDAcore/11.2.2/lib64/libcufft.so.10 (0x00007f69d8e9b000)
	libcudaforwraprand.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libcudaforwraprand.so (0x00007f69d8c98000)
	libcurand.so.10 => /cluster/easybuild/broadwell/software/CUDAcore/11.2.2/lib64/libcurand.so.10 (0x00007f69d3531000)
	libcusolver.so.11 => /cluster/easybuild/broadwell/software/CUDAcore/11.2.2/lib64/libcusolver.so.11 (0x00007f69bb47b000)
	libcublas.so.11 => /cluster/easybuild/broadwell/software/CUDAcore/11.2.2/lib64/libcublas.so.11 (0x00007f69b44b8000)
	libcublasLt.so.11 => /cluster/easybuild/broadwell/software/CUDAcore/11.2.2/lib64/libcublasLt.so.11 (0x00007f69a88d7000)
	libcudaforwrapblas.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libcudaforwrapblas.so (0x00007f69a869a000)
	libcudart.so.11.0 => /cluster/easybuild/broadwell/software/CUDAcore/11.2.2/lib64/libcudart.so.11.0 (0x00007f69a840b000)
	libcudafor_110.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libcudafor_110.so (0x00007f69a4858000)
	libcudafor.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libcudafor.so (0x00007f69a463b000)
	libaccdevaux.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libaccdevaux.so (0x00007f69a442f000)
	libacccuda.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libacccuda.so (0x00007f69a40fd000)
	libcudadevice.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libcudadevice.so (0x00007f69a3eea000)
	libcudafor2.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libcudafor2.so (0x00007f69a3ce7000)
	libnvf.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libnvf.so (0x00007f69a36bd000)
	libnvomp.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libnvomp.so (0x00007f69a2a43000)
	libnvcpumath.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libnvcpumath.so (0x00007f69a260e000)
	libnvc.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libnvc.so (0x00007f69a23b6000)
	libc.so.6 => /usr/lib/gcc/x86_64-redhat-linux/8/../../../../lib64/libc.so.6 (0x00007f69a1ff1000)
	libgcc_s.so.1 => /usr/lib/gcc/x86_64-redhat-linux/8/../../../../lib64/libgcc_s.so.1 (0x00007f69a1dd9000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f69efc29000)
	libstdc++.so.6 => /cluster/easybuild/broadwell/software/compiler/GCCcore/8.3.0/lib64/libstdc++.so.6 (0x00007f69efcb6000)

Python -v :Python 2.7.18
mpirun ? which mpirun  = /cluster/easybuild/broadwell/software/impi/2021.5.1-intel-compilers-2022.0.2/mpi/2021.5.1/bin/mpirun
mpirun --version Intel(R) MPI Library for Linux* OS, Version 2021.5 Build 20211102 (id: 9279b7d62)
Copyright 2003-2021, Intel Corporation.

 Current directory where this SLURM job is running /gpfs/fs1/home/mirilias/work/projects/valeria-band/quartz/alpha_quartz/cut_010_O3_4x4_Vicinal/slab/qe/single_point
 It has the disk space of (df -h) :
Filesystem                 Size  Used Avail Use% Mounted on
10.80.0.46:/gpfs/fs1/home  120T   42T   79T  35% /gpfs/fs1/home


 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     Error in routine  fft_scalar_cuFFT: cft_2xy_gpu (5):
  cufftPlanMany failed
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

     stopping ...
Abort(5) on node 6 (rank 6 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 5) - process 6
