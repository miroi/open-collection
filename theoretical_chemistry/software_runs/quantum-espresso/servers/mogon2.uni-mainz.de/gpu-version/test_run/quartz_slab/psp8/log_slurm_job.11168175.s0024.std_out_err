
Job user is SLURM_JOB_USER=mirilias
Job partition is: 
User's job is SLURM_JOB_NAME=QuartzV has assigned ID SLURM_JOBID=11168175
This job was submitted from the computer SLURM_SUBMIT_HOST=login22.mogon
and from the home directory: SLURM_SUBMIT_DIR=/gpfs/fs1/home/mirilias/work/projects/open-collection/theoretical_chemistry/software_runs/quantum-espresso/servers/mogon2.uni-mainz.de/gpu-version/test_run/quartz_slab/psp8
It is running on the cluster compute node: SLURM_CLUSTER_NAME=mogon2
Job  is employing SLURM_JOB_NUM_NODES=1 node/nodes:
SLURM_JOB_NODELIST=s0024
The job requests SLURM_CPUS_ON_NODE=48 CPU per task.

The following have been reloaded with a version change:
  1) compiler/GCCcore/11.2.0 => compiler/GCCcore/8.3.0

List of loaded modules:

Currently Loaded Modules:
  1) devel/CMake/3.21.1
  2) compiler/intel-compilers/2022.0.2
  3) lib/UCX/1.11.2-GCCcore-11.2.0
  4) mpi/impi/2021.5.1-intel-compilers-2022.0.2
  5) system/CUDA/11.4.2
  6) compiler/GCCcore/8.3.0
  7) compiler/PGI/20.4-GCC-8.3.0
  8) system/CUDAcore/11.2.2
  9) compiler/NVHPC/21.7
 10) numlib/imkl/2022.0.2

Running on host s0024.mogon
Time is Sun Jul 10 14:06:04 CEST 2022 


My PATH=/cluster/easybuild/broadwell/software/NVHPC/21.7:/cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/bin:/cluster/easybuild/broadwell/software/CUDAcore/11.2.2:/cluster/easybuild/broadwell/software/CUDAcore/11.2.2/nvvm/bin:/cluster/easybuild/broadwell/software/CUDAcore/11.2.2/bin:/cluster/easybuild/broadwell/software/PGI/20.4-GCC-8.3.0:/cluster/easybuild/broadwell/software/PGI/20.4-GCC-8.3.0/linux86-64/20.4/bin:/cluster/easybuild/broadwell/software/compiler/GCCcore/8.3.0/bin:/cluster/easybuild/broadwell/software/CUDA/11.4.2/nvvm/bin:/cluster/easybuild/broadwell/software/CUDA/11.4.2/bin:/cluster/easybuild/broadwell/software/impi/2021.5.1-intel-compilers-2022.0.2/mpi/2021.5.1/libfabric/bin:/cluster/easybuild/broadwell/software/impi/2021.5.1-intel-compilers-2022.0.2/mpi/2021.5.1/bin:/cluster/easybuild/broadwell/software/UCX/1.11.2-GCCcore-11.2.0/bin:/cluster/easybuild/broadwell/software/intel-compilers/2022.0.2/compiler/2022.0.2/linux/bin/intel64:/cluster/easybuild/broadwell/software/intel-compilers/2022.0.2/compiler/2022.0.2/linux/bin:/cluster/easybuild/broadwell/software/CMake/3.21.1/bin:/usr/lpp/mmfs/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/var/cfengine/bin:.

My LD_LIBRARY_PATH=/cluster/easybuild/broadwell/software/imkl/2022.0.2/mkl/2022.0.2/lib/intel64:/cluster/easybuild/broadwell/software/imkl/2022.0.2/compiler/2022.0.2/linux/compiler/lib/intel64_lin:/cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib:/cluster/easybuild/broadwell/software/CUDAcore/11.2.2/nvvm/lib64:/cluster/easybuild/broadwell/software/CUDAcore/11.2.2/extras/CUPTI/lib64:/cluster/easybuild/broadwell/software/CUDAcore/11.2.2/lib:/cluster/easybuild/broadwell/software/PGI/20.4-GCC-8.3.0/linux86-64/20.4/lib:/cluster/easybuild/broadwell/software/compiler/GCCcore/8.3.0/lib/gcc/x86_64-pc-linux-gnu/8.3.0:/cluster/easybuild/broadwell/software/compiler/GCCcore/8.3.0/lib64:/cluster/easybuild/broadwell/software/compiler/GCCcore/8.3.0/lib:/cluster/easybuild/broadwell/software/CUDA/11.4.2/nvvm/lib64:/cluster/easybuild/broadwell/software/CUDA/11.4.2/extras/CUPTI/lib64:/cluster/easybuild/broadwell/software/CUDA/11.4.2/lib:/cluster/easybuild/broadwell/software/impi/2021.5.1-intel-compilers-2022.0.2/mpi/2021.5.1/libfabric/lib:/cluster/easybuild/broadwell/software/impi/2021.5.1-intel-compilers-2022.0.2/mpi/2021.5.1/lib/release:/cluster/easybuild/broadwell/software/impi/2021.5.1-intel-compilers-2022.0.2/mpi/2021.5.1/lib:/cluster/easybuild/broadwell/software/UCX/1.11.2-GCCcore-11.2.0/lib:/cluster/easybuild/broadwell/software/intel-compilers/2022.0.2/tbb/2021.5.1/lib/intel64/gcc4.8:/cluster/easybuild/broadwell/software/intel-compilers/2022.0.2/compiler/2022.0.2/linux/compiler/lib/intel64_lin:/cluster/easybuild/broadwell/software/intel-compilers/2022.0.2/compiler/2022.0.2/linux/lib/x64:/cluster/easybuild/broadwell/software/intel-compilers/2022.0.2/compiler/2022.0.2/linux/lib

The node's CPU model name	: Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz
This node has 48 CPUs available.
(i) This node has SLURM_CPUS_ON_NODE=48 CPUs allocated for SLURM calculations.

 The TOTAL memory at the node (in GB); free -t -g
              total        used        free      shared  buff/cache   available
Mem:            125           2         122           0           0         119
Swap:            48           0          48
Total:          174           2         171


 Dependencies of QE main executable, ldd /home/mirilias/work/software/quantum_espresso/qe-7.1/build_impi_gpu/bin/pw.x:
	linux-vdso.so.1 (0x00007f661bf5b000)
	libmpifort.so.12 => /cluster/easybuild/broadwell/software/impi/2021.5.1-intel-compilers-2022.0.2/mpi/2021.5.1/lib/libmpifort.so.12 (0x00007f661b97d000)
	libmpi.so.12 => /cluster/easybuild/broadwell/software/impi/2021.5.1-intel-compilers-2022.0.2/mpi/2021.5.1/lib/release/libmpi.so.12 (0x00007f661a148000)
	librt.so.1 => /usr/lib/gcc/x86_64-redhat-linux/8/../../../../lib64/librt.so.1 (0x00007f6619f40000)
	libpthread.so.0 => /usr/lib/gcc/x86_64-redhat-linux/8/../../../../lib64/libpthread.so.0 (0x00007f6619d20000)
	libdl.so.2 => /usr/lib/gcc/x86_64-redhat-linux/8/../../../../lib64/libdl.so.2 (0x00007f6619b1c000)
	libacchost.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libacchost.so (0x00007f66198ac000)
	libmkl_intel_lp64.so.2 => /cluster/easybuild/broadwell/software/imkl/2022.0.2/mkl/2022.0.2/lib/intel64/libmkl_intel_lp64.so.2 (0x00007f6618a0c000)
	libmkl_intel_thread.so.2 => /cluster/easybuild/broadwell/software/imkl/2022.0.2/mkl/2022.0.2/lib/intel64/libmkl_intel_thread.so.2 (0x00007f66152aa000)
	libmkl_core.so.2 => /cluster/easybuild/broadwell/software/imkl/2022.0.2/mkl/2022.0.2/lib/intel64/libmkl_core.so.2 (0x00007f6610ef5000)
	libiomp5.so => /cluster/easybuild/broadwell/software/imkl/2022.0.2/compiler/2022.0.2/linux/compiler/lib/intel64_lin/libiomp5.so (0x00007f6610ad4000)
	libm.so.6 => /usr/lib/gcc/x86_64-redhat-linux/8/../../../../lib64/libm.so.6 (0x00007f6610752000)
	libcufft.so.10 => /cluster/easybuild/broadwell/software/CUDAcore/11.2.2/lib64/libcufft.so.10 (0x00007f6604fa3000)
	libcudaforwraprand.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libcudaforwraprand.so (0x00007f6604da0000)
	libcurand.so.10 => /cluster/easybuild/broadwell/software/CUDAcore/11.2.2/lib64/libcurand.so.10 (0x00007f65ff639000)
	libcusolver.so.11 => /cluster/easybuild/broadwell/software/CUDAcore/11.2.2/lib64/libcusolver.so.11 (0x00007f65e7583000)
	libcublas.so.11 => /cluster/easybuild/broadwell/software/CUDAcore/11.2.2/lib64/libcublas.so.11 (0x00007f65e05c0000)
	libcublasLt.so.11 => /cluster/easybuild/broadwell/software/CUDAcore/11.2.2/lib64/libcublasLt.so.11 (0x00007f65d49df000)
	libcudaforwrapblas.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libcudaforwrapblas.so (0x00007f65d47a2000)
	libcudart.so.11.0 => /cluster/easybuild/broadwell/software/CUDAcore/11.2.2/lib64/libcudart.so.11.0 (0x00007f65d4513000)
	libcudafor_110.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libcudafor_110.so (0x00007f65d0960000)
	libcudafor.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libcudafor.so (0x00007f65d0743000)
	libaccdevaux.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libaccdevaux.so (0x00007f65d0537000)
	libacccuda.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libacccuda.so (0x00007f65d0205000)
	libcudadevice.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libcudadevice.so (0x00007f65cfff2000)
	libcudafor2.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libcudafor2.so (0x00007f65cfdef000)
	libnvf.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libnvf.so (0x00007f65cf7c5000)
	libnvomp.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libnvomp.so (0x00007f65ceb4b000)
	libnvcpumath.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libnvcpumath.so (0x00007f65ce716000)
	libnvc.so => /cluster/easybuild/broadwell/software/NVHPC/21.7/Linux_x86_64/21.7/compilers/lib/libnvc.so (0x00007f65ce4be000)
	libc.so.6 => /usr/lib/gcc/x86_64-redhat-linux/8/../../../../lib64/libc.so.6 (0x00007f65ce0f9000)
	libgcc_s.so.1 => /usr/lib/gcc/x86_64-redhat-linux/8/../../../../lib64/libgcc_s.so.1 (0x00007f65cdee1000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f661bd31000)
	libstdc++.so.6 => /cluster/easybuild/broadwell/software/compiler/GCCcore/8.3.0/lib64/libstdc++.so.6 (0x00007f661bdb8000)

Python -v :Python 2.7.18
mpirun ? which mpirun  = /cluster/easybuild/broadwell/software/impi/2021.5.1-intel-compilers-2022.0.2/mpi/2021.5.1/bin/mpirun
mpirun --version Intel(R) MPI Library for Linux* OS, Version 2021.5 Build 20211102 (id: 9279b7d62)
Copyright 2003-2021, Intel Corporation.

 Current directory where this SLURM job is running /gpfs/fs1/home/mirilias/work/projects/open-collection/theoretical_chemistry/software_runs/quantum-espresso/servers/mogon2.uni-mainz.de/gpu-version/test_run/quartz_slab/psp8
 It has the disk space of (df -h) :
Filesystem                 Size  Used Avail Use% Mounted on
10.80.0.44:/gpfs/fs1/home  120T   41T   79T  35% /gpfs/fs1/home

Abort(1) on node 0 (rank 0 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0
Abort(1) on node 13 (rank 13 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 13
Abort(1) on node 1 (rank 1 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 1
Abort(1) on node 3 (rank 3 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 3
Abort(1) on node 4 (rank 4 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 4
Abort(1) on node 8 (rank 8 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 8
Abort(1) on node 9 (rank 9 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 9
Abort(1) on node 12 (rank 12 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 12
Abort(1) on node 15 (rank 15 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 15
Abort(1) on node 17 (rank 17 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 17
Abort(1) on node 20 (rank 20 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 20
Abort(1) on node 21 (rank 21 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 21
Abort(1) on node 22 (rank 22 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 22
Abort(1) on node 23 (rank 23 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 23
Abort(1) on node 24 (rank 24 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 24
Abort(1) on node 25 (rank 25 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 25
Abort(1) on node 26 (rank 26 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 26
Abort(1) on node 27 (rank 27 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 27
Abort(1) on node 28 (rank 28 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 28
Abort(1) on node 29 (rank 29 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 29
Abort(1) on node 32 (rank 32 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 32
Abort(1) on node 33 (rank 33 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 33
Abort(1) on node 34 (rank 34 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 34
Abort(1) on node 35 (rank 35 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 35
Abort(1) on node 36 (rank 36 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 36
Abort(1) on node 39 (rank 39 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 39
Abort(1) on node 41 (rank 41 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 41
Abort(1) on node 43 (rank 43 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 43
Abort(1) on node 46 (rank 46 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 46
Abort(1) on node 2 (rank 2 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 2
Abort(1) on node 5 (rank 5 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 5
Abort(1) on node 6 (rank 6 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 6
Abort(1) on node 7 (rank 7 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 7
Abort(1) on node 10 (rank 10 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 10
Abort(1) on node 11 (rank 11 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 11
Abort(1) on node 14 (rank 14 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 14
Abort(1) on node 16 (rank 16 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 16
Abort(1) on node 18 (rank 18 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 18
Abort(1) on node 19 (rank 19 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 19
Abort(1) on node 30 (rank 30 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 30
Abort(1) on node 31 (rank 31 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 31
Abort(1) on node 37 (rank 37 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 37
Abort(1) on node 38 (rank 38 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 38
Abort(1) on node 40 (rank 40 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 40
Abort(1) on node 42 (rank 42 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 42
Abort(1) on node 44 (rank 44 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 44
Abort(1) on node 45 (rank 45 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 45
Abort(1) on node 47 (rank 47 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 47
